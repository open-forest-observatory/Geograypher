{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Geograypher documentation","text":"<p>Welcome to the documentation for the Geograyher .</p>"},{"location":"#conceptual-workflow","title":"Conceptual workflow","text":"<p>Imagine you are trying to map the objects in a hypothetical region. Your world consists of three types of objects: cones, cubes, and cylinders. Cones are different shades of blue, cubes are difference shades of orange, and cylinders are different shades of green. Your landscape consists of a variety of these objects arranged randomly on a flat gray surface. You fly a drone survey and collect images of your scene, some of which are shown below.</p> <p> </p> <p>While you are there, you also do some field work and survey a small subset of your region. Field work is labor-intensive, so you can't survey the entire region your drone flew. You note down the class of the object and their location and shape in geospatial coordinates. This results in the following geospatial map.</p> <p> </p> <p>You use structure from motion to build a 3D model of your scene and also estimate the locations that each image was taken from.</p> <p> </p> <p>Up to this point, you have been following a fairly standard workflow. A common practice at this point would be to generate a top-down, 2D orthomosaic of the scene and do any prediction tasks, such as deep learning model training or inference, using this data. Instead, you decide it's important to maintain the high quality of the raw images and be able to see the sides of your objects when you are generating predictions. This is where geograypher comes in.</p> <p>Using your field reference map and the 3D model from photogrammetry, you determine which portions of your 3D scene correspond to each object. This is shown below, with the colors now representing the classification label.</p> <p> </p> <p>Your end goal is to generate predictions on the entire region. For this, you need a machine learning model that can generate automatic predictions on your data. No one else has developed a model for your cone-cube-cylinder classification task, so you need to train your own using labeled example data. Using the mesh that is textured with the classification information from the field survey, and the pose of the camera, you can \"render\" the labels onto the images. They are shown below, color-coded by class.</p> <p> </p> <p>These labels correspond to the images shown below.</p> <p> </p> <p>Now that you have pairs of real images and rendered labels, you can train a machine learning model to predict the class of the objects from the images. This model can be now used to generate predictions on un-labeled images. An example prediction is shown below.</p> <p> </p> <p>To make these predictions useful, you need the information in geospatial coordinates. We again use the mesh model as an intermediate step between the image coordinates and 2D geospatial coordinates. The predictions are projected or \"splatted\" onto the mesh from each viewpoint.</p> <p> </p> <p> </p> <p>As seen above, each prediction only captures a small region of the mesh, and cannot make any predictions about parts of the object that were occluded in the original viewpoint. Therefore, we need to aggregate the predictions from all viewpoints to have an understanding of the entire scene. This gives us added robustness, because we can tolerate some prediction errors for a single viewpoint, by choosing the most common prediction across all viewpoints of a single location. The aggregated prediction is shown below.</p> <p> </p> <p>Now, the final step is to transform these predictions back into geospatial coordinates.</p> <p> </p>"},{"location":"cameras/","title":"Cameras","text":"<ul> <li> <p>Cameras Docstrings</p> </li> <li> <p>Derived Cameras Docstrings</p> </li> <li> <p>Segmentor Docstrings</p> </li> </ul>"},{"location":"cameras/cameras/","title":"Cameras Docstrings","text":""},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera","title":"<code>PhotogrammetryCamera</code>","text":"Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>class PhotogrammetryCamera:\n    def __init__(\n        self,\n        image_filename: PATH_TYPE,\n        cam_to_world_transform: np.ndarray,\n        f: float,\n        cx: float,\n        cy: float,\n        image_width: int,\n        image_height: int,\n        distortion_params: Dict[str, float] = {},\n        lon_lat: Union[None, Tuple[float, float]] = None,\n    ):\n        \"\"\"Represents the information about one camera location/image as determined by photogrammetry\n\n        Args:\n            image_filename (PATH_TYPE): The image used for reconstruction\n            transform (np.ndarray): A 4x4 transform representing the camera-to-world transform\n            f (float): Focal length in pixels\n            cx (float): Principle point x (pixels) from center\n            cy (float): Principle point y (pixels) from center\n            image_width (int): Input image width pixels\n            image_height (int): Input image height pixels\n            distortion_params (dict, optional): Distortion parameters, currently unused\n            lon_lat (Union[None, Tuple[float, float]], optional): Location, defaults to None\n        \"\"\"\n        self.image_filename = image_filename\n        self.cam_to_world_transform = cam_to_world_transform\n        self.world_to_cam_transform = np.linalg.inv(cam_to_world_transform)\n        self.f = f\n        self.cx = cx\n        self.cy = cy\n        self.image_width = image_width\n        self.image_height = image_height\n        self.distortion_params = distortion_params\n\n        if lon_lat is None:\n            self.lon_lat = (None, None)\n        else:\n            self.lon_lat = lon_lat\n\n        self.image_size = (image_height, image_width)\n        self.image = None\n        self.cache_image = (\n            False  # Only set to true if you can hold all images in memory\n        )\n\n    def get_camera_hash(self, include_image_hash: bool = False):\n        \"\"\"Generates a hash value for the camera's geometry and optionally includes the image\n\n        Args:\n            include_image_hash (bool, optional): Whether to include the image filename in the hash computation. Defaults to false.\n\n        Returns:\n            int: A hash value representing the current state of the camera\n        \"\"\"\n        # Geometric information of hash\n        transform_hash = self.cam_to_world_transform.tolist()\n        camera_settings = {\n            \"transform\": transform_hash,\n            \"f\": self.f,\n            \"cx\": self.cx,\n            \"cy\": self.cy,\n            \"image_width\": self.image_width,\n            \"image_height\": self.image_height,\n            \"distortion_params\": self.distortion_params,\n            \"lon_lat\": self.lon_lat,\n        }\n\n        # Include the image associated with the hash if specified\n        if include_image_hash:\n            camera_settings[\"image_filename\"] = str(self.image_filename)\n\n        camera_settings_data = json.dumps(camera_settings, sort_keys=True)\n        hasher = hashlib.sha256()\n        hasher.update(camera_settings_data.encode(\"utf-8\"))\n\n        return hasher.hexdigest()\n\n    def get_image(self, image_scale: float = 1.0) -&gt; np.ndarray:\n        # Check if the image is cached\n        if self.image is None:\n            image = imread(self.image_filename)\n            if image.dtype == np.uint8:\n                image = image / 255.0\n\n            # Avoid unneccesary read if we have memory\n            if self.cache_image:\n                self.image = image\n        else:\n            image = self.image\n\n        # Resizing is never cached, consider revisiting\n        if image_scale != 1.0:\n            image = resize(\n                image,\n                (int(image.shape[0] * image_scale), int(image.shape[1] * image_scale)),\n            )\n\n        return image\n\n    def get_image_filename(self):\n        return self.image_filename\n\n    def get_image_size(self, image_scale=1.0):\n        \"\"\"Return image size, potentially scaled\n\n        Args:\n            image_scale (float, optional): How much to scale by. Defaults to 1.0.\n\n        Returns:\n            tuple[int]: (h, w) in pixels\n        \"\"\"\n        # We should never have to deal with other cases if the reported size is accurate\n        if self.image_size is not None:\n            pass\n        elif self.image is not None:\n            self.image_size = self.image.shape[:2]\n        else:\n            image = self.get_image()\n            self.image_size = image.shape[:2]\n\n        return (\n            int(self.image_size[0] * image_scale),\n            int(self.image_size[1] * image_scale),\n        )\n\n    def get_lon_lat(self, negate_easting=True):\n        \"\"\"Return the lon, lat tuple, reading from exif metadata if neccessary\"\"\"\n        if None in self.lon_lat:\n            self.lon_lat = get_GPS_exif(self.image_filename)\n\n            if negate_easting:\n                self.lon_lat = (-self.lon_lat[0], self.lon_lat[1])\n\n        return self.lon_lat\n\n    def get_camera_location(self, get_z_coordinate: bool = False):\n        \"\"\"Returns a tuple of camera coordinates from the camera-to-world transfromation matrix.\n        Args:\n            get_z_coordinate (bool):\n                Flag that user can set if they want z-coordinates. Defaults to False.\n        Returns:\n            Tuple[float, float (, float)]: tuple containing internal mesh coordinates of the camera\n        \"\"\"\n        return (\n            tuple(self.cam_to_world_transform[0:3, 3])\n            if get_z_coordinate\n            else tuple(self.cam_to_world_transform[0:2, 3])\n        )\n\n    def check_projected_in_image(\n        self, homogenous_image_coords: np.ndarray, image_size: Tuple[int, int]\n    ):\n        \"\"\"Check if projected points are within the bound of the image and in front of camera\n\n        Args:\n            homogenous_image_coords (np.ndarray): The points after the application of K[R|t]. (3, n_points)\n            image_size (Tuple[int, int]): The size of the image (width, height) in pixels\n\n        Returns:\n            np.ndarray: valid_points_bool, boolean array corresponding to which points were valid (n_points)\n            np.ndarray: valid_image_space_points, float array of image-space coordinates for only valid points, (n_valid_points, 2)\n        \"\"\"\n        img_width, image_height = image_size\n\n        # Divide by the z coord to project onto the image plane\n        image_space_points = homogenous_image_coords[:2] / homogenous_image_coords[2:3]\n        # Transpose for convenience, (n_points, 3)\n        image_space_points = image_space_points.T\n\n        # We only want to consider points in front of the camera. Simple projection cannot tell\n        # if a point is on the same ray behind the camera\n        in_front_of_cam = homogenous_image_coords[2] &gt; 0\n\n        # Check that the point is projected within the image and is in front of the camera\n        # Pytorch doesn't have a logical_and.reduce operator, so this is the equivilent using boolean multiplication\n        valid_points_bool = (\n            (image_space_points[:, 0] &gt; 0)\n            * (image_space_points[:, 1] &gt; 0)\n            * (image_space_points[:, 0] &lt; img_width)\n            * (image_space_points[:, 1] &lt; image_height)\n            * in_front_of_cam\n        )\n\n        # Extract the points that are valid\n        valid_image_space_points = image_space_points[valid_points_bool, :].to(\n            torch.int\n        )\n        # Return the boolean array\n        valid_points_bool = valid_points_bool.cpu().numpy()\n        valid_image_space_points = valid_image_space_points.cpu().numpy\n        return valid_points_bool, valid_image_space_points\n\n    def extract_colors(\n        self, valid_bool: np.ndarray, valid_locs: np.ndarray, img: np.ndarray\n    ):\n        \"\"\"_summary_\n\n        Args:\n            valid_bool (np.ndarray): (n_points,) boolean array cooresponding to valid points\n            valid_locs (np.ndarray): (n_valid, 2) float array of image-space locations (x,y)\n            img (np.ndarray): (h, w, n_channels) image to query from\n\n        Returns:\n            np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n        \"\"\"\n        # Set up the data arrays\n        colors_per_vertex = np.zeros((valid_bool.shape[0], img.shape[2]))\n        mask = np.ones((valid_bool.shape[0], img.shape[2])).astype(bool)\n\n        # Set the entries which are valid to false, meaning a valid entry in the masked array\n        # TODO see if I can use valid_bool directly instead\n        valid_inds = np.where(valid_bool)[0]\n        mask[valid_inds, :] = False\n\n        # Extract coordinates\n        i_locs = valid_locs[:, 1]\n        j_locs = valid_locs[:, 0]\n        # Index based on the coordinates\n        valid_color_samples = img[i_locs, j_locs, :]\n        # Insert the valid samples into the array at the valid locations\n        colors_per_vertex[valid_inds, :] = valid_color_samples\n        # Convert to a masked array\n        masked_color_per_vertex = ma.array(colors_per_vertex, mask=mask)\n        return masked_color_per_vertex\n\n    def project_mesh_verts(self, mesh_verts: np.ndarray, img: np.ndarray, device: str):\n        \"\"\"Get a color per vertex using only projective geometry, without considering occlusion or distortion\n\n        Returns:\n            np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n        \"\"\"\n        # [R|t] matrix\n        transform_3x4_world_to_cam = torch.Tensor(\n            self.world_to_cam_transform[:3, :]\n        ).to(device)\n        K = torch.Tensor(\n            [\n                [self.f, 0, self.image_width / 2.0 + self.cx],\n                [0, self.f, self.image_width + self.cy],\n                [0, 0, 1],\n            ],\n            device=device,\n        )\n        # K[R|t], (3,4). Premultiplying these two matrices avoids doing two steps of projections with all points\n        camera_matrix = K @ transform_3x4_world_to_cam\n\n        # Add the extra dimension of ones for matrix multiplication\n        homogenous_mesh_verts = torch.concatenate(\n            (\n                torch.Tensor(mesh_verts).to(device),\n                torch.ones((mesh_verts.shape[0], 1)).to(device),\n            ),\n            axis=1,\n        ).T\n\n        # TODO review terminology\n        homogenous_camera_points = camera_matrix @ homogenous_mesh_verts\n        # Determine what points project onto the image and at what locations\n        valid_bool, valid_locs = self.check_projected_in_image(\n            projected_verts=homogenous_camera_points,\n            image_size=(self.image_width, self.image_height),\n        )\n        # Extract corresponding colors from the image\n        colors_per_vertex = self.extract_colors(valid_bool, valid_locs, img)\n\n        return colors_per_vertex\n\n    def get_pyvista_camera(self, focal_dist: float = 10) -&gt; pv.Camera:\n        \"\"\"\n        Get a pyvista camera at the location specified by photogrammetry.\n        Note that there is no principle point and only the vertical field of view is set\n\n        Args:\n            focal_dist (float, optional): How far away from the camera the center point should be. Defaults to 10.\n\n        Returns:\n            pv.Camera: The pyvista camera from that viewpoint.\n        \"\"\"\n        # Instantiate a new camera\n        camera = pv.Camera()\n        # Get the position as the translational part of the transform\n        camera_position = self.cam_to_world_transform[:3, 3]\n        # Get the look point by transforming a ray along the camera's Z axis into world\n        # coordinates and then adding this to the location\n        camera_look = camera_position + self.cam_to_world_transform[:3, :3] @ np.array(\n            (0, 0, focal_dist)\n        )\n        # Get the up direction of the camera by finding which direction the -Y (image up) vector is transformed to\n        camera_up = self.cam_to_world_transform[:3, :3] @ np.array((0, -1, 0))\n        # Compute the vertical field of view\n        vertical_FOV_angle = np.rad2deg(2 * np.arctan((self.image_height / 2) / self.f))\n\n        # Set the values\n        camera.focal_point = camera_look\n        camera.position = camera_position\n        camera.up = camera_up\n        camera.view_angle = vertical_FOV_angle\n\n        return camera\n\n    def get_pytorch3d_camera(self, device: str):\n        \"\"\"Return a pytorch3d camera based on the parameters from metashape\n\n        Args:\n            device (str): What device (cuda/cpu) to put the object on\n\n        Returns:\n            pytorch3d.renderer.PerspectiveCameras:\n        \"\"\"\n        rotation_about_z = np.array(\n            [[-1, 0, 0, 0], [0, -1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n        )\n        # Rotate about the Z axis because the NDC coordinates are defined X: left, Y: up and we use X: right, Y: down\n        # See https://pytorch3d.org/docs/cameras\n        transform_4x4_world_to_cam = rotation_about_z @ self.world_to_cam_transform\n\n        R = torch.Tensor(np.expand_dims(transform_4x4_world_to_cam[:3, :3].T, axis=0))\n        T = torch.Tensor(np.expand_dims(transform_4x4_world_to_cam[:3, 3], axis=0))\n\n        # The image size is (height, width) which completely disreguards any other conventions they use...\n        image_size = ((self.image_height, self.image_width),)\n        # These parameters are in screen (pixel) coordinates.\n        # TODO see if a normalized version is more robust for any reason\n        fcl_screen = (self.f,)\n        prc_points_screen = (\n            (self.image_width / 2 + self.cx, self.image_height / 2 + self.cy),\n        )\n\n        # Create camera\n        # TODO use the pytorch3d FishEyeCamera model that uses distortion\n        # https://pytorch3d.readthedocs.io/en/latest/modules/renderer/fisheyecameras.html?highlight=distortion\n        cameras = PerspectiveCameras(\n            R=R,\n            T=T,\n            focal_length=fcl_screen,\n            principal_point=prc_points_screen,\n            device=device,\n            in_ndc=False,  # screen coords\n            image_size=image_size,\n        )\n        return cameras\n\n    def vis(self, plotter: pv.Plotter = None, frustum_scale: float = 0.1):\n        \"\"\"\n        Visualize the camera as a frustum, at the appropriate translation and\n        rotation and with the given focal length and aspect ratio.\n\n\n        Args:\n            plotter (pv.Plotter): The plotter to add the visualization to\n            frustum_scale (float, optional): The length of the frustum in world units. Defaults to 0.5.\n        \"\"\"\n        scaled_halfwidth = self.image_width / (self.f * 2)\n        scaled_halfheight = self.image_height / (self.f * 2)\n\n        scaled_cx = self.cx / self.f\n        scaled_cy = self.cy / self.f\n\n        right = scaled_cx + scaled_halfwidth\n        left = scaled_cx - scaled_halfwidth\n        top = scaled_cy + scaled_halfheight\n        bottom = scaled_cy - scaled_halfheight\n\n        vertices = (\n            np.array(\n                [\n                    [0, 0, 0],\n                    [\n                        right,\n                        top,\n                        1,\n                    ],\n                    [\n                        right,\n                        bottom,\n                        1,\n                    ],\n                    [\n                        left,\n                        bottom,\n                        1,\n                    ],\n                    [\n                        left,\n                        top,\n                        1,\n                    ],\n                ]\n            ).T\n            * frustum_scale\n        )\n        # Make the coordinates homogenous\n        vertices = np.vstack((vertices, np.ones((1, 5))))\n\n        # Project the vertices into the world cordinates\n        projected_vertices = self.cam_to_world_transform @ vertices\n\n        # Deal with the case where there is a scale transform\n        if self.cam_to_world_transform[3, 3] != 1.0:\n            projected_vertices /= self.cam_to_world_transform[3, 3]\n\n        ## mesh faces\n        faces = np.hstack(\n            [\n                [3, 0, 1, 2],  # side\n                [3, 0, 2, 3],  # bottom\n                [3, 0, 3, 4],  # side\n                [3, 0, 4, 1],  # top\n                [3, 1, 2, 3],  # endcap tiangle #1\n                [3, 3, 4, 1],  # endcap tiangle #2\n            ]\n        )\n        # All blue except the top (-Y) surface is red\n        face_colors = np.array(\n            [[0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]]\n        ).astype(float)\n\n        # Create a mesh for the camera frustum\n        frustum = pv.PolyData(projected_vertices[:3].T, faces)\n        # Unsure exactly what's going on here, but it's required for it to be valid\n        frustum.triangulate()\n        # Show the mesh with the given face colors\n        # TODO understand how this understands it's face vs. vertex colors? Simply by checking the number of values?\n        plotter.add_mesh(frustum, scalars=face_colors, rgb=True)\n\n    def cast_rays(self, pixel_coords_ij: np.ndarray, line_length: float = 10):\n        \"\"\"Compute rays eminating from the camera\n\n        Args:\n            image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n            line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n\n        Returns:\n            np.array: The projected vertices, TODO\n        \"\"\"\n        # Transform from i, j to x, y\n        pixel_coords_xy = np.flip(pixel_coords_ij, axis=1)\n\n        # Cast a ray from the center of the mesh for vis\n        principal_point = np.array(\n            [[self.image_width / 2.0 + self.cx, self.image_height / 2.0 + self.cy]]\n        )\n        centered_pixel_coords = pixel_coords_xy - principal_point\n        scaled_pixel_coords = centered_pixel_coords / self.f\n\n        n_points = len(scaled_pixel_coords)\n\n        if n_points == 0:\n            return\n\n        line_verts = [\n            np.array(\n                [\n                    [0, 0, 0, 1],\n                    [\n                        point[0] * line_length,\n                        point[1] * line_length,\n                        line_length,\n                        1,\n                    ],\n                ]\n            )\n            for point in scaled_pixel_coords\n        ]\n        line_verts = np.concatenate(line_verts, axis=0).T\n\n        projected_vertices = self.cam_to_world_transform @ line_verts\n\n        # Handle scale in transform\n        if self.cam_to_world_transform[3, 3] != 1.0:\n            projected_vertices /= self.cam_to_world_transform[3, 3]\n\n        projected_vertices = projected_vertices[:3, :].T\n\n        return projected_vertices\n\n    def vis_rays(\n        self, pixel_coords_ij: np.ndarray, plotter: pv.Plotter, line_length: float = 10\n    ):\n        \"\"\"Show rays eminating from the camera\n\n        Args:\n            image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n            plotter (pv.Plotter): Plotter to use.\n            line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n        \"\"\"\n        # If there are no detections, just skip it\n        if len(pixel_coords_ij) == 0:\n            return\n\n        projected_vertices = self.cast_rays(\n            pixel_coords_ij=pixel_coords_ij, line_length=line_length\n        )\n        n_points = int(projected_vertices.shape[0] / 2)\n\n        lines = np.vstack(\n            (\n                np.full(n_points, fill_value=2),\n                np.arange(0, 2 * n_points, 2),\n                np.arange(0, 2 * n_points, 2) + 1,\n            )\n        ).T\n\n        mesh = pv.PolyData(projected_vertices.copy(), lines=lines.copy())\n        plotter.add_mesh(mesh)\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera-functions","title":"Functions","text":""},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.__init__","title":"<code>__init__(image_filename, cam_to_world_transform, f, cx, cy, image_width, image_height, distortion_params={}, lon_lat=None)</code>","text":"<p>Represents the information about one camera location/image as determined by photogrammetry</p> <p>Parameters:</p> Name Type Description Default <code>image_filename</code> <code>PATH_TYPE</code> <p>The image used for reconstruction</p> required <code>transform</code> <code>ndarray</code> <p>A 4x4 transform representing the camera-to-world transform</p> required <code>f</code> <code>float</code> <p>Focal length in pixels</p> required <code>cx</code> <code>float</code> <p>Principle point x (pixels) from center</p> required <code>cy</code> <code>float</code> <p>Principle point y (pixels) from center</p> required <code>image_width</code> <code>int</code> <p>Input image width pixels</p> required <code>image_height</code> <code>int</code> <p>Input image height pixels</p> required <code>distortion_params</code> <code>dict</code> <p>Distortion parameters, currently unused</p> <code>{}</code> <code>lon_lat</code> <code>Union[None, Tuple[float, float]]</code> <p>Location, defaults to None</p> <code>None</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def __init__(\n    self,\n    image_filename: PATH_TYPE,\n    cam_to_world_transform: np.ndarray,\n    f: float,\n    cx: float,\n    cy: float,\n    image_width: int,\n    image_height: int,\n    distortion_params: Dict[str, float] = {},\n    lon_lat: Union[None, Tuple[float, float]] = None,\n):\n    \"\"\"Represents the information about one camera location/image as determined by photogrammetry\n\n    Args:\n        image_filename (PATH_TYPE): The image used for reconstruction\n        transform (np.ndarray): A 4x4 transform representing the camera-to-world transform\n        f (float): Focal length in pixels\n        cx (float): Principle point x (pixels) from center\n        cy (float): Principle point y (pixels) from center\n        image_width (int): Input image width pixels\n        image_height (int): Input image height pixels\n        distortion_params (dict, optional): Distortion parameters, currently unused\n        lon_lat (Union[None, Tuple[float, float]], optional): Location, defaults to None\n    \"\"\"\n    self.image_filename = image_filename\n    self.cam_to_world_transform = cam_to_world_transform\n    self.world_to_cam_transform = np.linalg.inv(cam_to_world_transform)\n    self.f = f\n    self.cx = cx\n    self.cy = cy\n    self.image_width = image_width\n    self.image_height = image_height\n    self.distortion_params = distortion_params\n\n    if lon_lat is None:\n        self.lon_lat = (None, None)\n    else:\n        self.lon_lat = lon_lat\n\n    self.image_size = (image_height, image_width)\n    self.image = None\n    self.cache_image = (\n        False  # Only set to true if you can hold all images in memory\n    )\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.cast_rays","title":"<code>cast_rays(pixel_coords_ij, line_length=10)</code>","text":"<p>Compute rays eminating from the camera</p> <p>Parameters:</p> Name Type Description Default <code>image_coords</code> <code>ndarray</code> <p>(n,2) array of (i,j) pixel coordinates in the image</p> required <code>line_length</code> <code>float</code> <p>How long the lines are. Defaults to 10. #TODO allow an array of different values</p> <code>10</code> <p>Returns:</p> Type Description <p>np.array: The projected vertices, TODO</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def cast_rays(self, pixel_coords_ij: np.ndarray, line_length: float = 10):\n    \"\"\"Compute rays eminating from the camera\n\n    Args:\n        image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n        line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n\n    Returns:\n        np.array: The projected vertices, TODO\n    \"\"\"\n    # Transform from i, j to x, y\n    pixel_coords_xy = np.flip(pixel_coords_ij, axis=1)\n\n    # Cast a ray from the center of the mesh for vis\n    principal_point = np.array(\n        [[self.image_width / 2.0 + self.cx, self.image_height / 2.0 + self.cy]]\n    )\n    centered_pixel_coords = pixel_coords_xy - principal_point\n    scaled_pixel_coords = centered_pixel_coords / self.f\n\n    n_points = len(scaled_pixel_coords)\n\n    if n_points == 0:\n        return\n\n    line_verts = [\n        np.array(\n            [\n                [0, 0, 0, 1],\n                [\n                    point[0] * line_length,\n                    point[1] * line_length,\n                    line_length,\n                    1,\n                ],\n            ]\n        )\n        for point in scaled_pixel_coords\n    ]\n    line_verts = np.concatenate(line_verts, axis=0).T\n\n    projected_vertices = self.cam_to_world_transform @ line_verts\n\n    # Handle scale in transform\n    if self.cam_to_world_transform[3, 3] != 1.0:\n        projected_vertices /= self.cam_to_world_transform[3, 3]\n\n    projected_vertices = projected_vertices[:3, :].T\n\n    return projected_vertices\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.check_projected_in_image","title":"<code>check_projected_in_image(homogenous_image_coords, image_size)</code>","text":"<p>Check if projected points are within the bound of the image and in front of camera</p> <p>Parameters:</p> Name Type Description Default <code>homogenous_image_coords</code> <code>ndarray</code> <p>The points after the application of K[R|t]. (3, n_points)</p> required <code>image_size</code> <code>Tuple[int, int]</code> <p>The size of the image (width, height) in pixels</p> required <p>Returns:</p> Type Description <p>np.ndarray: valid_points_bool, boolean array corresponding to which points were valid (n_points)</p> <p>np.ndarray: valid_image_space_points, float array of image-space coordinates for only valid points, (n_valid_points, 2)</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def check_projected_in_image(\n    self, homogenous_image_coords: np.ndarray, image_size: Tuple[int, int]\n):\n    \"\"\"Check if projected points are within the bound of the image and in front of camera\n\n    Args:\n        homogenous_image_coords (np.ndarray): The points after the application of K[R|t]. (3, n_points)\n        image_size (Tuple[int, int]): The size of the image (width, height) in pixels\n\n    Returns:\n        np.ndarray: valid_points_bool, boolean array corresponding to which points were valid (n_points)\n        np.ndarray: valid_image_space_points, float array of image-space coordinates for only valid points, (n_valid_points, 2)\n    \"\"\"\n    img_width, image_height = image_size\n\n    # Divide by the z coord to project onto the image plane\n    image_space_points = homogenous_image_coords[:2] / homogenous_image_coords[2:3]\n    # Transpose for convenience, (n_points, 3)\n    image_space_points = image_space_points.T\n\n    # We only want to consider points in front of the camera. Simple projection cannot tell\n    # if a point is on the same ray behind the camera\n    in_front_of_cam = homogenous_image_coords[2] &gt; 0\n\n    # Check that the point is projected within the image and is in front of the camera\n    # Pytorch doesn't have a logical_and.reduce operator, so this is the equivilent using boolean multiplication\n    valid_points_bool = (\n        (image_space_points[:, 0] &gt; 0)\n        * (image_space_points[:, 1] &gt; 0)\n        * (image_space_points[:, 0] &lt; img_width)\n        * (image_space_points[:, 1] &lt; image_height)\n        * in_front_of_cam\n    )\n\n    # Extract the points that are valid\n    valid_image_space_points = image_space_points[valid_points_bool, :].to(\n        torch.int\n    )\n    # Return the boolean array\n    valid_points_bool = valid_points_bool.cpu().numpy()\n    valid_image_space_points = valid_image_space_points.cpu().numpy\n    return valid_points_bool, valid_image_space_points\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.extract_colors","title":"<code>extract_colors(valid_bool, valid_locs, img)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>valid_bool</code> <code>ndarray</code> <p>(n_points,) boolean array cooresponding to valid points</p> required <code>valid_locs</code> <code>ndarray</code> <p>(n_valid, 2) float array of image-space locations (x,y)</p> required <code>img</code> <code>ndarray</code> <p>(h, w, n_channels) image to query from</p> required <p>Returns:</p> Type Description <p>np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def extract_colors(\n    self, valid_bool: np.ndarray, valid_locs: np.ndarray, img: np.ndarray\n):\n    \"\"\"_summary_\n\n    Args:\n        valid_bool (np.ndarray): (n_points,) boolean array cooresponding to valid points\n        valid_locs (np.ndarray): (n_valid, 2) float array of image-space locations (x,y)\n        img (np.ndarray): (h, w, n_channels) image to query from\n\n    Returns:\n        np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n    \"\"\"\n    # Set up the data arrays\n    colors_per_vertex = np.zeros((valid_bool.shape[0], img.shape[2]))\n    mask = np.ones((valid_bool.shape[0], img.shape[2])).astype(bool)\n\n    # Set the entries which are valid to false, meaning a valid entry in the masked array\n    # TODO see if I can use valid_bool directly instead\n    valid_inds = np.where(valid_bool)[0]\n    mask[valid_inds, :] = False\n\n    # Extract coordinates\n    i_locs = valid_locs[:, 1]\n    j_locs = valid_locs[:, 0]\n    # Index based on the coordinates\n    valid_color_samples = img[i_locs, j_locs, :]\n    # Insert the valid samples into the array at the valid locations\n    colors_per_vertex[valid_inds, :] = valid_color_samples\n    # Convert to a masked array\n    masked_color_per_vertex = ma.array(colors_per_vertex, mask=mask)\n    return masked_color_per_vertex\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_hash","title":"<code>get_camera_hash(include_image_hash=False)</code>","text":"<p>Generates a hash value for the camera's geometry and optionally includes the image</p> <p>Parameters:</p> Name Type Description Default <code>include_image_hash</code> <code>bool</code> <p>Whether to include the image filename in the hash computation. Defaults to false.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>int</code> <p>A hash value representing the current state of the camera</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_hash(self, include_image_hash: bool = False):\n    \"\"\"Generates a hash value for the camera's geometry and optionally includes the image\n\n    Args:\n        include_image_hash (bool, optional): Whether to include the image filename in the hash computation. Defaults to false.\n\n    Returns:\n        int: A hash value representing the current state of the camera\n    \"\"\"\n    # Geometric information of hash\n    transform_hash = self.cam_to_world_transform.tolist()\n    camera_settings = {\n        \"transform\": transform_hash,\n        \"f\": self.f,\n        \"cx\": self.cx,\n        \"cy\": self.cy,\n        \"image_width\": self.image_width,\n        \"image_height\": self.image_height,\n        \"distortion_params\": self.distortion_params,\n        \"lon_lat\": self.lon_lat,\n    }\n\n    # Include the image associated with the hash if specified\n    if include_image_hash:\n        camera_settings[\"image_filename\"] = str(self.image_filename)\n\n    camera_settings_data = json.dumps(camera_settings, sort_keys=True)\n    hasher = hashlib.sha256()\n    hasher.update(camera_settings_data.encode(\"utf-8\"))\n\n    return hasher.hexdigest()\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_camera_location","title":"<code>get_camera_location(get_z_coordinate=False)</code>","text":"<p>Returns a tuple of camera coordinates from the camera-to-world transfromation matrix. Args:     get_z_coordinate (bool):         Flag that user can set if they want z-coordinates. Defaults to False. Returns:     Tuple[float, float (, float)]: tuple containing internal mesh coordinates of the camera</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_location(self, get_z_coordinate: bool = False):\n    \"\"\"Returns a tuple of camera coordinates from the camera-to-world transfromation matrix.\n    Args:\n        get_z_coordinate (bool):\n            Flag that user can set if they want z-coordinates. Defaults to False.\n    Returns:\n        Tuple[float, float (, float)]: tuple containing internal mesh coordinates of the camera\n    \"\"\"\n    return (\n        tuple(self.cam_to_world_transform[0:3, 3])\n        if get_z_coordinate\n        else tuple(self.cam_to_world_transform[0:2, 3])\n    )\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_image_size","title":"<code>get_image_size(image_scale=1.0)</code>","text":"<p>Return image size, potentially scaled</p> <p>Parameters:</p> Name Type Description Default <code>image_scale</code> <code>float</code> <p>How much to scale by. Defaults to 1.0.</p> <code>1.0</code> <p>Returns:</p> Type Description <p>tuple[int]: (h, w) in pixels</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_image_size(self, image_scale=1.0):\n    \"\"\"Return image size, potentially scaled\n\n    Args:\n        image_scale (float, optional): How much to scale by. Defaults to 1.0.\n\n    Returns:\n        tuple[int]: (h, w) in pixels\n    \"\"\"\n    # We should never have to deal with other cases if the reported size is accurate\n    if self.image_size is not None:\n        pass\n    elif self.image is not None:\n        self.image_size = self.image.shape[:2]\n    else:\n        image = self.get_image()\n        self.image_size = image.shape[:2]\n\n    return (\n        int(self.image_size[0] * image_scale),\n        int(self.image_size[1] * image_scale),\n    )\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_lon_lat","title":"<code>get_lon_lat(negate_easting=True)</code>","text":"<p>Return the lon, lat tuple, reading from exif metadata if neccessary</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_lon_lat(self, negate_easting=True):\n    \"\"\"Return the lon, lat tuple, reading from exif metadata if neccessary\"\"\"\n    if None in self.lon_lat:\n        self.lon_lat = get_GPS_exif(self.image_filename)\n\n        if negate_easting:\n            self.lon_lat = (-self.lon_lat[0], self.lon_lat[1])\n\n    return self.lon_lat\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_pytorch3d_camera","title":"<code>get_pytorch3d_camera(device)</code>","text":"<p>Return a pytorch3d camera based on the parameters from metashape</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>What device (cuda/cpu) to put the object on</p> required <p>Returns:</p> Type Description <p>pytorch3d.renderer.PerspectiveCameras:</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_pytorch3d_camera(self, device: str):\n    \"\"\"Return a pytorch3d camera based on the parameters from metashape\n\n    Args:\n        device (str): What device (cuda/cpu) to put the object on\n\n    Returns:\n        pytorch3d.renderer.PerspectiveCameras:\n    \"\"\"\n    rotation_about_z = np.array(\n        [[-1, 0, 0, 0], [0, -1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]\n    )\n    # Rotate about the Z axis because the NDC coordinates are defined X: left, Y: up and we use X: right, Y: down\n    # See https://pytorch3d.org/docs/cameras\n    transform_4x4_world_to_cam = rotation_about_z @ self.world_to_cam_transform\n\n    R = torch.Tensor(np.expand_dims(transform_4x4_world_to_cam[:3, :3].T, axis=0))\n    T = torch.Tensor(np.expand_dims(transform_4x4_world_to_cam[:3, 3], axis=0))\n\n    # The image size is (height, width) which completely disreguards any other conventions they use...\n    image_size = ((self.image_height, self.image_width),)\n    # These parameters are in screen (pixel) coordinates.\n    # TODO see if a normalized version is more robust for any reason\n    fcl_screen = (self.f,)\n    prc_points_screen = (\n        (self.image_width / 2 + self.cx, self.image_height / 2 + self.cy),\n    )\n\n    # Create camera\n    # TODO use the pytorch3d FishEyeCamera model that uses distortion\n    # https://pytorch3d.readthedocs.io/en/latest/modules/renderer/fisheyecameras.html?highlight=distortion\n    cameras = PerspectiveCameras(\n        R=R,\n        T=T,\n        focal_length=fcl_screen,\n        principal_point=prc_points_screen,\n        device=device,\n        in_ndc=False,  # screen coords\n        image_size=image_size,\n    )\n    return cameras\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.get_pyvista_camera","title":"<code>get_pyvista_camera(focal_dist=10)</code>","text":"<p>Get a pyvista camera at the location specified by photogrammetry. Note that there is no principle point and only the vertical field of view is set</p> <p>Parameters:</p> Name Type Description Default <code>focal_dist</code> <code>float</code> <p>How far away from the camera the center point should be. Defaults to 10.</p> <code>10</code> <p>Returns:</p> Type Description <code>Camera</code> <p>pv.Camera: The pyvista camera from that viewpoint.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_pyvista_camera(self, focal_dist: float = 10) -&gt; pv.Camera:\n    \"\"\"\n    Get a pyvista camera at the location specified by photogrammetry.\n    Note that there is no principle point and only the vertical field of view is set\n\n    Args:\n        focal_dist (float, optional): How far away from the camera the center point should be. Defaults to 10.\n\n    Returns:\n        pv.Camera: The pyvista camera from that viewpoint.\n    \"\"\"\n    # Instantiate a new camera\n    camera = pv.Camera()\n    # Get the position as the translational part of the transform\n    camera_position = self.cam_to_world_transform[:3, 3]\n    # Get the look point by transforming a ray along the camera's Z axis into world\n    # coordinates and then adding this to the location\n    camera_look = camera_position + self.cam_to_world_transform[:3, :3] @ np.array(\n        (0, 0, focal_dist)\n    )\n    # Get the up direction of the camera by finding which direction the -Y (image up) vector is transformed to\n    camera_up = self.cam_to_world_transform[:3, :3] @ np.array((0, -1, 0))\n    # Compute the vertical field of view\n    vertical_FOV_angle = np.rad2deg(2 * np.arctan((self.image_height / 2) / self.f))\n\n    # Set the values\n    camera.focal_point = camera_look\n    camera.position = camera_position\n    camera.up = camera_up\n    camera.view_angle = vertical_FOV_angle\n\n    return camera\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.project_mesh_verts","title":"<code>project_mesh_verts(mesh_verts, img, device)</code>","text":"<p>Get a color per vertex using only projective geometry, without considering occlusion or distortion</p> <p>Returns:</p> Type Description <p>np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def project_mesh_verts(self, mesh_verts: np.ndarray, img: np.ndarray, device: str):\n    \"\"\"Get a color per vertex using only projective geometry, without considering occlusion or distortion\n\n    Returns:\n        np.ma.array: (n_points, n_channels) One color per valid vertex. Points that were invalid are masked out\n    \"\"\"\n    # [R|t] matrix\n    transform_3x4_world_to_cam = torch.Tensor(\n        self.world_to_cam_transform[:3, :]\n    ).to(device)\n    K = torch.Tensor(\n        [\n            [self.f, 0, self.image_width / 2.0 + self.cx],\n            [0, self.f, self.image_width + self.cy],\n            [0, 0, 1],\n        ],\n        device=device,\n    )\n    # K[R|t], (3,4). Premultiplying these two matrices avoids doing two steps of projections with all points\n    camera_matrix = K @ transform_3x4_world_to_cam\n\n    # Add the extra dimension of ones for matrix multiplication\n    homogenous_mesh_verts = torch.concatenate(\n        (\n            torch.Tensor(mesh_verts).to(device),\n            torch.ones((mesh_verts.shape[0], 1)).to(device),\n        ),\n        axis=1,\n    ).T\n\n    # TODO review terminology\n    homogenous_camera_points = camera_matrix @ homogenous_mesh_verts\n    # Determine what points project onto the image and at what locations\n    valid_bool, valid_locs = self.check_projected_in_image(\n        projected_verts=homogenous_camera_points,\n        image_size=(self.image_width, self.image_height),\n    )\n    # Extract corresponding colors from the image\n    colors_per_vertex = self.extract_colors(valid_bool, valid_locs, img)\n\n    return colors_per_vertex\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.vis","title":"<code>vis(plotter=None, frustum_scale=0.1)</code>","text":"<p>Visualize the camera as a frustum, at the appropriate translation and rotation and with the given focal length and aspect ratio.</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>The plotter to add the visualization to</p> <code>None</code> <code>frustum_scale</code> <code>float</code> <p>The length of the frustum in world units. Defaults to 0.5.</p> <code>0.1</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def vis(self, plotter: pv.Plotter = None, frustum_scale: float = 0.1):\n    \"\"\"\n    Visualize the camera as a frustum, at the appropriate translation and\n    rotation and with the given focal length and aspect ratio.\n\n\n    Args:\n        plotter (pv.Plotter): The plotter to add the visualization to\n        frustum_scale (float, optional): The length of the frustum in world units. Defaults to 0.5.\n    \"\"\"\n    scaled_halfwidth = self.image_width / (self.f * 2)\n    scaled_halfheight = self.image_height / (self.f * 2)\n\n    scaled_cx = self.cx / self.f\n    scaled_cy = self.cy / self.f\n\n    right = scaled_cx + scaled_halfwidth\n    left = scaled_cx - scaled_halfwidth\n    top = scaled_cy + scaled_halfheight\n    bottom = scaled_cy - scaled_halfheight\n\n    vertices = (\n        np.array(\n            [\n                [0, 0, 0],\n                [\n                    right,\n                    top,\n                    1,\n                ],\n                [\n                    right,\n                    bottom,\n                    1,\n                ],\n                [\n                    left,\n                    bottom,\n                    1,\n                ],\n                [\n                    left,\n                    top,\n                    1,\n                ],\n            ]\n        ).T\n        * frustum_scale\n    )\n    # Make the coordinates homogenous\n    vertices = np.vstack((vertices, np.ones((1, 5))))\n\n    # Project the vertices into the world cordinates\n    projected_vertices = self.cam_to_world_transform @ vertices\n\n    # Deal with the case where there is a scale transform\n    if self.cam_to_world_transform[3, 3] != 1.0:\n        projected_vertices /= self.cam_to_world_transform[3, 3]\n\n    ## mesh faces\n    faces = np.hstack(\n        [\n            [3, 0, 1, 2],  # side\n            [3, 0, 2, 3],  # bottom\n            [3, 0, 3, 4],  # side\n            [3, 0, 4, 1],  # top\n            [3, 1, 2, 3],  # endcap tiangle #1\n            [3, 3, 4, 1],  # endcap tiangle #2\n        ]\n    )\n    # All blue except the top (-Y) surface is red\n    face_colors = np.array(\n        [[0, 0, 1], [1, 0, 0], [0, 0, 1], [0, 0, 1], [0, 0, 1], [0, 0, 1]]\n    ).astype(float)\n\n    # Create a mesh for the camera frustum\n    frustum = pv.PolyData(projected_vertices[:3].T, faces)\n    # Unsure exactly what's going on here, but it's required for it to be valid\n    frustum.triangulate()\n    # Show the mesh with the given face colors\n    # TODO understand how this understands it's face vs. vertex colors? Simply by checking the number of values?\n    plotter.add_mesh(frustum, scalars=face_colors, rgb=True)\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCamera.vis_rays","title":"<code>vis_rays(pixel_coords_ij, plotter, line_length=10)</code>","text":"<p>Show rays eminating from the camera</p> <p>Parameters:</p> Name Type Description Default <code>image_coords</code> <code>ndarray</code> <p>(n,2) array of (i,j) pixel coordinates in the image</p> required <code>plotter</code> <code>Plotter</code> <p>Plotter to use.</p> required <code>line_length</code> <code>float</code> <p>How long the lines are. Defaults to 10. #TODO allow an array of different values</p> <code>10</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def vis_rays(\n    self, pixel_coords_ij: np.ndarray, plotter: pv.Plotter, line_length: float = 10\n):\n    \"\"\"Show rays eminating from the camera\n\n    Args:\n        image_coords (np.ndarray): (n,2) array of (i,j) pixel coordinates in the image\n        plotter (pv.Plotter): Plotter to use.\n        line_length (float, optional): How long the lines are. Defaults to 10. #TODO allow an array of different values\n    \"\"\"\n    # If there are no detections, just skip it\n    if len(pixel_coords_ij) == 0:\n        return\n\n    projected_vertices = self.cast_rays(\n        pixel_coords_ij=pixel_coords_ij, line_length=line_length\n    )\n    n_points = int(projected_vertices.shape[0] / 2)\n\n    lines = np.vstack(\n        (\n            np.full(n_points, fill_value=2),\n            np.arange(0, 2 * n_points, 2),\n            np.arange(0, 2 * n_points, 2) + 1,\n        )\n    ).T\n\n    mesh = pv.PolyData(projected_vertices.copy(), lines=lines.copy())\n    plotter.add_mesh(mesh)\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet","title":"<code>PhotogrammetryCameraSet</code>","text":"Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>class PhotogrammetryCameraSet:\n    def __init__(\n        self,\n        cameras: Union[None, PhotogrammetryCamera, List[PhotogrammetryCamera]] = None,\n        cam_to_world_transforms: Union[None, List[np.ndarray]] = None,\n        intrinsic_params_per_sensor_type: Dict[int, Dict[str, float]] = {\n            0: EXAMPLE_INTRINSICS\n        },\n        image_filenames: Union[List[PATH_TYPE], None] = None,\n        lon_lats: Union[None, List[Union[None, Tuple[float, float]]]] = None,\n        image_folder: PATH_TYPE = None,\n        sensor_IDs: List[int] = None,\n        validate_images: bool = False,\n    ):\n        \"\"\"_summary_\n\n        Args:\n            cam_to_world_transforms (List[np.ndarray]): The list of 4x4 camera to world transforms\n            intrinsic_params_per_sensor (Dict[int, Dict]): A dictionary mapping from an int camera ID to the intrinsic parameters\n            image_filenames (List[PATH_TYPE]): The list of image filenames, ideally absolute paths\n            lon_lats (Union[None, List[Union[None, Tuple[float, float]]]]): A list of lon,lat tuples, or list of Nones, or None\n            image_folder (PATH_TYPE): The top level folder of the images\n            sensor_IDs (List[int]): The list of sensor IDs, that index into the sensors_params_dict\n            validate_images (bool, optional): Should the existance of the images be checked. Defaults to False.\n\n        Raises:\n            ValueError: _description_\n        \"\"\"\n        # Create an object using the supplied cameras\n        if cameras is not None:\n            if isinstance(cameras, PhotogrammetryCamera):\n                cameras = [cameras]\n            self.cameras = cameras\n            return\n\n        # Standardization\n        n_transforms = len(cam_to_world_transforms)\n\n        # Create list of Nones for image filenames if not set\n        if image_filenames is None:\n            image_filenames = [None] * n_transforms\n\n        if sensor_IDs is None and len(intrinsic_params_per_sensor_type) == 1:\n            # Create a list of the only index if not set\n            sensor_IDs = [\n                list(intrinsic_params_per_sensor_type.keys())[0]\n            ] * n_transforms\n        elif len(sensor_IDs) != n_transforms:\n            raise ValueError(\n                f\"Number of sensor_IDs ({len(sensor_IDs)}) is different than the number of transforms ({n_transforms})\"\n            )\n\n        # If lon lats is None, set it to a list of Nones per transform\n        if lon_lats is None:\n            lon_lats = [None] * n_transforms\n\n        if image_folder is None:\n            # TODO set it to the least common ancestor of all filenames\n            pass\n\n        # Record the values\n        # TODO see if we ever use these\n        self.cam_to_world_transforms = cam_to_world_transforms\n        self.intrinsic_params_per_sensor_type = intrinsic_params_per_sensor_type\n        self.image_filenames = image_filenames\n        self.lon_lats = lon_lats\n        self.sensor_IDs = sensor_IDs\n        self.image_folder = image_folder\n\n        if validate_images:\n            missing_images, invalid_images = self.find_mising_images()\n            if len(missing_images) &gt; 0:\n                print(f\"Deleting {len(missing_images)} missing images\")\n                valid_images = np.where(np.logical_not(invalid_images))[0]\n                self.image_filenames = np.array(self.image_filenames)[\n                    valid_images\n                ].tolist()\n                # Avoid calling .tolist() because this will recursively set all elements to lists\n                # when this should be a list of np.arrays\n                self.cam_to_world_transforms = [\n                    x for x in np.array(self.cam_to_world_transforms)[valid_images]\n                ]\n                self.sensor_IDs = np.array(self.sensor_IDs)[valid_images].tolist()\n                self.lon_lats = np.array(self.lon_lats)[valid_images].tolist()\n\n        self.cameras = []\n\n        for image_filename, cam_to_world_transform, sensor_ID, lon_lat in zip(\n            self.image_filenames,\n            self.cam_to_world_transforms,\n            self.sensor_IDs,\n            self.lon_lats,\n        ):\n            sensor_params = self.intrinsic_params_per_sensor_type[sensor_ID]\n            new_camera = PhotogrammetryCamera(\n                image_filename, cam_to_world_transform, lon_lat=lon_lat, **sensor_params\n            )\n            self.cameras.append(new_camera)\n\n    def __len__(self):\n        return self.n_cameras()\n\n    def __getitem__(self, slice):\n        subset_cameras = self.cameras[slice]\n        if isinstance(subset_cameras, PhotogrammetryCamera):\n            # this is just one item indexed\n            return subset_cameras\n        # else, wrap the list of cameras in a CameraSet\n        return PhotogrammetryCameraSet(subset_cameras)\n\n    def get_image_folder(self):\n        return self.image_folder\n\n    def find_mising_images(self):\n        invalid_mask = []\n        for image_file in self.image_filenames:\n            if not image_file.is_file():\n                invalid_mask.append(True)\n            else:\n                invalid_mask.append(False)\n        invalid_images = np.array(self.image_filenames)[np.array(invalid_mask)].tolist()\n\n        return invalid_images, invalid_mask\n\n    def n_cameras(self) -&gt; int:\n        \"\"\"Return the number of cameras\"\"\"\n        return len(self.cameras)\n\n    def n_image_channels(self) -&gt; int:\n        \"\"\"Return the number of channels in the image\"\"\"\n        return 3\n\n    def get_cameras_in_folder(self, folder: PATH_TYPE):\n        \"\"\"Return the camera set with cameras corresponding to images in that folder\n\n        Args:\n            folder (PATH_TYPE): The folder location\n\n        Returns:\n            PhotogrammetryCameraSet: A copy of the camera set with only the cameras from that folder\n        \"\"\"\n        # Get the inds where that camera is in the folder\n        imgs_in_folder_inds = [\n            i\n            for i in range(len(self.cameras))\n            if self.cameras[i].image_filename.is_relative_to(folder)\n        ]\n        # Return the PhotogrammetryCameraSet with those subset of cameras\n        subset_cameras = self.get_subset_cameras(imgs_in_folder_inds)\n        return subset_cameras\n\n    def get_subset_cameras(self, inds: List[int]):\n        subset_camera_set = deepcopy(self)\n        subset_camera_set.cameras = [subset_camera_set[i] for i in inds]\n        return subset_camera_set\n\n    def get_image_by_index(self, index: int, image_scale: float = 1.0) -&gt; np.ndarray:\n        return self[index].get_image(image_scale=image_scale)\n\n    def get_image_filename(self, index: int, absolute=True):\n        filename = self.cameras[index].get_image_filename()\n        if absolute:\n            return Path(filename)\n        else:\n            return Path(filename).relative_to(self.get_image_folder())\n\n    def get_pytorch3d_camera(self, device: str):\n        \"\"\"\n        Return a pytorch3d cameras object based on the parameters from metashape.\n        This has the information from each of the camears in the set to enabled batched rendering.\n\n\n        Args:\n            device (str): What device (cuda/cpu) to put the object on\n\n        Returns:\n            pytorch3d.renderer.PerspectiveCameras:\n        \"\"\"\n        # Get the pytorch3d cameras for each of the cameras in the set\n        p3d_cameras = [camera.get_pytorch3d_camera(device) for camera in self.cameras]\n        # Get the image sizes\n        image_sizes = [camera.image_size.cpu().numpy() for camera in p3d_cameras]\n        # Check that all the image sizes are the same because this is required for proper batched rendering\n        if np.any([image_size != image_sizes[0] for image_size in image_sizes]):\n            raise ValueError(\"Not all cameras have the same image size\")\n        # Create the new pytorch3d cameras object with the information from each camera\n        cameras = PerspectiveCameras(\n            R=torch.cat([camera.R for camera in p3d_cameras], 0),\n            T=torch.cat([camera.T for camera in p3d_cameras], 0),\n            focal_length=torch.cat([camera.focal_length for camera in p3d_cameras], 0),\n            principal_point=torch.cat(\n                [camera.get_principal_point() for camera in p3d_cameras], 0\n            ),\n            device=device,\n            in_ndc=False,  # screen coords\n            image_size=image_sizes[0],\n        )\n        return cameras\n\n    def save_images(self, output_folder, copy=False, remove_folder: bool = True):\n        if remove_folder:\n            if os.path.isdir(output_folder):\n                print(f\"about to remove {output_folder}\")\n                shutil.rmtree(output_folder)\n\n        for i in tqdm(\n            range(len(self.cameras)),\n            f\"{'copying' if copy else 'linking'} images to {output_folder}\",\n        ):\n            output_file = Path(\n                output_folder, self.get_image_filename(i, absolute=False)\n            )\n            ensure_containing_folder(output_file)\n            src_file = self.get_image_filename(i, absolute=True)\n            if copy:\n                try:\n                    shutil.copy(src_file, output_file)\n                except FileNotFoundError:\n                    logging.warning(f\"Could not find {src_file}\")\n            else:\n                os.symlink(src_file, output_file)\n\n    def get_lon_lat_coords(self):\n        \"\"\"Returns a list of GPS coords for each camera\"\"\"\n        return [x.get_lon_lat() for x in self.cameras]\n\n    def get_camera_locations(self, **kwargs):\n        \"\"\"\n        Returns a list of camera locations for each camera.\n\n        Args:\n            **kwargs: Keyword arguments to be passed to the PhotogrammetryCamera.get_camera_location method.\n\n        Returns:\n            List[Tuple[float, float] or Tuple[float, float, float]]:\n                List of tuples containing the camera locations.\n        \"\"\"\n        return [x.get_camera_location(**kwargs) for x in self.cameras]\n\n    def get_subset_ROI(\n        self,\n        ROI: Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon],\n        buffer_radius: float = 0,\n        is_geospatial: bool = None,\n    ):\n        \"\"\"Return cameras that are within a radius of the provided geometry\n\n        Args:\n            geodata (Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon]):\n                This can be a Geopandas dataframe, path to a geofile readable by geopandas, or\n                Shapely Polygon/MultiPolygon information that can be loaded into a geodataframe\n            buffer_radius (float, optional):\n                Return points within this buffer of the geometry. Defaults to 0. Represents\n                meters if ROI is geospatial.\n            is_geospatial (bool, optional):\n                A flag for user to indicate if ROI is geospatial or not; if no flag is provided,\n                the flag is set if the provided geodata has a CRS.\n        Returns:\n            subset_camera_set (List[PhotogrammetryCamera]):\n                List of cameras that fall within the provided ROI\n        \"\"\"\n        # construct GeoDataFrame if not provided\n        if isinstance(ROI, (Polygon, MultiPolygon)):\n            # assume geodata is lat/lon if is_geospatial is True\n            if is_geospatial:\n                ROI = gpd.GeoDataFrame(crs=LAT_LON_EPSG_CODE, geometry=[ROI])\n            else:\n                ROI = gpd.GeoDataFrame(geometry=[ROI])\n        elif not isinstance(ROI, gpd.GeoDataFrame):\n            # Read in the geofile\n            ROI = gpd.read_file(ROI)\n\n        if is_geospatial is None:\n            is_geospatial = ROI.crs is not None\n\n        if not is_geospatial:\n            # get internal coordinate system camera locations\n            image_locations = [Point(*x) for x in self.get_camera_locations()]\n            image_locations_df = gpd.GeoDataFrame(geometry=image_locations)\n        else:\n            # Make sure it's a geometric (meters-based) CRS\n            ROI = ensure_projected_CRS(ROI)\n            # Read the locations of all the points\n            image_locations = [Point(*x) for x in self.get_lon_lat_coords()]\n            # Create a dataframe, assuming inputs are lat lon\n            image_locations_df = gpd.GeoDataFrame(\n                geometry=image_locations, crs=LAT_LON_EPSG_CODE\n            )\n            image_locations_df.to_crs(ROI.crs, inplace=True)\n\n        # Merge all of the elements together into one multipolygon, destroying any attributes that were there\n        ROI = ROI.dissolve()\n        # Expand the geometry of the shape by the buffer\n        ROI[\"geometry\"] = ROI.buffer(buffer_radius)\n        image_locations_df[\"index\"] = image_locations_df.index\n\n        points_in_field_buffer = gpd.sjoin(\n            image_locations_df, ROI, how=\"left\"\n        )  # TODO: look into using .contains\n        valid_camera_points = np.isfinite(\n            points_in_field_buffer[\"index_right\"].to_numpy()\n        )\n\n        valid_camera_inds = np.where(valid_camera_points)[0]\n        subset_camera_set = self.get_subset_cameras(valid_camera_inds)\n        return subset_camera_set\n\n    def triangulate_detections(\n        self,\n        detector: TabularRectangleSegmentor,\n        transform_to_epsg_4978=None,\n        similarity_threshold_meters: float = 0.1,\n        louvain_resolution: float = 2,\n        vis: bool = True,\n        plotter: pv.Plotter = pv.Plotter(),\n        vis_ray_length_meters: float = 200,\n    ) -&gt; np.ndarray:\n        \"\"\"Take per-image detections and triangulate them to 3D locations\n\n        Args:\n            detector (TabularRectangleSegmentor):\n                Produces detections per image using the get_detection_centers method\n            transform_to_epsg_4978 (typing.Union[np.ndarray, None], optional):\n                The 4x4 transform to earth centered earth fixed coordinates. Defaults to None.\n            similarity_threshold_meters (float, optional):\n                Consider rays a potential match if the distance between them is less than this\n                value. Defaults to 0.1.\n            louvain_resolution (float, optional):\n                The resolution parameter of the networkx.louvain_communities function. Defaults to\n                2.0.\n            vis (bool, optional):\n                Whether to show the detection projections and intersecting points. Defaults to True.\n            plotter (pv.Plotter, optional):\n                The plotter to add the visualizations to is vis=True. If not set, a new one will be\n                created. Defaults to pv.Plotter().\n            vis_ray_length_meters (float, optional):\n                The length of the visualized rays in meters. Defaults to 200.\n\n        Returns:\n            np.ndarray:\n                (n unique objects, 3), the 3D locations of the identified objects.\n                If transform_to_epsg_4978 is set, then this is in (lat, lon, alt), if not, it's in the\n                local coordinate system of the mesh\n        \"\"\"\n        # Determine scale factor relating meters to internal coordinates\n        meters_to_local_scale = 1 / get_scale_from_transform(transform_to_epsg_4978)\n        similarity_threshold_local = similarity_threshold_meters * meters_to_local_scale\n        vis_ray_length_local = vis_ray_length_meters * meters_to_local_scale\n\n        # Record the lines corresponding to each detection and the associated image ID\n        all_line_segments = []\n        all_image_IDs = []\n\n        # Iterate over the cameras\n        for camera_ind in range(len(self)):\n            # Get the image filename\n            image_filename = str(self.get_image_filename(camera_ind, absolute=False))\n            # Get the centers of associated detection from the detector\n            # TODO, this only works with \"detectors\" that can look up the detections based on the\n            # filename alone. In the future we might want to support real detectors that actually\n            # use the image.\n            detection_centers_pixels = detector.get_detection_centers(image_filename)\n            # Get the individual camera\n            camera = self[camera_ind]\n            # Project rays given the locations of the detections in pixel coordinates\n            line_segments = camera.cast_rays(\n                pixel_coords_ij=detection_centers_pixels,\n                line_length=vis_ray_length_local,\n            )\n            # If there are no detections, this will be None\n            if line_segments is not None:\n                # Record the line segments, which will be ordered as alternating (start, end) rows\n                all_line_segments.append(line_segments)\n                # Record which image ID generated each line\n                all_image_IDs.append(\n                    np.full(int(line_segments.shape[0] / 2), fill_value=camera_ind)\n                )\n\n        # Concatenate the lists of arrays into a single array\n        all_line_segments = np.concatenate(all_line_segments, axis=0)\n        all_image_IDs = np.concatenate(all_image_IDs, axis=0)\n\n        # Get the starts and ends, which are alternating rows\n        ray_starts = all_line_segments[0::2]\n        segment_ends = all_line_segments[1::2]\n        # Determine the direction\n        ray_directions = segment_ends - ray_starts\n        # Make the ray directions unit length\n        ray_directions = ray_directions / np.linalg.norm(\n            ray_directions, axis=1, keepdims=True\n        )\n\n        # Compute the distance matrix of ray-ray intersections\n        num_dets = ray_starts.shape[0]\n        interesection_dists = np.full((num_dets, num_dets), fill_value=np.nan)\n\n        # Calculate the upper triangular matrix of ray-ray interesections\n        for i in tqdm(range(num_dets), desc=\"Calculating quality of ray intersections\"):\n            for j in range(i, num_dets):\n                # Extract starts and directions\n                A = ray_starts[i]\n                B = ray_starts[j]\n                a = ray_directions[i]\n                b = ray_directions[j]\n                # TODO explore whether this could be vectorized\n                dist, valid = compute_approximate_ray_intersection(A, a, B, b)\n\n                interesection_dists[i, j] = dist if valid else np.nan\n\n        # Filter out intersections that are above the threshold distance\n        interesection_dists[interesection_dists &gt; similarity_threshold_local] = np.nan\n\n        # Determine which intersections are valid, represented by finite values\n        i_inds, j_inds = np.where(np.isfinite(interesection_dists))\n\n        # Build a list of (i, j, info_dict) tuples encoding the valid edges and their intersection\n        # distance\n        positive_edges = [\n            (i, j, {\"weight\": 1 / interesection_dists[i, j]})\n            for i, j in zip(i_inds, j_inds)\n        ]\n\n        # Build a networkx graph. The nodes represent an individual detection while the edges\n        # represent the quality of the matches between detections.\n        graph = networkx.Graph(positive_edges)\n        # Determine Louvain communities which are sets of nodes. Ideally, this represents a set of\n        # detections that all coorespond to one 3D object\n        communities = networkx.community.louvain_communities(\n            graph, weight=\"weight\", resolution=louvain_resolution\n        )\n        # Sort the communities by size\n        communities = sorted(communities, key=len, reverse=True)\n\n        ## Triangulate the rays for each community to identify the 3D location\n        community_points = []\n        # Record the community IDs per detection\n        community_IDs = np.full(num_dets, fill_value=np.nan)\n        # Iterate over communities\n        for community_ID, community in enumerate(communities):\n            # Get the indices of the detections for that community\n            community_detection_inds = np.array(list(community))\n            # Record the community ID for the corresponding detection IDs\n            community_IDs[community_detection_inds] = community_ID\n\n            # Get the set of starts and directions for that community\n            community_starts = ray_starts[community_detection_inds]\n            community_directions = ray_directions[community_detection_inds]\n\n            # Determine the least squares triangulation of the rays\n            community_3D_point = triangulate_rays_lstsq(\n                community_starts, community_directions\n            )\n            community_points.append(community_3D_point)\n\n        # Stack all of the points into one vector\n        community_points = np.vstack(community_points)\n\n        # Show the rays and detections\n        if vis:\n            # Show the line segements\n            # TODO: consider coloring these lines by community\n            lines_mesh = pv.line_segments_from_points(all_line_segments)\n            plotter.add_mesh(\n                lines_mesh,\n                scalars=community_IDs,\n                label=\"Rays, colored by community ID\",\n            )\n            # Show the triangulated communtities as red spheres\n            detected_points = pv.PolyData(community_points)\n            plotter.add_points(\n                detected_points,\n                color=\"r\",\n                render_points_as_spheres=True,\n                point_size=10,\n                label=\"Triangulated locations\",\n            )\n            plotter.add_legend()\n\n        # Convert the intersection points from the local mesh coordinate system to lat lon\n        if transform_to_epsg_4978 is not None:\n            # Append a column of all ones to make the homogenous coordinates\n            community_points_homogenous = np.concatenate(\n                [community_points, np.ones_like(community_points[:, 0:1])], axis=1\n            )\n            # Use the transform matrix to transform the points into the earth centered, earth fixed\n            # frame, EPSG:4978\n            community_points_epsg_4978 = (\n                transform_to_epsg_4978 @ community_points_homogenous.T\n            ).T\n            # Convert the points from earth centered, earth fixed frame to lat lon\n            community_points_lat_lon = convert_CRS_3D_points(\n                community_points_epsg_4978,\n                input_CRS=EARTH_CENTERED_EARTH_FIXED_EPSG_CODE,\n                output_CRS=LAT_LON_EPSG_CODE,\n            )\n            # Set the community points to lat lon\n            community_points = community_points_lat_lon\n\n        # Return the 3D locations of the community points\n        return community_points\n\n    def vis(\n        self,\n        plotter: pv.Plotter = None,\n        add_orientation_cube: bool = False,\n        show: bool = False,\n        frustum_scale: float = None,\n        force_xvfb: bool = False,\n        interactive_jupyter: bool = False,\n    ):\n        \"\"\"Visualize all the cameras\n\n        Args:\n            plotter (pv.Plotter):\n                Plotter to add the cameras to. If None, will be created and then plotted\n            add_orientation_cube (bool, optional):\n                Add a cube to visualize the coordinate system. Defaults to False.\n            show (bool, optional):\n                Show the results instead of waiting for other content to be added\n            frustum_scale (float, optional):\n                Size of cameras in world units. If None, will set to 1/120th of the maximum distance\n                between two cameras.\n            force_xvfb (bool, optional):\n                Force a headless rendering backend\n            interactive_jupyter (bool, optional):\n                Will allow you to interact with the visualization in your notebook if supported by\n                the notebook server. Otherwise will fail. Only applicable if `show=True`. Defaults\n                to False.\n\n        \"\"\"\n\n        if plotter is None:\n            plotter = pv.Plotter()\n            show = True\n\n        # Determine pairwise distance between each camera and set frustum_scale to 1/120th of the maximum distance found\n        if frustum_scale is None:\n            if self.n_cameras() &gt;= 2:\n                camera_translation_matrices = np.array(\n                    [transform[:3, 3] for transform in self.cam_to_world_transforms]\n                )\n                distances = pdist(camera_translation_matrices, metric=\"euclidean\")\n                max_distance = np.max(distances)\n                frustum_scale = (\n                    (max_distance / 120) if max_distance &gt; 0 else DEFAULT_FRUSTUM_SCALE\n                )\n            # else, set it to a default\n            else:\n                frustum_scale = DEFAULT_FRUSTUM_SCALE\n\n        for camera in self.cameras:\n            camera.vis(plotter, frustum_scale=frustum_scale)\n        if add_orientation_cube:\n            # TODO Consider adding to a freestanding vis module\n            ocube = demos.orientation_cube()\n            plotter.add_mesh(ocube[\"cube\"], show_edges=True)\n            plotter.add_mesh(ocube[\"x_p\"], color=\"blue\")\n            plotter.add_mesh(ocube[\"x_n\"], color=\"blue\")\n            plotter.add_mesh(ocube[\"y_p\"], color=\"green\")\n            plotter.add_mesh(ocube[\"y_n\"], color=\"green\")\n            plotter.add_mesh(ocube[\"z_p\"], color=\"red\")\n            plotter.add_mesh(ocube[\"z_n\"], color=\"red\")\n            plotter.show_axes()\n\n        if show:\n            if force_xvfb:\n                safe_start_xvfb()\n            plotter.show(jupyter_backend=\"trame\" if interactive_jupyter else \"static\")\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet-functions","title":"Functions","text":""},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.__init__","title":"<code>__init__(cameras=None, cam_to_world_transforms=None, intrinsic_params_per_sensor_type={0: EXAMPLE_INTRINSICS}, image_filenames=None, lon_lats=None, image_folder=None, sensor_IDs=None, validate_images=False)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>cam_to_world_transforms</code> <code>List[ndarray]</code> <p>The list of 4x4 camera to world transforms</p> <code>None</code> <code>intrinsic_params_per_sensor</code> <code>Dict[int, Dict]</code> <p>A dictionary mapping from an int camera ID to the intrinsic parameters</p> required <code>image_filenames</code> <code>List[PATH_TYPE]</code> <p>The list of image filenames, ideally absolute paths</p> <code>None</code> <code>lon_lats</code> <code>Union[None, List[Union[None, Tuple[float, float]]]]</code> <p>A list of lon,lat tuples, or list of Nones, or None</p> <code>None</code> <code>image_folder</code> <code>PATH_TYPE</code> <p>The top level folder of the images</p> <code>None</code> <code>sensor_IDs</code> <code>List[int]</code> <p>The list of sensor IDs, that index into the sensors_params_dict</p> <code>None</code> <code>validate_images</code> <code>bool</code> <p>Should the existance of the images be checked. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>description</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def __init__(\n    self,\n    cameras: Union[None, PhotogrammetryCamera, List[PhotogrammetryCamera]] = None,\n    cam_to_world_transforms: Union[None, List[np.ndarray]] = None,\n    intrinsic_params_per_sensor_type: Dict[int, Dict[str, float]] = {\n        0: EXAMPLE_INTRINSICS\n    },\n    image_filenames: Union[List[PATH_TYPE], None] = None,\n    lon_lats: Union[None, List[Union[None, Tuple[float, float]]]] = None,\n    image_folder: PATH_TYPE = None,\n    sensor_IDs: List[int] = None,\n    validate_images: bool = False,\n):\n    \"\"\"_summary_\n\n    Args:\n        cam_to_world_transforms (List[np.ndarray]): The list of 4x4 camera to world transforms\n        intrinsic_params_per_sensor (Dict[int, Dict]): A dictionary mapping from an int camera ID to the intrinsic parameters\n        image_filenames (List[PATH_TYPE]): The list of image filenames, ideally absolute paths\n        lon_lats (Union[None, List[Union[None, Tuple[float, float]]]]): A list of lon,lat tuples, or list of Nones, or None\n        image_folder (PATH_TYPE): The top level folder of the images\n        sensor_IDs (List[int]): The list of sensor IDs, that index into the sensors_params_dict\n        validate_images (bool, optional): Should the existance of the images be checked. Defaults to False.\n\n    Raises:\n        ValueError: _description_\n    \"\"\"\n    # Create an object using the supplied cameras\n    if cameras is not None:\n        if isinstance(cameras, PhotogrammetryCamera):\n            cameras = [cameras]\n        self.cameras = cameras\n        return\n\n    # Standardization\n    n_transforms = len(cam_to_world_transforms)\n\n    # Create list of Nones for image filenames if not set\n    if image_filenames is None:\n        image_filenames = [None] * n_transforms\n\n    if sensor_IDs is None and len(intrinsic_params_per_sensor_type) == 1:\n        # Create a list of the only index if not set\n        sensor_IDs = [\n            list(intrinsic_params_per_sensor_type.keys())[0]\n        ] * n_transforms\n    elif len(sensor_IDs) != n_transforms:\n        raise ValueError(\n            f\"Number of sensor_IDs ({len(sensor_IDs)}) is different than the number of transforms ({n_transforms})\"\n        )\n\n    # If lon lats is None, set it to a list of Nones per transform\n    if lon_lats is None:\n        lon_lats = [None] * n_transforms\n\n    if image_folder is None:\n        # TODO set it to the least common ancestor of all filenames\n        pass\n\n    # Record the values\n    # TODO see if we ever use these\n    self.cam_to_world_transforms = cam_to_world_transforms\n    self.intrinsic_params_per_sensor_type = intrinsic_params_per_sensor_type\n    self.image_filenames = image_filenames\n    self.lon_lats = lon_lats\n    self.sensor_IDs = sensor_IDs\n    self.image_folder = image_folder\n\n    if validate_images:\n        missing_images, invalid_images = self.find_mising_images()\n        if len(missing_images) &gt; 0:\n            print(f\"Deleting {len(missing_images)} missing images\")\n            valid_images = np.where(np.logical_not(invalid_images))[0]\n            self.image_filenames = np.array(self.image_filenames)[\n                valid_images\n            ].tolist()\n            # Avoid calling .tolist() because this will recursively set all elements to lists\n            # when this should be a list of np.arrays\n            self.cam_to_world_transforms = [\n                x for x in np.array(self.cam_to_world_transforms)[valid_images]\n            ]\n            self.sensor_IDs = np.array(self.sensor_IDs)[valid_images].tolist()\n            self.lon_lats = np.array(self.lon_lats)[valid_images].tolist()\n\n    self.cameras = []\n\n    for image_filename, cam_to_world_transform, sensor_ID, lon_lat in zip(\n        self.image_filenames,\n        self.cam_to_world_transforms,\n        self.sensor_IDs,\n        self.lon_lats,\n    ):\n        sensor_params = self.intrinsic_params_per_sensor_type[sensor_ID]\n        new_camera = PhotogrammetryCamera(\n            image_filename, cam_to_world_transform, lon_lat=lon_lat, **sensor_params\n        )\n        self.cameras.append(new_camera)\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_camera_locations","title":"<code>get_camera_locations(**kwargs)</code>","text":"<p>Returns a list of camera locations for each camera.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Keyword arguments to be passed to the PhotogrammetryCamera.get_camera_location method.</p> <code>{}</code> <p>Returns:</p> Type Description <p>List[Tuple[float, float] or Tuple[float, float, float]]: List of tuples containing the camera locations.</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_camera_locations(self, **kwargs):\n    \"\"\"\n    Returns a list of camera locations for each camera.\n\n    Args:\n        **kwargs: Keyword arguments to be passed to the PhotogrammetryCamera.get_camera_location method.\n\n    Returns:\n        List[Tuple[float, float] or Tuple[float, float, float]]:\n            List of tuples containing the camera locations.\n    \"\"\"\n    return [x.get_camera_location(**kwargs) for x in self.cameras]\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_cameras_in_folder","title":"<code>get_cameras_in_folder(folder)</code>","text":"<p>Return the camera set with cameras corresponding to images in that folder</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>PATH_TYPE</code> <p>The folder location</p> required <p>Returns:</p> Name Type Description <code>PhotogrammetryCameraSet</code> <p>A copy of the camera set with only the cameras from that folder</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_cameras_in_folder(self, folder: PATH_TYPE):\n    \"\"\"Return the camera set with cameras corresponding to images in that folder\n\n    Args:\n        folder (PATH_TYPE): The folder location\n\n    Returns:\n        PhotogrammetryCameraSet: A copy of the camera set with only the cameras from that folder\n    \"\"\"\n    # Get the inds where that camera is in the folder\n    imgs_in_folder_inds = [\n        i\n        for i in range(len(self.cameras))\n        if self.cameras[i].image_filename.is_relative_to(folder)\n    ]\n    # Return the PhotogrammetryCameraSet with those subset of cameras\n    subset_cameras = self.get_subset_cameras(imgs_in_folder_inds)\n    return subset_cameras\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_lon_lat_coords","title":"<code>get_lon_lat_coords()</code>","text":"<p>Returns a list of GPS coords for each camera</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_lon_lat_coords(self):\n    \"\"\"Returns a list of GPS coords for each camera\"\"\"\n    return [x.get_lon_lat() for x in self.cameras]\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_pytorch3d_camera","title":"<code>get_pytorch3d_camera(device)</code>","text":"<p>Return a pytorch3d cameras object based on the parameters from metashape. This has the information from each of the camears in the set to enabled batched rendering.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str</code> <p>What device (cuda/cpu) to put the object on</p> required <p>Returns:</p> Type Description <p>pytorch3d.renderer.PerspectiveCameras:</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_pytorch3d_camera(self, device: str):\n    \"\"\"\n    Return a pytorch3d cameras object based on the parameters from metashape.\n    This has the information from each of the camears in the set to enabled batched rendering.\n\n\n    Args:\n        device (str): What device (cuda/cpu) to put the object on\n\n    Returns:\n        pytorch3d.renderer.PerspectiveCameras:\n    \"\"\"\n    # Get the pytorch3d cameras for each of the cameras in the set\n    p3d_cameras = [camera.get_pytorch3d_camera(device) for camera in self.cameras]\n    # Get the image sizes\n    image_sizes = [camera.image_size.cpu().numpy() for camera in p3d_cameras]\n    # Check that all the image sizes are the same because this is required for proper batched rendering\n    if np.any([image_size != image_sizes[0] for image_size in image_sizes]):\n        raise ValueError(\"Not all cameras have the same image size\")\n    # Create the new pytorch3d cameras object with the information from each camera\n    cameras = PerspectiveCameras(\n        R=torch.cat([camera.R for camera in p3d_cameras], 0),\n        T=torch.cat([camera.T for camera in p3d_cameras], 0),\n        focal_length=torch.cat([camera.focal_length for camera in p3d_cameras], 0),\n        principal_point=torch.cat(\n            [camera.get_principal_point() for camera in p3d_cameras], 0\n        ),\n        device=device,\n        in_ndc=False,  # screen coords\n        image_size=image_sizes[0],\n    )\n    return cameras\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.get_subset_ROI","title":"<code>get_subset_ROI(ROI, buffer_radius=0, is_geospatial=None)</code>","text":"<p>Return cameras that are within a radius of the provided geometry</p> <p>Parameters:</p> Name Type Description Default <code>geodata</code> <code>Union[PATH_TYPE, GeoDataFrame, Polygon, MultiPolygon]</code> <p>This can be a Geopandas dataframe, path to a geofile readable by geopandas, or Shapely Polygon/MultiPolygon information that can be loaded into a geodataframe</p> required <code>buffer_radius</code> <code>float</code> <p>Return points within this buffer of the geometry. Defaults to 0. Represents meters if ROI is geospatial.</p> <code>0</code> <code>is_geospatial</code> <code>bool</code> <p>A flag for user to indicate if ROI is geospatial or not; if no flag is provided, the flag is set if the provided geodata has a CRS.</p> <code>None</code> <p>Returns:     subset_camera_set (List[PhotogrammetryCamera]):         List of cameras that fall within the provided ROI</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def get_subset_ROI(\n    self,\n    ROI: Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon],\n    buffer_radius: float = 0,\n    is_geospatial: bool = None,\n):\n    \"\"\"Return cameras that are within a radius of the provided geometry\n\n    Args:\n        geodata (Union[PATH_TYPE, gpd.GeoDataFrame, Polygon, MultiPolygon]):\n            This can be a Geopandas dataframe, path to a geofile readable by geopandas, or\n            Shapely Polygon/MultiPolygon information that can be loaded into a geodataframe\n        buffer_radius (float, optional):\n            Return points within this buffer of the geometry. Defaults to 0. Represents\n            meters if ROI is geospatial.\n        is_geospatial (bool, optional):\n            A flag for user to indicate if ROI is geospatial or not; if no flag is provided,\n            the flag is set if the provided geodata has a CRS.\n    Returns:\n        subset_camera_set (List[PhotogrammetryCamera]):\n            List of cameras that fall within the provided ROI\n    \"\"\"\n    # construct GeoDataFrame if not provided\n    if isinstance(ROI, (Polygon, MultiPolygon)):\n        # assume geodata is lat/lon if is_geospatial is True\n        if is_geospatial:\n            ROI = gpd.GeoDataFrame(crs=LAT_LON_EPSG_CODE, geometry=[ROI])\n        else:\n            ROI = gpd.GeoDataFrame(geometry=[ROI])\n    elif not isinstance(ROI, gpd.GeoDataFrame):\n        # Read in the geofile\n        ROI = gpd.read_file(ROI)\n\n    if is_geospatial is None:\n        is_geospatial = ROI.crs is not None\n\n    if not is_geospatial:\n        # get internal coordinate system camera locations\n        image_locations = [Point(*x) for x in self.get_camera_locations()]\n        image_locations_df = gpd.GeoDataFrame(geometry=image_locations)\n    else:\n        # Make sure it's a geometric (meters-based) CRS\n        ROI = ensure_projected_CRS(ROI)\n        # Read the locations of all the points\n        image_locations = [Point(*x) for x in self.get_lon_lat_coords()]\n        # Create a dataframe, assuming inputs are lat lon\n        image_locations_df = gpd.GeoDataFrame(\n            geometry=image_locations, crs=LAT_LON_EPSG_CODE\n        )\n        image_locations_df.to_crs(ROI.crs, inplace=True)\n\n    # Merge all of the elements together into one multipolygon, destroying any attributes that were there\n    ROI = ROI.dissolve()\n    # Expand the geometry of the shape by the buffer\n    ROI[\"geometry\"] = ROI.buffer(buffer_radius)\n    image_locations_df[\"index\"] = image_locations_df.index\n\n    points_in_field_buffer = gpd.sjoin(\n        image_locations_df, ROI, how=\"left\"\n    )  # TODO: look into using .contains\n    valid_camera_points = np.isfinite(\n        points_in_field_buffer[\"index_right\"].to_numpy()\n    )\n\n    valid_camera_inds = np.where(valid_camera_points)[0]\n    subset_camera_set = self.get_subset_cameras(valid_camera_inds)\n    return subset_camera_set\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.n_cameras","title":"<code>n_cameras()</code>","text":"<p>Return the number of cameras</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def n_cameras(self) -&gt; int:\n    \"\"\"Return the number of cameras\"\"\"\n    return len(self.cameras)\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.n_image_channels","title":"<code>n_image_channels()</code>","text":"<p>Return the number of channels in the image</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def n_image_channels(self) -&gt; int:\n    \"\"\"Return the number of channels in the image\"\"\"\n    return 3\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.triangulate_detections","title":"<code>triangulate_detections(detector, transform_to_epsg_4978=None, similarity_threshold_meters=0.1, louvain_resolution=2, vis=True, plotter=pv.Plotter(), vis_ray_length_meters=200)</code>","text":"<p>Take per-image detections and triangulate them to 3D locations</p> <p>Parameters:</p> Name Type Description Default <code>detector</code> <code>TabularRectangleSegmentor</code> <p>Produces detections per image using the get_detection_centers method</p> required <code>transform_to_epsg_4978</code> <code>Union[ndarray, None]</code> <p>The 4x4 transform to earth centered earth fixed coordinates. Defaults to None.</p> <code>None</code> <code>similarity_threshold_meters</code> <code>float</code> <p>Consider rays a potential match if the distance between them is less than this value. Defaults to 0.1.</p> <code>0.1</code> <code>louvain_resolution</code> <code>float</code> <p>The resolution parameter of the networkx.louvain_communities function. Defaults to 2.0.</p> <code>2</code> <code>vis</code> <code>bool</code> <p>Whether to show the detection projections and intersecting points. Defaults to True.</p> <code>True</code> <code>plotter</code> <code>Plotter</code> <p>The plotter to add the visualizations to is vis=True. If not set, a new one will be created. Defaults to pv.Plotter().</p> <code>Plotter()</code> <code>vis_ray_length_meters</code> <code>float</code> <p>The length of the visualized rays in meters. Defaults to 200.</p> <code>200</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: (n unique objects, 3), the 3D locations of the identified objects. If transform_to_epsg_4978 is set, then this is in (lat, lon, alt), if not, it's in the local coordinate system of the mesh</p> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def triangulate_detections(\n    self,\n    detector: TabularRectangleSegmentor,\n    transform_to_epsg_4978=None,\n    similarity_threshold_meters: float = 0.1,\n    louvain_resolution: float = 2,\n    vis: bool = True,\n    plotter: pv.Plotter = pv.Plotter(),\n    vis_ray_length_meters: float = 200,\n) -&gt; np.ndarray:\n    \"\"\"Take per-image detections and triangulate them to 3D locations\n\n    Args:\n        detector (TabularRectangleSegmentor):\n            Produces detections per image using the get_detection_centers method\n        transform_to_epsg_4978 (typing.Union[np.ndarray, None], optional):\n            The 4x4 transform to earth centered earth fixed coordinates. Defaults to None.\n        similarity_threshold_meters (float, optional):\n            Consider rays a potential match if the distance between them is less than this\n            value. Defaults to 0.1.\n        louvain_resolution (float, optional):\n            The resolution parameter of the networkx.louvain_communities function. Defaults to\n            2.0.\n        vis (bool, optional):\n            Whether to show the detection projections and intersecting points. Defaults to True.\n        plotter (pv.Plotter, optional):\n            The plotter to add the visualizations to is vis=True. If not set, a new one will be\n            created. Defaults to pv.Plotter().\n        vis_ray_length_meters (float, optional):\n            The length of the visualized rays in meters. Defaults to 200.\n\n    Returns:\n        np.ndarray:\n            (n unique objects, 3), the 3D locations of the identified objects.\n            If transform_to_epsg_4978 is set, then this is in (lat, lon, alt), if not, it's in the\n            local coordinate system of the mesh\n    \"\"\"\n    # Determine scale factor relating meters to internal coordinates\n    meters_to_local_scale = 1 / get_scale_from_transform(transform_to_epsg_4978)\n    similarity_threshold_local = similarity_threshold_meters * meters_to_local_scale\n    vis_ray_length_local = vis_ray_length_meters * meters_to_local_scale\n\n    # Record the lines corresponding to each detection and the associated image ID\n    all_line_segments = []\n    all_image_IDs = []\n\n    # Iterate over the cameras\n    for camera_ind in range(len(self)):\n        # Get the image filename\n        image_filename = str(self.get_image_filename(camera_ind, absolute=False))\n        # Get the centers of associated detection from the detector\n        # TODO, this only works with \"detectors\" that can look up the detections based on the\n        # filename alone. In the future we might want to support real detectors that actually\n        # use the image.\n        detection_centers_pixels = detector.get_detection_centers(image_filename)\n        # Get the individual camera\n        camera = self[camera_ind]\n        # Project rays given the locations of the detections in pixel coordinates\n        line_segments = camera.cast_rays(\n            pixel_coords_ij=detection_centers_pixels,\n            line_length=vis_ray_length_local,\n        )\n        # If there are no detections, this will be None\n        if line_segments is not None:\n            # Record the line segments, which will be ordered as alternating (start, end) rows\n            all_line_segments.append(line_segments)\n            # Record which image ID generated each line\n            all_image_IDs.append(\n                np.full(int(line_segments.shape[0] / 2), fill_value=camera_ind)\n            )\n\n    # Concatenate the lists of arrays into a single array\n    all_line_segments = np.concatenate(all_line_segments, axis=0)\n    all_image_IDs = np.concatenate(all_image_IDs, axis=0)\n\n    # Get the starts and ends, which are alternating rows\n    ray_starts = all_line_segments[0::2]\n    segment_ends = all_line_segments[1::2]\n    # Determine the direction\n    ray_directions = segment_ends - ray_starts\n    # Make the ray directions unit length\n    ray_directions = ray_directions / np.linalg.norm(\n        ray_directions, axis=1, keepdims=True\n    )\n\n    # Compute the distance matrix of ray-ray intersections\n    num_dets = ray_starts.shape[0]\n    interesection_dists = np.full((num_dets, num_dets), fill_value=np.nan)\n\n    # Calculate the upper triangular matrix of ray-ray interesections\n    for i in tqdm(range(num_dets), desc=\"Calculating quality of ray intersections\"):\n        for j in range(i, num_dets):\n            # Extract starts and directions\n            A = ray_starts[i]\n            B = ray_starts[j]\n            a = ray_directions[i]\n            b = ray_directions[j]\n            # TODO explore whether this could be vectorized\n            dist, valid = compute_approximate_ray_intersection(A, a, B, b)\n\n            interesection_dists[i, j] = dist if valid else np.nan\n\n    # Filter out intersections that are above the threshold distance\n    interesection_dists[interesection_dists &gt; similarity_threshold_local] = np.nan\n\n    # Determine which intersections are valid, represented by finite values\n    i_inds, j_inds = np.where(np.isfinite(interesection_dists))\n\n    # Build a list of (i, j, info_dict) tuples encoding the valid edges and their intersection\n    # distance\n    positive_edges = [\n        (i, j, {\"weight\": 1 / interesection_dists[i, j]})\n        for i, j in zip(i_inds, j_inds)\n    ]\n\n    # Build a networkx graph. The nodes represent an individual detection while the edges\n    # represent the quality of the matches between detections.\n    graph = networkx.Graph(positive_edges)\n    # Determine Louvain communities which are sets of nodes. Ideally, this represents a set of\n    # detections that all coorespond to one 3D object\n    communities = networkx.community.louvain_communities(\n        graph, weight=\"weight\", resolution=louvain_resolution\n    )\n    # Sort the communities by size\n    communities = sorted(communities, key=len, reverse=True)\n\n    ## Triangulate the rays for each community to identify the 3D location\n    community_points = []\n    # Record the community IDs per detection\n    community_IDs = np.full(num_dets, fill_value=np.nan)\n    # Iterate over communities\n    for community_ID, community in enumerate(communities):\n        # Get the indices of the detections for that community\n        community_detection_inds = np.array(list(community))\n        # Record the community ID for the corresponding detection IDs\n        community_IDs[community_detection_inds] = community_ID\n\n        # Get the set of starts and directions for that community\n        community_starts = ray_starts[community_detection_inds]\n        community_directions = ray_directions[community_detection_inds]\n\n        # Determine the least squares triangulation of the rays\n        community_3D_point = triangulate_rays_lstsq(\n            community_starts, community_directions\n        )\n        community_points.append(community_3D_point)\n\n    # Stack all of the points into one vector\n    community_points = np.vstack(community_points)\n\n    # Show the rays and detections\n    if vis:\n        # Show the line segements\n        # TODO: consider coloring these lines by community\n        lines_mesh = pv.line_segments_from_points(all_line_segments)\n        plotter.add_mesh(\n            lines_mesh,\n            scalars=community_IDs,\n            label=\"Rays, colored by community ID\",\n        )\n        # Show the triangulated communtities as red spheres\n        detected_points = pv.PolyData(community_points)\n        plotter.add_points(\n            detected_points,\n            color=\"r\",\n            render_points_as_spheres=True,\n            point_size=10,\n            label=\"Triangulated locations\",\n        )\n        plotter.add_legend()\n\n    # Convert the intersection points from the local mesh coordinate system to lat lon\n    if transform_to_epsg_4978 is not None:\n        # Append a column of all ones to make the homogenous coordinates\n        community_points_homogenous = np.concatenate(\n            [community_points, np.ones_like(community_points[:, 0:1])], axis=1\n        )\n        # Use the transform matrix to transform the points into the earth centered, earth fixed\n        # frame, EPSG:4978\n        community_points_epsg_4978 = (\n            transform_to_epsg_4978 @ community_points_homogenous.T\n        ).T\n        # Convert the points from earth centered, earth fixed frame to lat lon\n        community_points_lat_lon = convert_CRS_3D_points(\n            community_points_epsg_4978,\n            input_CRS=EARTH_CENTERED_EARTH_FIXED_EPSG_CODE,\n            output_CRS=LAT_LON_EPSG_CODE,\n        )\n        # Set the community points to lat lon\n        community_points = community_points_lat_lon\n\n    # Return the 3D locations of the community points\n    return community_points\n</code></pre>"},{"location":"cameras/cameras/#geograypher.cameras.PhotogrammetryCameraSet.vis","title":"<code>vis(plotter=None, add_orientation_cube=False, show=False, frustum_scale=None, force_xvfb=False, interactive_jupyter=False)</code>","text":"<p>Visualize all the cameras</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>Plotter to add the cameras to. If None, will be created and then plotted</p> <code>None</code> <code>add_orientation_cube</code> <code>bool</code> <p>Add a cube to visualize the coordinate system. Defaults to False.</p> <code>False</code> <code>show</code> <code>bool</code> <p>Show the results instead of waiting for other content to be added</p> <code>False</code> <code>frustum_scale</code> <code>float</code> <p>Size of cameras in world units. If None, will set to 1/120th of the maximum distance between two cameras.</p> <code>None</code> <code>force_xvfb</code> <code>bool</code> <p>Force a headless rendering backend</p> <code>False</code> <code>interactive_jupyter</code> <code>bool</code> <p>Will allow you to interact with the visualization in your notebook if supported by the notebook server. Otherwise will fail. Only applicable if <code>show=True</code>. Defaults to False.</p> <code>False</code> Source code in <code>geograypher/cameras/cameras.py</code> <pre><code>def vis(\n    self,\n    plotter: pv.Plotter = None,\n    add_orientation_cube: bool = False,\n    show: bool = False,\n    frustum_scale: float = None,\n    force_xvfb: bool = False,\n    interactive_jupyter: bool = False,\n):\n    \"\"\"Visualize all the cameras\n\n    Args:\n        plotter (pv.Plotter):\n            Plotter to add the cameras to. If None, will be created and then plotted\n        add_orientation_cube (bool, optional):\n            Add a cube to visualize the coordinate system. Defaults to False.\n        show (bool, optional):\n            Show the results instead of waiting for other content to be added\n        frustum_scale (float, optional):\n            Size of cameras in world units. If None, will set to 1/120th of the maximum distance\n            between two cameras.\n        force_xvfb (bool, optional):\n            Force a headless rendering backend\n        interactive_jupyter (bool, optional):\n            Will allow you to interact with the visualization in your notebook if supported by\n            the notebook server. Otherwise will fail. Only applicable if `show=True`. Defaults\n            to False.\n\n    \"\"\"\n\n    if plotter is None:\n        plotter = pv.Plotter()\n        show = True\n\n    # Determine pairwise distance between each camera and set frustum_scale to 1/120th of the maximum distance found\n    if frustum_scale is None:\n        if self.n_cameras() &gt;= 2:\n            camera_translation_matrices = np.array(\n                [transform[:3, 3] for transform in self.cam_to_world_transforms]\n            )\n            distances = pdist(camera_translation_matrices, metric=\"euclidean\")\n            max_distance = np.max(distances)\n            frustum_scale = (\n                (max_distance / 120) if max_distance &gt; 0 else DEFAULT_FRUSTUM_SCALE\n            )\n        # else, set it to a default\n        else:\n            frustum_scale = DEFAULT_FRUSTUM_SCALE\n\n    for camera in self.cameras:\n        camera.vis(plotter, frustum_scale=frustum_scale)\n    if add_orientation_cube:\n        # TODO Consider adding to a freestanding vis module\n        ocube = demos.orientation_cube()\n        plotter.add_mesh(ocube[\"cube\"], show_edges=True)\n        plotter.add_mesh(ocube[\"x_p\"], color=\"blue\")\n        plotter.add_mesh(ocube[\"x_n\"], color=\"blue\")\n        plotter.add_mesh(ocube[\"y_p\"], color=\"green\")\n        plotter.add_mesh(ocube[\"y_n\"], color=\"green\")\n        plotter.add_mesh(ocube[\"z_p\"], color=\"red\")\n        plotter.add_mesh(ocube[\"z_n\"], color=\"red\")\n        plotter.show_axes()\n\n    if show:\n        if force_xvfb:\n            safe_start_xvfb()\n        plotter.show(jupyter_backend=\"trame\" if interactive_jupyter else \"static\")\n</code></pre>"},{"location":"cameras/derived_cameras/","title":"Derived Cameras Docstrings","text":""},{"location":"cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet","title":"<code>MetashapeCameraSet</code>","text":"<p>               Bases: <code>PhotogrammetryCameraSet</code></p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>class MetashapeCameraSet(PhotogrammetryCameraSet):\n    def __init__(\n        self,\n        camera_file: PATH_TYPE,\n        image_folder: PATH_TYPE,\n        validate_images: bool = False,\n        default_sensor_params: dict = {},\n    ):\n        \"\"\"Parse the information about the camera intrinsics and extrinsics\n\n        Args:\n            camera_file (PATH_TYPE): Path to metashape .xml export\n            image_folder: (PATH_TYPE): Path to image folder root\n\n        Raises:\n            ValueError: If camera calibration does not contain the f, cx, and cy params\n        \"\"\"\n        # Load the xml file\n        # Taken from here https://rowelldionicio.com/parsing-xml-with-python-minidom/\n        tree = ET.parse(camera_file)\n        root = tree.getroot()\n        # first level\n        chunk = root.find(\"chunk\")\n        # second level\n        sensors = chunk.find(\"sensors\")\n        # Parse the sensors representation\n        sensors_dict = parse_sensors(sensors, default_sensor_dict=default_sensor_params)\n\n        # Set up the lists to populate\n        image_filenames = []\n        cam_to_world_transforms = []\n        sensor_IDs = []\n\n        cameras = chunk[2]\n        # Iterate over metashape cameras and fill out required information\n        for cam_or_group in cameras:\n            if cam_or_group.tag == \"group\":\n                for cam in cam_or_group:\n                    update_lists(\n                        cam,\n                        image_folder,\n                        cam_to_world_transforms,\n                        image_filenames,\n                        sensor_IDs,\n                    )\n            else:\n                # 4x4 transform\n                update_lists(\n                    cam_or_group,\n                    image_folder,\n                    cam_to_world_transforms,\n                    image_filenames,\n                    sensor_IDs,\n                )\n\n        # Compute the lat lon using the transforms, because the reference values recorded in the file\n        # reflect the EXIF values, not the optimized ones\n\n        # Get the transform from the chunk to the earth-centered, earth-fixed (ECEF) frame\n        chunk_to_epsg4327 = parse_transform_metashape(camera_file=camera_file)\n\n        if chunk_to_epsg4327 is not None:\n            # Compute the location of each camera in ECEF\n            cam_locs_in_epsg4327 = []\n            for cam_to_world_transform in cam_to_world_transforms:\n                cam_loc_in_chunk = cam_to_world_transform[:, 3:]\n                cam_locs_in_epsg4327.append(chunk_to_epsg4327 @ cam_loc_in_chunk)\n            cam_locs_in_epsg4327 = np.concatenate(cam_locs_in_epsg4327, axis=1)[:3].T\n            # Transform these points into lat-lon-alt\n            transformer = pyproj.Transformer.from_crs(\n                EARTH_CENTERED_EARTH_FIXED_EPSG_CODE, LAT_LON_EPSG_CODE\n            )\n            lat, lon, _ = transformer.transform(\n                xx=cam_locs_in_epsg4327[:, 0],\n                yy=cam_locs_in_epsg4327[:, 1],\n                zz=cam_locs_in_epsg4327[:, 2],\n            )\n            lon_lats = list(zip(lon, lat))\n        else:\n            # TODO consider trying to parse from the xml\n            lon_lats = None\n\n        # Actually construct the camera objects using the base class\n        super().__init__(\n            cam_to_world_transforms=cam_to_world_transforms,\n            intrinsic_params_per_sensor_type=sensors_dict,\n            image_filenames=image_filenames,\n            lon_lats=lon_lats,\n            image_folder=image_folder,\n            sensor_IDs=sensor_IDs,\n            validate_images=validate_images,\n        )\n</code></pre>"},{"location":"cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet-functions","title":"Functions","text":""},{"location":"cameras/derived_cameras/#geograypher.cameras.derived_cameras.MetashapeCameraSet.__init__","title":"<code>__init__(camera_file, image_folder, validate_images=False, default_sensor_params={})</code>","text":"<p>Parse the information about the camera intrinsics and extrinsics</p> <p>Parameters:</p> Name Type Description Default <code>camera_file</code> <code>PATH_TYPE</code> <p>Path to metashape .xml export</p> required <code>image_folder</code> <code>PATH_TYPE</code> <p>(PATH_TYPE): Path to image folder root</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If camera calibration does not contain the f, cx, and cy params</p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>def __init__(\n    self,\n    camera_file: PATH_TYPE,\n    image_folder: PATH_TYPE,\n    validate_images: bool = False,\n    default_sensor_params: dict = {},\n):\n    \"\"\"Parse the information about the camera intrinsics and extrinsics\n\n    Args:\n        camera_file (PATH_TYPE): Path to metashape .xml export\n        image_folder: (PATH_TYPE): Path to image folder root\n\n    Raises:\n        ValueError: If camera calibration does not contain the f, cx, and cy params\n    \"\"\"\n    # Load the xml file\n    # Taken from here https://rowelldionicio.com/parsing-xml-with-python-minidom/\n    tree = ET.parse(camera_file)\n    root = tree.getroot()\n    # first level\n    chunk = root.find(\"chunk\")\n    # second level\n    sensors = chunk.find(\"sensors\")\n    # Parse the sensors representation\n    sensors_dict = parse_sensors(sensors, default_sensor_dict=default_sensor_params)\n\n    # Set up the lists to populate\n    image_filenames = []\n    cam_to_world_transforms = []\n    sensor_IDs = []\n\n    cameras = chunk[2]\n    # Iterate over metashape cameras and fill out required information\n    for cam_or_group in cameras:\n        if cam_or_group.tag == \"group\":\n            for cam in cam_or_group:\n                update_lists(\n                    cam,\n                    image_folder,\n                    cam_to_world_transforms,\n                    image_filenames,\n                    sensor_IDs,\n                )\n        else:\n            # 4x4 transform\n            update_lists(\n                cam_or_group,\n                image_folder,\n                cam_to_world_transforms,\n                image_filenames,\n                sensor_IDs,\n            )\n\n    # Compute the lat lon using the transforms, because the reference values recorded in the file\n    # reflect the EXIF values, not the optimized ones\n\n    # Get the transform from the chunk to the earth-centered, earth-fixed (ECEF) frame\n    chunk_to_epsg4327 = parse_transform_metashape(camera_file=camera_file)\n\n    if chunk_to_epsg4327 is not None:\n        # Compute the location of each camera in ECEF\n        cam_locs_in_epsg4327 = []\n        for cam_to_world_transform in cam_to_world_transforms:\n            cam_loc_in_chunk = cam_to_world_transform[:, 3:]\n            cam_locs_in_epsg4327.append(chunk_to_epsg4327 @ cam_loc_in_chunk)\n        cam_locs_in_epsg4327 = np.concatenate(cam_locs_in_epsg4327, axis=1)[:3].T\n        # Transform these points into lat-lon-alt\n        transformer = pyproj.Transformer.from_crs(\n            EARTH_CENTERED_EARTH_FIXED_EPSG_CODE, LAT_LON_EPSG_CODE\n        )\n        lat, lon, _ = transformer.transform(\n            xx=cam_locs_in_epsg4327[:, 0],\n            yy=cam_locs_in_epsg4327[:, 1],\n            zz=cam_locs_in_epsg4327[:, 2],\n        )\n        lon_lats = list(zip(lon, lat))\n    else:\n        # TODO consider trying to parse from the xml\n        lon_lats = None\n\n    # Actually construct the camera objects using the base class\n    super().__init__(\n        cam_to_world_transforms=cam_to_world_transforms,\n        intrinsic_params_per_sensor_type=sensors_dict,\n        image_filenames=image_filenames,\n        lon_lats=lon_lats,\n        image_folder=image_folder,\n        sensor_IDs=sensor_IDs,\n        validate_images=validate_images,\n    )\n</code></pre>"},{"location":"cameras/derived_cameras/#geograypher.cameras.derived_cameras.COLMAPCameraSet","title":"<code>COLMAPCameraSet</code>","text":"<p>               Bases: <code>PhotogrammetryCameraSet</code></p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>class COLMAPCameraSet(PhotogrammetryCameraSet):\n\n    def __init__(\n        self,\n        cameras_file: PATH_TYPE,\n        images_file: PATH_TYPE,\n        image_folder: typing.Union[None, PATH_TYPE] = None,\n        validate_images: bool = False,\n    ):\n        \"\"\"\n        Create a camera set from the files exported by the open-source structure-from-motion\n        software COLMAP as defined here: https://colmap.github.io/format.html\n\n        Args:\n            cameras_file (PATH_TYPE):\n                Path to the file containing the camera models definitions\n            images_file (PATH_TYPE):\n                Path to the per-image information, including the pose and which camera model is used\n            image_folder (typing.Union[None, PATH_TYPE], optional):\n                Path to the folder of images used to generate the reconstruction. Defaults to None.\n            validate_images (bool, optional):\n                Ensure that the images described in images_file are present in image_folder.\n                Defaults to False.\n\n        Raises:\n            NotImplementedError: If the camera is not a Simple radial model\n        \"\"\"\n        # Parse the csv representation of the camera models\n        cameras_data = pd.read_csv(\n            cameras_file,\n            sep=\" \",\n            skiprows=[0, 1, 2],\n            header=None,\n            names=(\n                \"CAMERA_ID\",\n                \"MODEL\",\n                \"WIDTH\",\n                \"HEIGHT\",\n                \"PARAMS_F\",\n                \"PARAMS_CX\",\n                \"PARAMS_CY\",\n                \"PARAMS_RADIAL\",\n            ),\n        )\n        # Parse the csv of the per-image information\n        # Note that every image has first the useful information on one row and then unneeded\n        # keypoint information on the following row. Therefore, the keypoints are discarded.\n        images_data = pd.read_csv(\n            images_file,\n            sep=\" \",\n            skiprows=lambda x: (x in (0, 1, 2, 3) or x % 2),\n            header=None,\n            names=(\n                \"IMAGE_ID\",\n                \"QW\",\n                \"QX\",\n                \"QY\",\n                \"QZ\",\n                \"TX\",\n                \"TY\",\n                \"TZ\",\n                \"CAMERA_ID\",\n                \"NAME\",\n            ),\n            usecols=list(range(10)),\n        )\n\n        # TODO support more camera models\n        if np.any(cameras_data[\"MODEL\"] != \"SIMPLE_RADIAL\"):\n            raise NotImplementedError(\"Not a supported camera model\")\n\n        # Parse the camera parameters, creating a dict for each distinct camera model\n        sensors_dict = {}\n        for _, row in cameras_data.iterrows():\n            # Note that the convention in this tool is for cx, cy to be defined from the center\n            # not the corner so it must be shifted\n            sensor_dict = {\n                \"image_width\": row[\"WIDTH\"],\n                \"image_height\": row[\"HEIGHT\"],\n                \"f\": row[\"PARAMS_F\"],\n                \"cx\": row[\"PARAMS_CX\"] - row[\"WIDTH\"] / 2,\n                \"cy\": row[\"PARAMS_CY\"] - row[\"HEIGHT\"] / 2,\n                \"distortion_params\": {\"r\": row[\"PARAMS_RADIAL\"]},\n            }\n            sensors_dict[row[\"CAMERA_ID\"]] = sensor_dict\n\n        # Parse the per-image information\n        cam_to_world_transforms = []\n        sensor_IDs = []\n        image_filenames = []\n\n        for _, row in images_data.iterrows():\n            # Convert from the quaternion representation to the matrix one. Note that the W element\n            # is the first one in the COLMAP convention but the last one in scipy.\n            rot_mat = Rotation.from_quat(\n                (row[\"QX\"], row[\"QY\"], row[\"QZ\"], row[\"QW\"])\n            ).as_matrix()\n            # Get the camera translation\n            translation_vec = np.array([row[\"TX\"], row[\"TY\"], row[\"TZ\"]])\n\n            # Create a 4x4 homogenous matrix representing the world_to_cam transform\n            world_to_cam = np.eye(4)\n            # Populate the sub-elements\n            world_to_cam[:3, :3] = rot_mat\n            world_to_cam[:3, 3] = translation_vec\n            # We need the cam to world transform. Since we're using a 4x4 representation, we can\n            # just invert the matrix\n            cam_to_world = np.linalg.inv(world_to_cam)\n            cam_to_world_transforms.append(cam_to_world)\n\n            # Record which camera model is used and the image filename\n            sensor_IDs.append(row[\"CAMERA_ID\"])\n            image_filenames.append(Path(image_folder, row[\"NAME\"]))\n\n        # Instantiate the camera set\n        super().__init__(\n            cam_to_world_transforms=cam_to_world_transforms,\n            intrinsic_params_per_sensor_type=sensors_dict,\n            image_filenames=image_filenames,\n            sensor_IDs=sensor_IDs,\n            image_folder=image_folder,\n            validate_images=validate_images,\n        )\n</code></pre>"},{"location":"cameras/derived_cameras/#geograypher.cameras.derived_cameras.COLMAPCameraSet-functions","title":"Functions","text":""},{"location":"cameras/derived_cameras/#geograypher.cameras.derived_cameras.COLMAPCameraSet.__init__","title":"<code>__init__(cameras_file, images_file, image_folder=None, validate_images=False)</code>","text":"<p>Create a camera set from the files exported by the open-source structure-from-motion software COLMAP as defined here: https://colmap.github.io/format.html</p> <p>Parameters:</p> Name Type Description Default <code>cameras_file</code> <code>PATH_TYPE</code> <p>Path to the file containing the camera models definitions</p> required <code>images_file</code> <code>PATH_TYPE</code> <p>Path to the per-image information, including the pose and which camera model is used</p> required <code>image_folder</code> <code>Union[None, PATH_TYPE]</code> <p>Path to the folder of images used to generate the reconstruction. Defaults to None.</p> <code>None</code> <code>validate_images</code> <code>bool</code> <p>Ensure that the images described in images_file are present in image_folder. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the camera is not a Simple radial model</p> Source code in <code>geograypher/cameras/derived_cameras.py</code> <pre><code>def __init__(\n    self,\n    cameras_file: PATH_TYPE,\n    images_file: PATH_TYPE,\n    image_folder: typing.Union[None, PATH_TYPE] = None,\n    validate_images: bool = False,\n):\n    \"\"\"\n    Create a camera set from the files exported by the open-source structure-from-motion\n    software COLMAP as defined here: https://colmap.github.io/format.html\n\n    Args:\n        cameras_file (PATH_TYPE):\n            Path to the file containing the camera models definitions\n        images_file (PATH_TYPE):\n            Path to the per-image information, including the pose and which camera model is used\n        image_folder (typing.Union[None, PATH_TYPE], optional):\n            Path to the folder of images used to generate the reconstruction. Defaults to None.\n        validate_images (bool, optional):\n            Ensure that the images described in images_file are present in image_folder.\n            Defaults to False.\n\n    Raises:\n        NotImplementedError: If the camera is not a Simple radial model\n    \"\"\"\n    # Parse the csv representation of the camera models\n    cameras_data = pd.read_csv(\n        cameras_file,\n        sep=\" \",\n        skiprows=[0, 1, 2],\n        header=None,\n        names=(\n            \"CAMERA_ID\",\n            \"MODEL\",\n            \"WIDTH\",\n            \"HEIGHT\",\n            \"PARAMS_F\",\n            \"PARAMS_CX\",\n            \"PARAMS_CY\",\n            \"PARAMS_RADIAL\",\n        ),\n    )\n    # Parse the csv of the per-image information\n    # Note that every image has first the useful information on one row and then unneeded\n    # keypoint information on the following row. Therefore, the keypoints are discarded.\n    images_data = pd.read_csv(\n        images_file,\n        sep=\" \",\n        skiprows=lambda x: (x in (0, 1, 2, 3) or x % 2),\n        header=None,\n        names=(\n            \"IMAGE_ID\",\n            \"QW\",\n            \"QX\",\n            \"QY\",\n            \"QZ\",\n            \"TX\",\n            \"TY\",\n            \"TZ\",\n            \"CAMERA_ID\",\n            \"NAME\",\n        ),\n        usecols=list(range(10)),\n    )\n\n    # TODO support more camera models\n    if np.any(cameras_data[\"MODEL\"] != \"SIMPLE_RADIAL\"):\n        raise NotImplementedError(\"Not a supported camera model\")\n\n    # Parse the camera parameters, creating a dict for each distinct camera model\n    sensors_dict = {}\n    for _, row in cameras_data.iterrows():\n        # Note that the convention in this tool is for cx, cy to be defined from the center\n        # not the corner so it must be shifted\n        sensor_dict = {\n            \"image_width\": row[\"WIDTH\"],\n            \"image_height\": row[\"HEIGHT\"],\n            \"f\": row[\"PARAMS_F\"],\n            \"cx\": row[\"PARAMS_CX\"] - row[\"WIDTH\"] / 2,\n            \"cy\": row[\"PARAMS_CY\"] - row[\"HEIGHT\"] / 2,\n            \"distortion_params\": {\"r\": row[\"PARAMS_RADIAL\"]},\n        }\n        sensors_dict[row[\"CAMERA_ID\"]] = sensor_dict\n\n    # Parse the per-image information\n    cam_to_world_transforms = []\n    sensor_IDs = []\n    image_filenames = []\n\n    for _, row in images_data.iterrows():\n        # Convert from the quaternion representation to the matrix one. Note that the W element\n        # is the first one in the COLMAP convention but the last one in scipy.\n        rot_mat = Rotation.from_quat(\n            (row[\"QX\"], row[\"QY\"], row[\"QZ\"], row[\"QW\"])\n        ).as_matrix()\n        # Get the camera translation\n        translation_vec = np.array([row[\"TX\"], row[\"TY\"], row[\"TZ\"]])\n\n        # Create a 4x4 homogenous matrix representing the world_to_cam transform\n        world_to_cam = np.eye(4)\n        # Populate the sub-elements\n        world_to_cam[:3, :3] = rot_mat\n        world_to_cam[:3, 3] = translation_vec\n        # We need the cam to world transform. Since we're using a 4x4 representation, we can\n        # just invert the matrix\n        cam_to_world = np.linalg.inv(world_to_cam)\n        cam_to_world_transforms.append(cam_to_world)\n\n        # Record which camera model is used and the image filename\n        sensor_IDs.append(row[\"CAMERA_ID\"])\n        image_filenames.append(Path(image_folder, row[\"NAME\"]))\n\n    # Instantiate the camera set\n    super().__init__(\n        cam_to_world_transforms=cam_to_world_transforms,\n        intrinsic_params_per_sensor_type=sensors_dict,\n        image_filenames=image_filenames,\n        sensor_IDs=sensor_IDs,\n        image_folder=image_folder,\n        validate_images=validate_images,\n    )\n</code></pre>"},{"location":"cameras/segmentor/","title":"Segmentor Docstrings","text":""},{"location":"cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet","title":"<code>SegmentorPhotogrammetryCameraSet</code>","text":"<p>               Bases: <code>PhotogrammetryCameraSet</code></p> Source code in <code>geograypher/cameras/segmentor.py</code> <pre><code>class SegmentorPhotogrammetryCameraSet(PhotogrammetryCameraSet):\n    def __init__(\n        self,\n        base_camera_set: PhotogrammetryCameraSet,\n        segmentor: Segmentor,\n        dont_load_base_image: bool = True,\n    ):\n        \"\"\"Wraps a camera set to provide segmented versions of the image\n\n        Args:\n            base_camera_set (PhotogrammetryCameraSet): The original camera set\n            segmentor (Segmentor): A fully instantiated segmentor\n        \"\"\"\n        self.base_camera_set = base_camera_set\n        self.segmentor = segmentor\n        self.dont_load_base_image = dont_load_base_image\n\n        # This should allow all un-overridden methods to work as expected\n        self.cameras = self.base_camera_set.cameras\n\n    def get_image_by_index(self, index: int, image_scale: float = 1) -&gt; np.ndarray:\n        if self.dont_load_base_image:\n            raw_image = None\n        else:\n            raw_image = self.base_camera_set.get_image_by_index(index, image_scale)\n        image_filename = self.base_camera_set.get_image_filename(index)\n        segmented_image = self.segmentor.segment_image(\n            raw_image, filename=image_filename, image_scale=image_scale\n        )\n        return segmented_image\n\n    def get_raw_image_by_index(self, index: int, image_scale: float = 1) -&gt; np.ndarray:\n        return self.base_camera_set.get_image_by_index(\n            index=index, image_scale=image_scale\n        )\n\n    def get_subset_cameras(self, inds: typing.List[int]):\n        subset_camera_set = deepcopy(self)\n        subset_camera_set.cameras = [subset_camera_set.cameras[i] for i in inds]\n        subset_camera_set.base_camera_set = (\n            subset_camera_set.base_camera_set.get_subset_cameras(inds)\n        )\n        return subset_camera_set\n\n    def n_image_channels(self) -&gt; int:\n        return self.segmentor.num_classes\n</code></pre>"},{"location":"cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet-functions","title":"Functions","text":""},{"location":"cameras/segmentor/#geograypher.cameras.SegmentorPhotogrammetryCameraSet.__init__","title":"<code>__init__(base_camera_set, segmentor, dont_load_base_image=True)</code>","text":"<p>Wraps a camera set to provide segmented versions of the image</p> <p>Parameters:</p> Name Type Description Default <code>base_camera_set</code> <code>PhotogrammetryCameraSet</code> <p>The original camera set</p> required <code>segmentor</code> <code>Segmentor</code> <p>A fully instantiated segmentor</p> required Source code in <code>geograypher/cameras/segmentor.py</code> <pre><code>def __init__(\n    self,\n    base_camera_set: PhotogrammetryCameraSet,\n    segmentor: Segmentor,\n    dont_load_base_image: bool = True,\n):\n    \"\"\"Wraps a camera set to provide segmented versions of the image\n\n    Args:\n        base_camera_set (PhotogrammetryCameraSet): The original camera set\n        segmentor (Segmentor): A fully instantiated segmentor\n    \"\"\"\n    self.base_camera_set = base_camera_set\n    self.segmentor = segmentor\n    self.dont_load_base_image = dont_load_base_image\n\n    # This should allow all un-overridden methods to work as expected\n    self.cameras = self.base_camera_set.cameras\n</code></pre>"},{"location":"meshes/","title":"Meshes","text":"<ul> <li> <p>Derived Meshes Docstrings</p> </li> <li> <p>Meshes Docstrings</p> </li> </ul>"},{"location":"meshes/derived_meshes/","title":"Derived Mesh Docstrings","text":""},{"location":"meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked","title":"<code>TexturedPhotogrammetryMeshChunked</code>","text":"<p>               Bases: <code>TexturedPhotogrammetryMesh</code></p> <p>Extends the TexturedPhotogrammtery mesh by allowing chunked operations for large meshes</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>class TexturedPhotogrammetryMeshChunked(TexturedPhotogrammetryMesh):\n    \"\"\"Extends the TexturedPhotogrammtery mesh by allowing chunked operations for large meshes\"\"\"\n\n    def get_mesh_chunks_for_cameras(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        n_clusters: int = 8,\n        buffer_dist_meters=50,\n        vis_clusters: bool = False,\n        include_texture: bool = False,\n    ):\n        \"\"\"Return a generator of sub-meshes, chunked to align with clusters of cameras\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The chunks of the mesh are generated by clustering the cameras\n            n_clusters (int, optional):\n                The mesh is broken up into this many clusters. Defaults to 8.\n            buffer_dist_meters (int, optional):\n                Each cluster contains the mesh that is within this distance in meters of the camera\n                locations. Defaults to 50.\n            vis_clusters (bool, optional):\n                Should the location of the cameras and resultant clusters be shown. Defaults to False.\n            include_texture (bool, optional): Should the texture from the full mesh be included\n                in the subset mesh. Defaults to False.\n\n        Yields:\n            pv.PolyData: The subset mesh\n            PhotogrammetryCameraSet: The cameras associated with that mesh\n            np.ndarray: The IDs of the faces in the original mesh used to generate the sub mesh\n\n        \"\"\"\n        # Extract the points depending on whether it's a single camera or a set\n        if isinstance(cameras, PhotogrammetryCamera):\n            camera_points = [Point(*cameras.get_lon_lat())]\n        else:\n            # Get the lat lon for each camera point and turn into a shapely Point\n            camera_points = [\n                Point(*lon_lat) for lon_lat in cameras.get_lon_lat_coords()\n            ]\n\n        # Create a geodataframe from the points\n        camera_points = gpd.GeoDataFrame(\n            geometry=camera_points, crs=pyproj.CRS.from_epsg(\"4326\")\n        )\n        # Make sure the gdf has a gemetric CRS so there is no warping of the space\n        camera_points = ensure_projected_CRS(camera_points)\n        # Extract the x, y points now in a geometric CRS\n        camera_points_numpy = np.stack(\n            camera_points.geometry.apply(lambda point: (point.x, point.y))\n        )\n\n        # Assign each camera to a cluster\n        camera_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(\n            camera_points_numpy\n        )\n        if vis_clusters:\n            # Show the camera locations, colored by which one they were assigned to\n            plt.scatter(\n                camera_points_numpy[:, 0],\n                camera_points_numpy[:, 1],\n                c=camera_cluster_IDs,\n                cmap=\"tab20\",\n            )\n            plt.show()\n\n        # Get the texture from the full mesh\n        full_mesh_texture = (\n            self.get_texture(request_vertex_texture=False) if include_texture else None\n        )\n\n        # Iterate over the clusters of cameras\n        for cluster_ID in tqdm(range(n_clusters), desc=\"Chunks in mesh\"):\n            # Get indices of cameras for that cluster\n            matching_camera_inds = np.where(cluster_ID == camera_cluster_IDs)[0]\n            # Get the segmentor camera set for the subset of the camera inds\n            sub_camera_set = cameras.get_subset_cameras(matching_camera_inds)\n            # Extract the rows in the dataframe for those IDs\n            subset_camera_points = camera_points.iloc[matching_camera_inds]\n\n            # TODO this could be accellerated by computing the membership for all points at the begining.\n            # This would require computing all the ROIs (potentially-overlapping) for each region first. Then, finding all the non-overlapping\n            # partition where each polygon corresponds to a set of ROIs. Then the membership for each vertex could be found for each polygon\n            # and the membership in each ROI could be computed. This should be benchmarked though, because having more polygons than original\n            # ROIs may actually lead to slower computations than doing it sequentially\n\n            # Extract a sub mesh for a region around the camera points and also retain the indices into the original mesh\n            sub_mesh_pv, _, face_IDs = self.select_mesh_ROI(\n                region_of_interest=subset_camera_points,\n                buffer_meters=buffer_dist_meters,\n                return_original_IDs=True,\n            )\n            # Extract the corresponding texture elements for this sub mesh if needed\n            # If include_texture=False, the full_mesh_texture will not be set\n            # If there is no mesh, the texture should also be set to None, otherwise it will be\n            # ambigious whether it's a face or vertex texture\n            sub_mesh_texture = (\n                full_mesh_texture[face_IDs]\n                if full_mesh_texture is not None and len(face_IDs) &gt; 0\n                else None\n            )\n\n            # Wrap this pyvista mesh in a photogrammetry mesh\n            sub_mesh_TPM = TexturedPhotogrammetryMesh(\n                sub_mesh_pv, texture=sub_mesh_texture\n            )\n\n            # Return the submesh as a Textured Photogrammetry Mesh, the subset of cameras, and the\n            # face IDs mapping the faces in the sub mesh back to the full one\n            yield sub_mesh_TPM, sub_camera_set, face_IDs\n\n    def render_flat(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        render_img_scale: float = 1,\n        n_clusters: int = 8,\n        buffer_dist_meters: float = 50,\n        vis_clusters: bool = False,\n        **pix2face_kwargs\n    ):\n        \"\"\"\n        Render the texture from the viewpoint of each camera in cameras. Note that this is a\n        generator so if you want to actually execute the computation, call list(*) on the output.\n        This version first clusters the cameras, extracts a region of the mesh surrounding each\n        cluster of cameras, and then performs rendering on each sub-region.\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                Either a single camera or a camera set. The texture will be rendered from the\n                perspective of each one\n            batch_size (int, optional):\n                The batch size for pix2face. Defaults to 1.\n            render_img_scale (float, optional):\n                The rendered image will be this fraction of the original image corresponding to the\n                virtual camera. Defaults to 1.\n            n_clusters (int, optional):\n                Number of clusters to break the cameras into. Defaults to 8.\n            buffer_dist_meters (float, optional):\n                How far around the cameras to include the mesh. Defaults to 50.\n            vis_clusters (bool, optional):\n                Should the clusters of camera locations be shown. Defaults to False.\n\n        Raises:\n            TypeError: If cameras is not the correct type\n\n        Yields:\n            np.ndarray:\n               The pix2face array for the next camera. The shape is\n               (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n        \"\"\"\n        # Create a generator to chunked meshes based on clusters of cameras\n        chunk_gen = self.get_mesh_chunks_for_cameras(\n            cameras,\n            n_clusters=n_clusters,\n            buffer_dist_meters=buffer_dist_meters,\n            vis_clusters=vis_clusters,\n            include_texture=True,\n        )\n\n        for sub_mesh_TPM, sub_camera_set, _ in tqdm(\n            chunk_gen, total=n_clusters, desc=\"Rendering by chunks\"\n        ):\n            # Create the render generator\n            render_gen = sub_mesh_TPM.render_flat(\n                sub_camera_set,\n                batch_size=batch_size,\n                render_img_scale=render_img_scale,\n                **pix2face_kwargs,\n            )\n            # Yield items from the returned generator\n            for render_item in render_gen:\n                yield render_item\n\n    def aggregate_projected_images(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        aggregate_img_scale: float = 1,\n        n_clusters: int = 8,\n        buffer_dist_meters: float = 50,\n        vis_clusters: bool = False,\n        **kwargs\n    ):\n        \"\"\"\n        Aggregate the imagery from multiple cameras into per-face averges. This version chunks the\n        mesh up and performs aggregation on sub-regions to decrease the runtime.\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The cameras to aggregate the images from. cam.get_image() will be called on each\n                element.\n            batch_size (int, optional):\n                The number of cameras to compute correspondences for at once. Defaults to 1.\n            aggregate_img_scale (float, optional):\n                The scale of pixel-to-face correspondences image, as a fraction of the original\n                image. Lower values lead to better runtimes but decreased precision at content\n                boundaries in the images. Defaults to 1.\n            n_clusters (int, optional):\n                The mesh is broken up into this many clusters. Defaults to 8.\n            buffer_dist_meters (int, optional):\n                Each cluster contains the mesh that is within this distance in meters of the camera\n                locations. Defaults to 50.\n            vis_clusters (bool, optional):\n                Should the location of the cameras and resultant clusters be shown. Defaults to False.\n\n        Returns:\n            np.ndarray: (n_faces, n_image_channels) The average projected image per face\n            dict: Additional information, including the summed projections, observations per face,\n                  and potentially each individual projection\n        \"\"\"\n\n        # Initialize the values that will be incremented per cluster\n        summed_projections = np.zeros(\n            (self.pyvista_mesh.n_faces, cameras.n_image_channels()), dtype=float\n        )\n        projection_counts = np.zeros(self.pyvista_mesh.n_faces, dtype=int)\n\n        # Create a generator to generate chunked meshes\n        chunk_gen = self.get_mesh_chunks_for_cameras(\n            cameras,\n            n_clusters=n_clusters,\n            buffer_dist_meters=buffer_dist_meters,\n            vis_clusters=vis_clusters,\n        )\n\n        # Iterate over chunks in the mesh\n        for sub_mesh_TPM, sub_camera_set, face_IDs in chunk_gen:\n            # This means there was no mesh for these cameras\n            if len(face_IDs) == 0:\n                continue\n\n            # Aggregate the projections from a set of cameras corresponding to\n            _, additional_information_submesh = sub_mesh_TPM.aggregate_projected_images(\n                sub_camera_set,\n                batch_size=batch_size,\n                aggregate_img_scale=aggregate_img_scale,\n                return_all=False,\n                **kwargs,\n            )\n\n            # Increment the summed predictions and counts\n            # Make sure that nans don't propogate, since they should just be treated as zeros\n            # TODO ensure this is correct\n            summed_projections[face_IDs] = np.nansum(\n                [\n                    summed_projections[face_IDs],\n                    additional_information_submesh[\"summed_projections\"],\n                ],\n                axis=0,\n            )\n            projection_counts[face_IDs] = (\n                projection_counts[face_IDs]\n                + additional_information_submesh[\"projection_counts\"]\n            )\n\n        # Same as the parent class\n        no_projections = projection_counts == 0\n        summed_projections[no_projections] = np.nan\n\n        additional_information = {\n            \"projection_counts\": projection_counts,\n            \"summed_projections\": summed_projections,\n        }\n\n        average_projections = np.divide(\n            summed_projections, np.expand_dims(projection_counts, 1)\n        )\n\n        return average_projections, additional_information\n\n    def label_polygons(\n        self,\n        face_labels: np.ndarray,\n        polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n        face_weighting: typing.Union[None, np.ndarray] = None,\n        sjoin_overlay: bool = True,\n        return_class_labels: bool = True,\n        unknown_class_label: str = \"unknown\",\n        buffer_dist_meters: float = 2,\n        n_polygons_per_cluster: int = 1000,\n    ):\n        \"\"\"\n        Assign a class label to polygons using labels per face. This implementation is useful for\n        large numbers of polygons. To make the expensive sjoin/overlay more efficient, this\n        implementation first clusters the polygons and labels each cluster indepenently. This makes\n        use of the fact that the mesh faces around this cluster can be extracted relatively quickly.\n        Then the sjoin/overlay is computed with substaintially-fewer polygons and faces, leading to\n        better performance.\n\n        Args:\n            face_labels (np.ndarray): (n_faces,) array of integer labels\n            polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n            face_weighting (typing.Union[None, np.ndarray], optional):\n                (n_faces,) array of scalar weights for each face, to be multiplied with the\n                contribution of this face. Defaults to None.\n            sjoin_overlay (bool, optional):\n                Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n                substaintially faster, but only uses mesh faces that are entirely within the bounds\n                of the polygon, rather than computing the intersecting region for\n                partially-overlapping faces. Defaults to True.\n            return_class_labels: (bool, optional):\n                Return string representation of class labels rather than float. Defaults to True.\n            unknown_class_label (str, optional):\n                Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n            buffer_dist_meters: (Union[float, None], optional)\n                Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n                the region that is this distance in meters from the polygons. Defaults to 2.0.\n            n_polygons_per_cluster: (int):\n                Set the number of clusters so there are approximately this number polygons per\n                cluster on average. Defaults to 1000\n\n        Raises:\n            ValueError: if faces_labels or face_weighting is not 1D\n\n        Returns:\n            list(typing.Union[str, int]):\n                (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n                or string values representing the class label\n        \"\"\"\n        # Load in the polygons\n        polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n        # Extract the centroid of each one and convert to a numpy array\n        centroids_xy = np.stack(\n            polygons_gdf.centroid.apply(lambda point: (point.x, point.y))\n        )\n        # Determine how many clusters there should be\n        n_clusters = int(np.ceil(len(polygons_gdf) / n_polygons_per_cluster))\n        # Assign each polygon to a cluster\n        polygon_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(centroids_xy)\n\n        # This will be set later once we figure out the datatype of the per-cluster labels\n        all_labels = None\n\n        # Loop over the individual clusters\n        for cluster_ID in tqdm(range(n_clusters), desc=\"Clusters of polygons\"):\n            # Determine which polygons are part of that cluster\n            cluster_mask = polygon_cluster_IDs == cluster_ID\n            # Extract the polygons from one cluster\n            cluster_polygons = polygons_gdf.iloc[cluster_mask]\n            # Compute the labeling per polygon\n            cluster_labels = super().label_polygons(\n                face_labels,\n                cluster_polygons,\n                face_weighting,\n                sjoin_overlay,\n                return_class_labels,\n                unknown_class_label,\n                buffer_dist_meters,\n            )\n            # Convert to numpy array\n            cluster_labels = np.array(cluster_labels)\n            # Create the aggregation array with the appropriate datatype\n            if all_labels is None:\n                # We assume that this list will be at least one element since each cluster\n                # should be non-empty. All values should be overwritten so the default value doesn't matter\n                all_labels = np.zeros(len(polygons_gdf), dtype=cluster_labels.dtype)\n\n            # Set the appropriate elements of the full array with the newly-computed cluster labels\n            all_labels[cluster_mask] = cluster_labels\n\n        # The output is expected to be a list\n        all_labels = all_labels.tolist()\n        return all_labels\n</code></pre>"},{"location":"meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked-functions","title":"Functions","text":""},{"location":"meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.aggregate_projected_images","title":"<code>aggregate_projected_images(cameras, batch_size=1, aggregate_img_scale=1, n_clusters=8, buffer_dist_meters=50, vis_clusters=False, **kwargs)</code>","text":"<p>Aggregate the imagery from multiple cameras into per-face averges. This version chunks the mesh up and performs aggregation on sub-regions to decrease the runtime.</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The cameras to aggregate the images from. cam.get_image() will be called on each element.</p> required <code>batch_size</code> <code>int</code> <p>The number of cameras to compute correspondences for at once. Defaults to 1.</p> <code>1</code> <code>aggregate_img_scale</code> <code>float</code> <p>The scale of pixel-to-face correspondences image, as a fraction of the original image. Lower values lead to better runtimes but decreased precision at content boundaries in the images. Defaults to 1.</p> <code>1</code> <code>n_clusters</code> <code>int</code> <p>The mesh is broken up into this many clusters. Defaults to 8.</p> <code>8</code> <code>buffer_dist_meters</code> <code>int</code> <p>Each cluster contains the mesh that is within this distance in meters of the camera locations. Defaults to 50.</p> <code>50</code> <code>vis_clusters</code> <code>bool</code> <p>Should the location of the cameras and resultant clusters be shown. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <p>np.ndarray: (n_faces, n_image_channels) The average projected image per face</p> <code>dict</code> <p>Additional information, including the summed projections, observations per face,   and potentially each individual projection</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def aggregate_projected_images(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    aggregate_img_scale: float = 1,\n    n_clusters: int = 8,\n    buffer_dist_meters: float = 50,\n    vis_clusters: bool = False,\n    **kwargs\n):\n    \"\"\"\n    Aggregate the imagery from multiple cameras into per-face averges. This version chunks the\n    mesh up and performs aggregation on sub-regions to decrease the runtime.\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The cameras to aggregate the images from. cam.get_image() will be called on each\n            element.\n        batch_size (int, optional):\n            The number of cameras to compute correspondences for at once. Defaults to 1.\n        aggregate_img_scale (float, optional):\n            The scale of pixel-to-face correspondences image, as a fraction of the original\n            image. Lower values lead to better runtimes but decreased precision at content\n            boundaries in the images. Defaults to 1.\n        n_clusters (int, optional):\n            The mesh is broken up into this many clusters. Defaults to 8.\n        buffer_dist_meters (int, optional):\n            Each cluster contains the mesh that is within this distance in meters of the camera\n            locations. Defaults to 50.\n        vis_clusters (bool, optional):\n            Should the location of the cameras and resultant clusters be shown. Defaults to False.\n\n    Returns:\n        np.ndarray: (n_faces, n_image_channels) The average projected image per face\n        dict: Additional information, including the summed projections, observations per face,\n              and potentially each individual projection\n    \"\"\"\n\n    # Initialize the values that will be incremented per cluster\n    summed_projections = np.zeros(\n        (self.pyvista_mesh.n_faces, cameras.n_image_channels()), dtype=float\n    )\n    projection_counts = np.zeros(self.pyvista_mesh.n_faces, dtype=int)\n\n    # Create a generator to generate chunked meshes\n    chunk_gen = self.get_mesh_chunks_for_cameras(\n        cameras,\n        n_clusters=n_clusters,\n        buffer_dist_meters=buffer_dist_meters,\n        vis_clusters=vis_clusters,\n    )\n\n    # Iterate over chunks in the mesh\n    for sub_mesh_TPM, sub_camera_set, face_IDs in chunk_gen:\n        # This means there was no mesh for these cameras\n        if len(face_IDs) == 0:\n            continue\n\n        # Aggregate the projections from a set of cameras corresponding to\n        _, additional_information_submesh = sub_mesh_TPM.aggregate_projected_images(\n            sub_camera_set,\n            batch_size=batch_size,\n            aggregate_img_scale=aggregate_img_scale,\n            return_all=False,\n            **kwargs,\n        )\n\n        # Increment the summed predictions and counts\n        # Make sure that nans don't propogate, since they should just be treated as zeros\n        # TODO ensure this is correct\n        summed_projections[face_IDs] = np.nansum(\n            [\n                summed_projections[face_IDs],\n                additional_information_submesh[\"summed_projections\"],\n            ],\n            axis=0,\n        )\n        projection_counts[face_IDs] = (\n            projection_counts[face_IDs]\n            + additional_information_submesh[\"projection_counts\"]\n        )\n\n    # Same as the parent class\n    no_projections = projection_counts == 0\n    summed_projections[no_projections] = np.nan\n\n    additional_information = {\n        \"projection_counts\": projection_counts,\n        \"summed_projections\": summed_projections,\n    }\n\n    average_projections = np.divide(\n        summed_projections, np.expand_dims(projection_counts, 1)\n    )\n\n    return average_projections, additional_information\n</code></pre>"},{"location":"meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.get_mesh_chunks_for_cameras","title":"<code>get_mesh_chunks_for_cameras(cameras, n_clusters=8, buffer_dist_meters=50, vis_clusters=False, include_texture=False)</code>","text":"<p>Return a generator of sub-meshes, chunked to align with clusters of cameras</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The chunks of the mesh are generated by clustering the cameras</p> required <code>n_clusters</code> <code>int</code> <p>The mesh is broken up into this many clusters. Defaults to 8.</p> <code>8</code> <code>buffer_dist_meters</code> <code>int</code> <p>Each cluster contains the mesh that is within this distance in meters of the camera locations. Defaults to 50.</p> <code>50</code> <code>vis_clusters</code> <code>bool</code> <p>Should the location of the cameras and resultant clusters be shown. Defaults to False.</p> <code>False</code> <code>include_texture</code> <code>bool</code> <p>Should the texture from the full mesh be included in the subset mesh. Defaults to False.</p> <code>False</code> <p>Yields:</p> Name Type Description <p>pv.PolyData: The subset mesh</p> <code>PhotogrammetryCameraSet</code> <p>The cameras associated with that mesh</p> <p>np.ndarray: The IDs of the faces in the original mesh used to generate the sub mesh</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def get_mesh_chunks_for_cameras(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    n_clusters: int = 8,\n    buffer_dist_meters=50,\n    vis_clusters: bool = False,\n    include_texture: bool = False,\n):\n    \"\"\"Return a generator of sub-meshes, chunked to align with clusters of cameras\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The chunks of the mesh are generated by clustering the cameras\n        n_clusters (int, optional):\n            The mesh is broken up into this many clusters. Defaults to 8.\n        buffer_dist_meters (int, optional):\n            Each cluster contains the mesh that is within this distance in meters of the camera\n            locations. Defaults to 50.\n        vis_clusters (bool, optional):\n            Should the location of the cameras and resultant clusters be shown. Defaults to False.\n        include_texture (bool, optional): Should the texture from the full mesh be included\n            in the subset mesh. Defaults to False.\n\n    Yields:\n        pv.PolyData: The subset mesh\n        PhotogrammetryCameraSet: The cameras associated with that mesh\n        np.ndarray: The IDs of the faces in the original mesh used to generate the sub mesh\n\n    \"\"\"\n    # Extract the points depending on whether it's a single camera or a set\n    if isinstance(cameras, PhotogrammetryCamera):\n        camera_points = [Point(*cameras.get_lon_lat())]\n    else:\n        # Get the lat lon for each camera point and turn into a shapely Point\n        camera_points = [\n            Point(*lon_lat) for lon_lat in cameras.get_lon_lat_coords()\n        ]\n\n    # Create a geodataframe from the points\n    camera_points = gpd.GeoDataFrame(\n        geometry=camera_points, crs=pyproj.CRS.from_epsg(\"4326\")\n    )\n    # Make sure the gdf has a gemetric CRS so there is no warping of the space\n    camera_points = ensure_projected_CRS(camera_points)\n    # Extract the x, y points now in a geometric CRS\n    camera_points_numpy = np.stack(\n        camera_points.geometry.apply(lambda point: (point.x, point.y))\n    )\n\n    # Assign each camera to a cluster\n    camera_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(\n        camera_points_numpy\n    )\n    if vis_clusters:\n        # Show the camera locations, colored by which one they were assigned to\n        plt.scatter(\n            camera_points_numpy[:, 0],\n            camera_points_numpy[:, 1],\n            c=camera_cluster_IDs,\n            cmap=\"tab20\",\n        )\n        plt.show()\n\n    # Get the texture from the full mesh\n    full_mesh_texture = (\n        self.get_texture(request_vertex_texture=False) if include_texture else None\n    )\n\n    # Iterate over the clusters of cameras\n    for cluster_ID in tqdm(range(n_clusters), desc=\"Chunks in mesh\"):\n        # Get indices of cameras for that cluster\n        matching_camera_inds = np.where(cluster_ID == camera_cluster_IDs)[0]\n        # Get the segmentor camera set for the subset of the camera inds\n        sub_camera_set = cameras.get_subset_cameras(matching_camera_inds)\n        # Extract the rows in the dataframe for those IDs\n        subset_camera_points = camera_points.iloc[matching_camera_inds]\n\n        # TODO this could be accellerated by computing the membership for all points at the begining.\n        # This would require computing all the ROIs (potentially-overlapping) for each region first. Then, finding all the non-overlapping\n        # partition where each polygon corresponds to a set of ROIs. Then the membership for each vertex could be found for each polygon\n        # and the membership in each ROI could be computed. This should be benchmarked though, because having more polygons than original\n        # ROIs may actually lead to slower computations than doing it sequentially\n\n        # Extract a sub mesh for a region around the camera points and also retain the indices into the original mesh\n        sub_mesh_pv, _, face_IDs = self.select_mesh_ROI(\n            region_of_interest=subset_camera_points,\n            buffer_meters=buffer_dist_meters,\n            return_original_IDs=True,\n        )\n        # Extract the corresponding texture elements for this sub mesh if needed\n        # If include_texture=False, the full_mesh_texture will not be set\n        # If there is no mesh, the texture should also be set to None, otherwise it will be\n        # ambigious whether it's a face or vertex texture\n        sub_mesh_texture = (\n            full_mesh_texture[face_IDs]\n            if full_mesh_texture is not None and len(face_IDs) &gt; 0\n            else None\n        )\n\n        # Wrap this pyvista mesh in a photogrammetry mesh\n        sub_mesh_TPM = TexturedPhotogrammetryMesh(\n            sub_mesh_pv, texture=sub_mesh_texture\n        )\n\n        # Return the submesh as a Textured Photogrammetry Mesh, the subset of cameras, and the\n        # face IDs mapping the faces in the sub mesh back to the full one\n        yield sub_mesh_TPM, sub_camera_set, face_IDs\n</code></pre>"},{"location":"meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.label_polygons","title":"<code>label_polygons(face_labels, polygons, face_weighting=None, sjoin_overlay=True, return_class_labels=True, unknown_class_label='unknown', buffer_dist_meters=2, n_polygons_per_cluster=1000)</code>","text":"<p>Assign a class label to polygons using labels per face. This implementation is useful for large numbers of polygons. To make the expensive sjoin/overlay more efficient, this implementation first clusters the polygons and labels each cluster indepenently. This makes use of the fact that the mesh faces around this cluster can be extracted relatively quickly. Then the sjoin/overlay is computed with substaintially-fewer polygons and faces, leading to better performance.</p> <p>Parameters:</p> Name Type Description Default <code>face_labels</code> <code>ndarray</code> <p>(n_faces,) array of integer labels</p> required <code>polygons</code> <code>Union[PATH_TYPE, GeoDataFrame]</code> <p>Geospatial polygons to be labeled</p> required <code>face_weighting</code> <code>Union[None, ndarray]</code> <p>(n_faces,) array of scalar weights for each face, to be multiplied with the contribution of this face. Defaults to None.</p> <code>None</code> <code>sjoin_overlay</code> <code>bool</code> <p>Whether to use <code>gpd.sjoin</code> or <code>gpd.overlay</code> to compute the overlay. Sjoin is substaintially faster, but only uses mesh faces that are entirely within the bounds of the polygon, rather than computing the intersecting region for partially-overlapping faces. Defaults to True.</p> <code>True</code> <code>return_class_labels</code> <code>bool</code> <p>(bool, optional): Return string representation of class labels rather than float. Defaults to True.</p> <code>True</code> <code>unknown_class_label</code> <code>str</code> <p>Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".</p> <code>'unknown'</code> <code>buffer_dist_meters</code> <code>float</code> <p>(Union[float, None], optional) Only applicable if sjoin_overlay=False. In that case, include faces entirely within the region that is this distance in meters from the polygons. Defaults to 2.0.</p> <code>2</code> <code>n_polygons_per_cluster</code> <code>int</code> <p>(int): Set the number of clusters so there are approximately this number polygons per cluster on average. Defaults to 1000</p> <code>1000</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if faces_labels or face_weighting is not 1D</p> <p>Returns:</p> Name Type Description <code>list</code> <code>Union[str, int]</code> <p>(n_polygons,) list of labels. Either float values, represnting integer IDs or nan, or string values representing the class label</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def label_polygons(\n    self,\n    face_labels: np.ndarray,\n    polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n    face_weighting: typing.Union[None, np.ndarray] = None,\n    sjoin_overlay: bool = True,\n    return_class_labels: bool = True,\n    unknown_class_label: str = \"unknown\",\n    buffer_dist_meters: float = 2,\n    n_polygons_per_cluster: int = 1000,\n):\n    \"\"\"\n    Assign a class label to polygons using labels per face. This implementation is useful for\n    large numbers of polygons. To make the expensive sjoin/overlay more efficient, this\n    implementation first clusters the polygons and labels each cluster indepenently. This makes\n    use of the fact that the mesh faces around this cluster can be extracted relatively quickly.\n    Then the sjoin/overlay is computed with substaintially-fewer polygons and faces, leading to\n    better performance.\n\n    Args:\n        face_labels (np.ndarray): (n_faces,) array of integer labels\n        polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n        face_weighting (typing.Union[None, np.ndarray], optional):\n            (n_faces,) array of scalar weights for each face, to be multiplied with the\n            contribution of this face. Defaults to None.\n        sjoin_overlay (bool, optional):\n            Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n            substaintially faster, but only uses mesh faces that are entirely within the bounds\n            of the polygon, rather than computing the intersecting region for\n            partially-overlapping faces. Defaults to True.\n        return_class_labels: (bool, optional):\n            Return string representation of class labels rather than float. Defaults to True.\n        unknown_class_label (str, optional):\n            Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n        buffer_dist_meters: (Union[float, None], optional)\n            Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n            the region that is this distance in meters from the polygons. Defaults to 2.0.\n        n_polygons_per_cluster: (int):\n            Set the number of clusters so there are approximately this number polygons per\n            cluster on average. Defaults to 1000\n\n    Raises:\n        ValueError: if faces_labels or face_weighting is not 1D\n\n    Returns:\n        list(typing.Union[str, int]):\n            (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n            or string values representing the class label\n    \"\"\"\n    # Load in the polygons\n    polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n    # Extract the centroid of each one and convert to a numpy array\n    centroids_xy = np.stack(\n        polygons_gdf.centroid.apply(lambda point: (point.x, point.y))\n    )\n    # Determine how many clusters there should be\n    n_clusters = int(np.ceil(len(polygons_gdf) / n_polygons_per_cluster))\n    # Assign each polygon to a cluster\n    polygon_cluster_IDs = KMeans(n_clusters=n_clusters).fit_predict(centroids_xy)\n\n    # This will be set later once we figure out the datatype of the per-cluster labels\n    all_labels = None\n\n    # Loop over the individual clusters\n    for cluster_ID in tqdm(range(n_clusters), desc=\"Clusters of polygons\"):\n        # Determine which polygons are part of that cluster\n        cluster_mask = polygon_cluster_IDs == cluster_ID\n        # Extract the polygons from one cluster\n        cluster_polygons = polygons_gdf.iloc[cluster_mask]\n        # Compute the labeling per polygon\n        cluster_labels = super().label_polygons(\n            face_labels,\n            cluster_polygons,\n            face_weighting,\n            sjoin_overlay,\n            return_class_labels,\n            unknown_class_label,\n            buffer_dist_meters,\n        )\n        # Convert to numpy array\n        cluster_labels = np.array(cluster_labels)\n        # Create the aggregation array with the appropriate datatype\n        if all_labels is None:\n            # We assume that this list will be at least one element since each cluster\n            # should be non-empty. All values should be overwritten so the default value doesn't matter\n            all_labels = np.zeros(len(polygons_gdf), dtype=cluster_labels.dtype)\n\n        # Set the appropriate elements of the full array with the newly-computed cluster labels\n        all_labels[cluster_mask] = cluster_labels\n\n    # The output is expected to be a list\n    all_labels = all_labels.tolist()\n    return all_labels\n</code></pre>"},{"location":"meshes/derived_meshes/#geograypher.meshes.derived_meshes.TexturedPhotogrammetryMeshChunked.render_flat","title":"<code>render_flat(cameras, batch_size=1, render_img_scale=1, n_clusters=8, buffer_dist_meters=50, vis_clusters=False, **pix2face_kwargs)</code>","text":"<p>Render the texture from the viewpoint of each camera in cameras. Note that this is a generator so if you want to actually execute the computation, call list(*) on the output. This version first clusters the cameras, extracts a region of the mesh surrounding each cluster of cameras, and then performs rendering on each sub-region.</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>Either a single camera or a camera set. The texture will be rendered from the perspective of each one</p> required <code>batch_size</code> <code>int</code> <p>The batch size for pix2face. Defaults to 1.</p> <code>1</code> <code>render_img_scale</code> <code>float</code> <p>The rendered image will be this fraction of the original image corresponding to the virtual camera. Defaults to 1.</p> <code>1</code> <code>n_clusters</code> <code>int</code> <p>Number of clusters to break the cameras into. Defaults to 8.</p> <code>8</code> <code>buffer_dist_meters</code> <code>float</code> <p>How far around the cameras to include the mesh. Defaults to 50.</p> <code>50</code> <code>vis_clusters</code> <code>bool</code> <p>Should the clusters of camera locations be shown. Defaults to False.</p> <code>False</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If cameras is not the correct type</p> <p>Yields:</p> Type Description <p>np.ndarray: The pix2face array for the next camera. The shape is (int(img_hrender_img_scale), int(img_wrender_img_scale)).</p> Source code in <code>geograypher/meshes/derived_meshes.py</code> <pre><code>def render_flat(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    render_img_scale: float = 1,\n    n_clusters: int = 8,\n    buffer_dist_meters: float = 50,\n    vis_clusters: bool = False,\n    **pix2face_kwargs\n):\n    \"\"\"\n    Render the texture from the viewpoint of each camera in cameras. Note that this is a\n    generator so if you want to actually execute the computation, call list(*) on the output.\n    This version first clusters the cameras, extracts a region of the mesh surrounding each\n    cluster of cameras, and then performs rendering on each sub-region.\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            Either a single camera or a camera set. The texture will be rendered from the\n            perspective of each one\n        batch_size (int, optional):\n            The batch size for pix2face. Defaults to 1.\n        render_img_scale (float, optional):\n            The rendered image will be this fraction of the original image corresponding to the\n            virtual camera. Defaults to 1.\n        n_clusters (int, optional):\n            Number of clusters to break the cameras into. Defaults to 8.\n        buffer_dist_meters (float, optional):\n            How far around the cameras to include the mesh. Defaults to 50.\n        vis_clusters (bool, optional):\n            Should the clusters of camera locations be shown. Defaults to False.\n\n    Raises:\n        TypeError: If cameras is not the correct type\n\n    Yields:\n        np.ndarray:\n           The pix2face array for the next camera. The shape is\n           (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n    \"\"\"\n    # Create a generator to chunked meshes based on clusters of cameras\n    chunk_gen = self.get_mesh_chunks_for_cameras(\n        cameras,\n        n_clusters=n_clusters,\n        buffer_dist_meters=buffer_dist_meters,\n        vis_clusters=vis_clusters,\n        include_texture=True,\n    )\n\n    for sub_mesh_TPM, sub_camera_set, _ in tqdm(\n        chunk_gen, total=n_clusters, desc=\"Rendering by chunks\"\n    ):\n        # Create the render generator\n        render_gen = sub_mesh_TPM.render_flat(\n            sub_camera_set,\n            batch_size=batch_size,\n            render_img_scale=render_img_scale,\n            **pix2face_kwargs,\n        )\n        # Yield items from the returned generator\n        for render_item in render_gen:\n            yield render_item\n</code></pre>"},{"location":"meshes/meshes/","title":"Mesh Docstrings","text":""},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh","title":"<code>TexturedPhotogrammetryMesh</code>","text":"Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>class TexturedPhotogrammetryMesh:\n    def __init__(\n        self,\n        mesh: typing.Union[PATH_TYPE, pv.PolyData],\n        downsample_target: float = 1.0,\n        transform_filename: PATH_TYPE = None,\n        texture: typing.Union[PATH_TYPE, np.ndarray, None] = None,\n        texture_column_name: typing.Union[PATH_TYPE, None] = None,\n        IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n        ROI=None,\n        ROI_buffer_meters: float = 0,\n        require_transform: bool = False,\n        log_level: str = \"INFO\",\n    ):\n        \"\"\"_summary_\n\n        Args:\n            mesh (typing.Union[PATH_TYPE, pv.PolyData]): Path to the mesh, in a format pyvista can read, or pyvista mesh\n            downsample_target (float, optional): Downsample to this fraction of vertices. Defaults to 1.0.\n            texture (typing.Union[PATH_TYPE, np.ndarray, None]): Texture or path to one. See more details in `load_texture` documentation\n            texture_column_name: The name of the column to use for a vectorfile input\n            IDs_to_labels (typing.Union[PATH_TYPE, dict, None]): dictionary or JSON file containing the mapping from integer IDs to string class names\n        \"\"\"\n        self.downsample_target = downsample_target\n\n        self.pyvista_mesh = None\n        self.texture = None\n        self.vertex_texture = None\n        self.face_texture = None\n        self.local_to_epgs_4978_transform = None\n        self.IDs_to_labels = None\n        # Create the plotter that will later be used to compute correspondences between pixels\n        # and the mesh. Note that this is only done to prevent a memory leak from creating multiple\n        # plotters. See https://github.com/pyvista/pyvista/issues/2252\n        self.pix2face_plotter = create_pv_plotter(off_screen=True)\n        self.face_polygons_cache = {}\n        self.face_2d_3d_ratios_cache = {}\n\n        self.logger = logging.getLogger(f\"mesh_{id(self)}\")\n        self.logger.setLevel(log_level)\n        # Potentially necessary for Jupyter\n        # https://stackoverflow.com/questions/35936086/jupyter-notebook-does-not-print-logs-to-the-output-cell\n        # If you don't check that there's already a handler, you can have situations with duplicated\n        # print outs if you have multiple mesh objects\n        if not self.logger.hasHandlers():\n            self.logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n\n        # Load the transform\n        self.logger.info(\"Loading transform to EPSG:4326\")\n        self.load_transform_to_epsg_4326(\n            transform_filename, require_transform=require_transform\n        )\n        # Load the mesh with the pyvista loader\n        self.logger.info(\"Loading mesh\")\n        self.load_mesh(\n            mesh=mesh,\n            downsample_target=downsample_target,\n            ROI=ROI,\n            ROI_buffer_meters=ROI_buffer_meters,\n        )\n        # Load the texture\n        self.logger.info(\"Loading texture\")\n        # load IDs_to_labels\n        # if IDs_to_labels not provided, check the directory of the mesh and get the file if found\n        if IDs_to_labels is None and isinstance(mesh, PATH_TYPE.__args__):\n            possible_json = Path(Path(mesh).stem + \"_IDs_to_labels.json\")\n            if possible_json.exists():\n                IDs_to_labels = possible_json\n        # convert IDs_to_labels from file to dict\n        if isinstance(IDs_to_labels, PATH_TYPE.__args__):\n            with open(IDs_to_labels, \"r\") as file:\n                IDs_to_labels = json.load(file)\n                IDs_to_labels = {int(id): label for id, label in IDs_to_labels.items()}\n        self.load_texture(texture, texture_column_name, IDs_to_labels=IDs_to_labels)\n\n    # Setup methods\n    def load_mesh(\n        self,\n        mesh: typing.Union[PATH_TYPE, pv.PolyData],\n        downsample_target: float = 1.0,\n        ROI=None,\n        ROI_buffer_meters=0,\n        ROI_simplify_tol_meters=2,\n    ):\n        \"\"\"Load the pyvista mesh and create the texture\n\n        Args:\n            mesh (typing.Union[PATH_TYPE, pv.PolyData]):\n                Path to the mesh or actual mesh\n            downsample_target (float, optional):\n                What fraction of mesh vertices to downsample to. Defaults to 1.0, (does nothing).\n            ROI:\n                See select_mesh_ROI. Defaults to None\n            ROI_buffer_meters:\n                See select_mesh_ROI. Defaults to 0.\n            ROI_simplify_tol_meters:\n                See select_mesh_ROI. Defaults to 2.\n        \"\"\"\n        if isinstance(mesh, pv.PolyData):\n            self.pyvista_mesh = mesh\n        else:\n            # Load the mesh using pyvista\n            # TODO see if pytorch3d has faster/more flexible readers. I'd assume no, but it's good to check\n            self.logger.info(\"Reading the mesh\")\n            self.pyvista_mesh = pv.read(mesh)\n\n        self.logger.info(\"Selecting an ROI from mesh\")\n        # Select a region of interest if needed\n        self.pyvista_mesh = self.select_mesh_ROI(\n            region_of_interest=ROI,\n            buffer_meters=ROI_buffer_meters,\n            simplify_tol_meters=ROI_simplify_tol_meters,\n        )\n\n        # Downsample mesh and transfer active scalars from original mesh to downsampled mesh\n        if downsample_target != 1.0:\n            # TODO try decimate_pro and compare quality and runtime\n            # TODO see if there's a way to preserve the mesh colors\n            # TODO also see this decimation algorithm: https://pyvista.github.io/fast-simplification/\n            self.logger.info(\"Downsampling the mesh\")\n            # Have a temporary mesh so we can use the original mesh to transfer the active scalars to the downsampled one\n            downsampled_mesh_without_textures = self.pyvista_mesh.decimate(\n                target_reduction=(1 - downsample_target)\n            )\n            self.pyvista_mesh = self.transfer_texture(downsampled_mesh_without_textures)\n        self.logger.info(\"Extracting faces from mesh\")\n        # See here for format: https://github.com/pyvista/pyvista-support/issues/96\n        self.faces = self.pyvista_mesh.faces.reshape((-1, 4))[:, 1:4].copy()\n\n    def transfer_texture(self, downsampled_mesh):\n        \"\"\"Transfer texture from original mesh to a downsampled version using KDTree for nearest neighbor point searches\n\n        Args:\n            downsampled_mesh (pv.PolyData): The downsampled version of the original mesh\n\n        Returns:\n            pv.PolyData: The downsampled mesh with the transferred textures\n        \"\"\"\n        # Only transfer textures if there are point based scalars in the original mesh\n        if self.pyvista_mesh.point_data:\n            # Store original mesh points in KDTree for nearest neighbor search\n            kdtree = KDTree(self.pyvista_mesh.points)\n\n            # For ecah point in the downsampled mesh find the nearest neighbor point in the original mesh\n            _, nearest_neighbor_indices = kdtree.query(downsampled_mesh.points)\n\n            # Iterate over all the point based scalars\n            for scalar_name in self.pyvista_mesh.point_data.keys():\n                # Retrieve scalar data of appropriate index using the nearest neighbor indices\n                transferred_scalars = self.pyvista_mesh.point_data[scalar_name][\n                    nearest_neighbor_indices\n                ]\n                # Set the corresponding scalar data in the downsampled mesh\n                downsampled_mesh.point_data[scalar_name] = transferred_scalars\n\n            # Set active mesh of downsampled mesh\n            if self.pyvista_mesh.active_scalars_name:\n                downsampled_mesh.active_scalars_name = (\n                    self.pyvista_mesh.active_scalars_name\n                )\n        else:\n            self.logger.warning(\n                \"Textures not transferred, active scalars data is assoicated with cell data not point data\"\n            )\n        return downsampled_mesh\n\n    def load_transform_to_epsg_4326(\n        self, transform_filename: PATH_TYPE, require_transform: bool = False\n    ):\n        \"\"\"\n        Load the 4x4 transform projects points from their local coordnate system into EPSG:4326,\n        the earth-centered, earth-fixed coordinate frame. This can either be from a CSV file specifying\n        it directly or extracted from a Metashape camera output\n\n        Args\n            transform_filename (PATH_TYPE):\n            require_transform (bool): Does a local-to-global transform file need to be available\"\n        Raises:\n            FileNotFoundError: Cannot find texture file\n            ValueError: Transform file doesn't have 4x4 matrix\n        \"\"\"\n        if transform_filename is None:\n            if require_transform:\n                raise ValueError(\"Transform is required but not provided\")\n            # If not required, do nothing. TODO consider adding a warning\n            return\n\n        elif Path(transform_filename).suffix == \".xml\":\n            self.local_to_epgs_4978_transform = parse_transform_metashape(\n                transform_filename\n            )\n        elif Path(transform_filename).suffix == \".csv\":\n            self.local_to_epgs_4978_transform = np.loadtxt(\n                transform_filename, delimiter=\",\"\n            )\n            if self.local_to_epgs_4978_transform.shape != (4, 4):\n                raise ValueError(\n                    f\"Transform should be (4,4) but is {self.local_to_epgs_4978_transform.shape}\"\n                )\n        else:\n            if require_transform:\n                raise ValueError(\n                    f\"Transform could not be loaded from {transform_filename}\"\n                )\n            # Not set\n            return\n\n    def standardize_texture(self, texture_array: np.ndarray):\n        # TODO consider coercing into a numpy array\n\n        # Check the dimensions\n        if texture_array.ndim == 1:\n            texture_array = np.expand_dims(texture_array, axis=1)\n        elif texture_array.ndim != 2:\n            raise ValueError(\n                f\"Input texture should have 1 or 2 dimensions but instead has {texture_array.ndim}\"\n            )\n        return texture_array\n\n    def get_texture(\n        self,\n        request_vertex_texture: typing.Union[bool, None] = None,\n        try_verts_faces_conversion: bool = True,\n    ):\n        if self.vertex_texture is None and self.face_texture is None:\n            return\n\n        # If this is unset, try to infer it\n        if request_vertex_texture is None:\n            if self.vertex_texture is not None and self.face_texture is not None:\n                raise ValueError(\n                    \"Ambigious which texture is requested, set request_vertex_texture appropriately\"\n                )\n\n            # Assume that the only one available is being requested\n            request_vertex_texture = self.vertex_texture is not None\n\n        if request_vertex_texture:\n            if self.vertex_texture is not None:\n                return self.standardize_texture(self.vertex_texture)\n            elif try_verts_faces_conversion:\n                self.set_texture(self.face_to_vert_texture(self.face_texture))\n                self.vertex_texture\n            else:\n                raise ValueError(\n                    \"Vertex texture not present and conversion was not requested\"\n                )\n        else:\n            if self.face_texture is not None:\n                return self.standardize_texture(self.face_texture)\n            elif try_verts_faces_conversion:\n                face_texture = self.vert_to_face_texture(\n                    self.vertex_texture, discrete=self.is_discrete_texture()\n                )\n                self.set_texture(face_texture)\n                return self.face_texture\n            else:\n                raise ValueError(\n                    \"Face texture not present and conversion was not requested\"\n                )\n\n    def is_discrete_texture(self):\n        return self.IDs_to_labels is not None\n\n    def set_texture(\n        self,\n        texture_array: np.ndarray,\n        IDs_to_labels: typing.Union[None, dict] = None,\n        all_discrete_texture_values: typing.Union[typing.List, None] = None,\n        is_vertex_texture: typing.Union[bool, None] = None,\n        use_derived_IDs_to_labels: bool = False,\n        delete_existing: bool = True,\n    ):\n        \"\"\"Set the internal texture representation\n\n        Args:\n            texture_array (np.ndarray):\n                The array of texture values. The first dimension must be the length of faces or verts. A second dimension is optional.\n            IDs_to_labels (typing.Union[None, dict], optional): Mapping from integer IDs to string names. Defaults to None.\n            all_discrete_texture_values (typing.Union[typing.List, None], optional):\n                Are all the texture values known to be discrete, representing IDs. Computed from the data if not set. Defaults to None.\n            is_vertex_texture (typing.Union[bool, None], optional):\n                Are the texture values supposed to correspond to the vertices. Computed from the data if not set. Defaults to None.\n            use_derived_IDs_to_labels (bool, optional): Use IDs to labels derived from data if not explicitly provided. Defaults to False.\n            delete_existing (bool, optional): Delete the existing texture when the other one (face, vertex) is set. Defaults to True.\n\n        Raises:\n            ValueError: If the size of the texture doesn't match the number of either faces or vertices\n            ValueError: If the number of faces and vertices are the same and is_vertex_texture isn't set\n        \"\"\"\n        texture_array = self.standardize_texture(texture_array)\n        # IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n\n        # If it is not specified whether this is a vertex texture, attempt to infer it from the shape\n        # TODO consider refactoring to check whether it matches the number of one of them,\n        # no matter whether is_vertex_texture is specified\n        if is_vertex_texture is None:\n            # Check that the number of matches face or verts\n            n_values = texture_array.shape[0]\n            n_faces = self.faces.shape[0]\n            n_verts = self.pyvista_mesh.points.shape[0]\n\n            if n_verts == n_faces:\n                raise ValueError(\n                    \"Cannot infer whether texture should be applied to vertices of faces because the number is the same\"\n                )\n            elif n_values == n_verts:\n                is_vertex_texture = True\n            elif n_values == n_faces:\n                is_vertex_texture = False\n            else:\n                raise ValueError(\n                    f\"The number of elements in the texture ({n_values}) did not match the number of faces ({n_faces}) or vertices ({n_verts})\"\n                )\n\n        # Ensure that the actual data type is float, and record label names\n        if texture_array.ndim == 2 and texture_array.shape[1] != 1:\n            # If it is more than one column, it's assumed to be a real-valued\n            # quantity and we try to cast it to a float\n            texture_array = texture_array.astype(float)\n            derived_IDs_to_labels = None\n        else:\n            texture_array, derived_IDs_to_labels = ensure_float_labels(\n                texture_array, full_array=all_discrete_texture_values\n            )\n\n        # If IDs to labels is explicitly provided, trust that\n        # TODO should do some type checking here\n        if isinstance(IDs_to_labels, dict):\n            self.IDs_to_labels = IDs_to_labels\n        # If not, but we can compute it, use that. Otherwise, we might want to force them to be set to None\n        elif use_derived_IDs_to_labels:\n            self.IDs_to_labels = derived_IDs_to_labels\n\n        # Set the appropriate texture and optionally delete the other one\n        if is_vertex_texture:\n            self.vertex_texture = texture_array\n            if delete_existing:\n                self.face_texture = None\n        else:\n            self.face_texture = texture_array\n            if delete_existing:\n                self.vertex_texture = None\n\n    def load_texture(\n        self,\n        texture: typing.Union[str, PATH_TYPE, np.ndarray, None],\n        texture_column_name: typing.Union[None, PATH_TYPE] = None,\n        IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n    ):\n        \"\"\"Sets either self.face_texture or self.vertex_texture to an (n_{faces, verts}, m channels) array. Note that the other\n           one will be left as None\n\n        Args:\n            texture (typing.Union[PATH_TYPE, np.ndarray, None]): This is either a numpy array or a file to one of the following\n                * A numpy array file in \".npy\" format\n                * A vector file readable by geopandas and a label(s) specifying which column to use.\n                  This should be dataset of polygons/multipolygons. Ideally, there should be no overlap between\n                  regions with different labels. These regions may be assigned based on the order of the rows.\n                * A raster file readable by rasterio. We may want to support using a subset of bands\n            texture_column_name: The column to use as the label for a vector data input\n            IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n        \"\"\"\n        # The easy case, a texture is passed in directly\n        if isinstance(texture, np.ndarray):\n            self.set_texture(\n                texture_array=texture,\n                IDs_to_labels=IDs_to_labels,\n                use_derived_IDs_to_labels=True,\n            )\n        # If the texture is None, try to load it from the mesh\n        # Note that this requires us to have not decimated yet\n        elif texture is None:\n            # See if the mesh has a texture, else this will be None\n            texture_array = self.pyvista_mesh.active_scalars\n\n            if texture_array is not None:\n                # Check if this was a really one channel that had to be tiled to\n                # three for saving\n                if len(texture_array.shape) == 2:\n                    min_val_per_row = np.min(texture_array, axis=1)\n                    max_val_per_row = np.max(texture_array, axis=1)\n                    if np.array_equal(min_val_per_row, max_val_per_row):\n                        # This is supposted to be one channel\n                        texture_array = texture_array[:, 0].astype(float)\n                        # Set any values that are the ignore int value to nan\n                texture_array = texture_array.astype(float)\n                texture_array[texture_array == NULL_TEXTURE_INT_VALUE] = np.nan\n\n                self.set_texture(\n                    texture_array,\n                    IDs_to_labels=IDs_to_labels,\n                    use_derived_IDs_to_labels=True,\n                )\n            else:\n                if IDs_to_labels is not None:\n                    self.IDs_to_labels = IDs_to_labels\n                # Assume that no texture will be needed, consider printing a warning\n                self.logger.warn(\"No texture provided\")\n        else:\n            # Try handling all the other supported filetypes\n            texture_array = None\n            all_values = None\n\n            # Name of scalar in the mesh\n            try:\n                self.logger.warn(\n                    \"Trying to read texture as a scalar from the pyvista mesh:\"\n                )\n                texture_array = self.pyvista_mesh[texture]\n                self.logger.warn(\"- success\")\n            except (KeyError, ValueError):\n                self.logger.warn(\"- failed\")\n\n            # Numpy file\n            if texture_array is None:\n                try:\n                    self.logger.warn(\"Trying to read texture as a numpy file:\")\n                    texture_array = np.load(texture, allow_pickle=True)\n                    self.logger.warn(\"- success\")\n                except:\n                    self.logger.warn(\"- failed\")\n\n            # Vector file\n            if texture_array is None:\n                try:\n                    self.logger.warn(\"Trying to read texture as vector file:\")\n                    # TODO IDs to labels should be used here if set so the computed IDs are aligned with that mapping\n                    texture_array, all_values = self.get_values_for_verts_from_vector(\n                        column_names=texture_column_name,\n                        vector_source=texture,\n                    )\n                    self.logger.warn(\"- success\")\n                except IndexError:\n                    self.logger.warn(\"- failed\")\n\n            # Raster file\n            if texture_array is None:\n                try:\n                    # TODO\n                    self.logger.warn(\"Trying to read as texture as raster file: \")\n                    texture_array = self.get_vert_values_from_raster_file(texture)\n                    self.logger.warn(\"- success\")\n                except:\n                    self.logger.warn(\"- failed\")\n\n            # Error out if not set, since we assume the intent was to have a texture at this point\n            if texture_array is None:\n                raise ValueError(f\"Could not load texture for {texture}\")\n\n            # This will error if something is wrong with the texture that was loaded\n            self.set_texture(\n                texture_array,\n                all_discrete_texture_values=all_values,\n                use_derived_IDs_to_labels=True,\n                IDs_to_labels=IDs_to_labels,\n            )\n\n    def select_mesh_ROI(\n        self,\n        region_of_interest: typing.Union[\n            gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None\n        ],\n        buffer_meters: float = 0,\n        simplify_tol_meters: int = 0,\n        default_CRS: pyproj.CRS = pyproj.CRS.from_epsg(4326),\n        return_original_IDs: bool = False,\n    ):\n        \"\"\"Get a subset of the mesh based on geospatial data\n\n        Args:\n            region_of_interest (typing.Union[gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE]):\n                Region of interest. Can be a\n                * dataframe, where all columns will be colapsed\n                * A shapely polygon/multipolygon\n                * A file that can be loaded by geopandas\n            buffer_meters (float, optional): Expand the geometry by this amount of meters. Defaults to 0.\n            simplify_tol_meters (float, optional): Simplify the geometry using this as the tolerance. Defaults to 0.\n            default_CRS (pyproj.CRS, optional): The CRS to use if one isn't provided. Defaults to pyproj.CRS.from_epsg(4326).\n            return_original_IDs (bool, optional): Return the indices into the original mesh. Defaults to False.\n\n        Returns:\n            pyvista.PolyData: The subset of the mesh\n            np.ndarray: The indices of the points in the original mesh (only if return_original_IDs set)\n            np.ndarray: The indices of the faces in the original mesh (only if return_original_IDs set)\n        \"\"\"\n        if region_of_interest is None:\n            return self.pyvista_mesh\n\n        # Get the ROI into a geopandas GeoDataFrame\n        self.logger.info(\"Standardizing ROI\")\n        if isinstance(region_of_interest, gpd.GeoDataFrame):\n            ROI_gpd = region_of_interest\n        elif isinstance(region_of_interest, (Polygon, MultiPolygon)):\n            ROI_gpd = gpd.DataFrame(crs=default_CRS, geometry=[region_of_interest])\n        else:\n            ROI_gpd = gpd.read_file(region_of_interest)\n\n        self.logger.info(\"Dissolving ROI\")\n        # Disolve to ensure there is only one row\n        ROI_gpd = ROI_gpd.dissolve()\n        self.logger.info(\"Setting CRS and buffering ROI\")\n        # Make sure we're using a projected CRS so a buffer can be applied\n        ROI_gpd = ensure_projected_CRS(ROI_gpd)\n        # Apply the buffer, plus the tolerance, to ensure we keep at least the requested region\n        ROI_gpd[\"geometry\"] = ROI_gpd.buffer(buffer_meters + simplify_tol_meters)\n        # Simplify the geometry to reduce the computational load\n        ROI_gpd.geometry = ROI_gpd.geometry.simplify(simplify_tol_meters)\n        self.logger.info(\"Dissolving buffered ROI\")\n        # Disolve again in case\n        ROI_gpd = ROI_gpd.dissolve()\n\n        self.logger.info(\"Extracting verts for dataframe\")\n        # Get the vertices as a dataframe in the same CRS\n        verts_df = self.get_verts_geodataframe(ROI_gpd.crs)\n        self.logger.info(\"Checking intersection of verts with ROI\")\n        # Determine which vertices are within the ROI polygon\n        verts_in_ROI = gpd.tools.overlay(verts_df, ROI_gpd, how=\"intersection\")\n        # Extract the IDs of the set within the polygon\n        vert_inds = verts_in_ROI[\"vert_ID\"].to_numpy()\n\n        self.logger.info(\"Extracting points from pyvista mesh\")\n        # Extract a submesh using these IDs, which is returned as an UnstructuredGrid\n        subset_unstructured_grid = self.pyvista_mesh.extract_points(vert_inds)\n        self.logger.info(\"Extraction surface from subset mesh\")\n        # Convert the unstructured grid to a PolyData (mesh) again\n        subset_mesh = subset_unstructured_grid.extract_surface()\n\n        # If we need the indices into the original mesh, return those\n        if return_original_IDs:\n            try:\n                point_IDs = subset_unstructured_grid[\"vtkOriginalPointIds\"]\n                face_IDs = subset_unstructured_grid[\"vtkOriginalCellIds\"]\n            except KeyError:\n                point_IDs = np.array([])\n                face_IDs = np.array([])\n\n            return (\n                subset_mesh,\n                point_IDs,\n                face_IDs,\n            )\n        # Else return just the mesh\n        return subset_mesh\n\n    def add_label(self, label_name, label_ID):\n        if label_ID is not np.nan:\n            self.IDs_to_labels[label_ID] = label_name\n\n    def get_IDs_to_labels(self):\n        return self.IDs_to_labels\n\n    def get_label_names(self):\n        self.logger.warning(\n            \"This method will be deprecated in favor of get_IDs_to_labels since it doesn't handle non-sequential indices\"\n        )\n        if self.IDs_to_labels is None:\n            return None\n        return list(self.IDs_to_labels.values())\n\n    # Vertex methods\n\n    def transform_vertices(self, transform_4x4: np.ndarray, in_place: bool = False):\n        \"\"\"Apply a transform to the vertex coordinates\n\n        Args:\n            transform_4x4 (np.ndarray): Transform to be applied\n            in_place (bool): Should the vertices be updated for all member objects\n        \"\"\"\n        homogenous_local_points = np.vstack(\n            (self.pyvista_mesh.points.T, np.ones(self.pyvista_mesh.points.shape[0]))\n        )\n        transformed_local_points = transform_4x4 @ homogenous_local_points\n        transformed_local_points = transformed_local_points[:3].T\n\n        # Overwrite existing vertices in both pytorch3d and pyvista mesh\n        if in_place:\n            self.pyvista_mesh.points = transformed_local_points.copy()\n        return transformed_local_points\n\n    def get_vertices_in_CRS(\n        self, output_CRS: pyproj.CRS, force_easting_northing: bool = True\n    ):\n        \"\"\"Return the coordinates of the mesh vertices in a given CRS\n\n        Args:\n            output_CRS (pyproj.CRS): The coordinate reference system to transform to\n            force_easting_northing (bool, optional): Ensure that the returned points are east first, then north\n\n        Returns:\n            np.ndarray: (n_points, 3)\n        \"\"\"\n        # If no CRS is requested, just return the points\n        if output_CRS is None:\n            return self.pyvista_mesh.points\n\n        # The mesh points are defined in an arbitrary local coordinate system but we can transform them to EPGS:4978,\n        # the earth-centered, earth-fixed coordinate system, using an included transform\n        epgs4978_verts = self.transform_vertices(self.local_to_epgs_4978_transform)\n\n        # TODO figure out why this conversion was required. I think it was some typing issue\n        output_CRS = pyproj.CRS.from_epsg(output_CRS.to_epsg())\n        # Build a pyproj transfrormer from EPGS:4978 to the desired CRS\n        transformer = pyproj.Transformer.from_crs(\n            EARTH_CENTERED_EARTH_FIXED_EPSG_CODE, output_CRS\n        )\n\n        # Transform the coordinates\n        verts_in_output_CRS = transformer.transform(\n            xx=epgs4978_verts[:, 0],\n            yy=epgs4978_verts[:, 1],\n            zz=epgs4978_verts[:, 2],\n        )\n        # Stack and transpose\n        verts_in_output_CRS = np.vstack(verts_in_output_CRS).T\n\n        # Pyproj respects the CRS axis ordering, which is northing/easting for most projected coordinate systems\n        # This causes headaches because it's assumed by rasterio and geopandas to be easting/northing\n        # https://rasterio.readthedocs.io/en/stable/api/rasterio.crs.html#rasterio.crs.epsg_treats_as_latlong\n        if force_easting_northing and rio.crs.epsg_treats_as_latlong(output_CRS):\n            # Swap first two columns\n            verts_in_output_CRS = verts_in_output_CRS[:, [1, 0, 2]]\n\n        return verts_in_output_CRS\n\n    def get_verts_geodataframe(self, crs: pyproj.CRS) -&gt; gpd.GeoDataFrame:\n        \"\"\"Obtain the vertices as a dataframe\n\n        Args:\n            crs (pyproj.CRS): The CRS to use\n\n        Returns:\n            gpd.GeoDataFrame: A dataframe with all the vertices\n        \"\"\"\n        # Get the vertices in the same CRS as the geofile\n        verts_in_geopolygon_crs = self.get_vertices_in_CRS(crs)\n\n        df = pd.DataFrame(\n            {\n                \"east\": verts_in_geopolygon_crs[:, 0],\n                \"north\": verts_in_geopolygon_crs[:, 1],\n            }\n        )\n        # Create a column of Point objects to use as the geometry\n        df[\"geometry\"] = gpd.points_from_xy(df[\"east\"], df[\"north\"])\n        points = gpd.GeoDataFrame(df, crs=crs)\n\n        # Add an index column because the normal index will not be preserved in future operations\n        points[VERT_ID] = df.index\n\n        return points\n\n    def get_faces_2d_gdf(\n        self,\n        crs: pyproj.CRS,\n        include_3d_2d_ratio: bool = False,\n        data_dict: dict = {},\n        faces_mask: typing.Union[np.ndarray, None] = None,\n        cache_data: bool = False,\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Get a geodataframe of triangles for the 2D projection of each face of the mesh\n\n        Args:\n            crs (pyproj.CRS):\n                Coordinate reference system of the dataframe\n            include_3d_2d_ratio (bool, optional):\n                Compute the ratio of the 3D area of the face to the 2D area. This relates to the\n                slope of the face relative to horizontal. The computed data will be stored in the\n                column corresponding to the value of RATIO_3D_2D_KEY. Defaults to False.\n            data_dict (dict, optional):\n                Additional information to add to the dataframe. It must be a dict where the keys\n                are the names of the columns and the data is a np.ndarray of n_faces elemenets.\n                Defaults to {}.\n            faces_mask (typing.Union[np.ndarray, None], optional):\n                A binary mask corresponding to which faces to return. Used to improve runtime of\n                creating the dataframe or downstream steps. Defaults to None.\n            cache_data (bool):\n                Whether to cache expensive results in memory as object attributes. Defaults to False.\n\n        Returns:\n            geopandas.GeoDataFrame: A dataframe for each triangular face\n        \"\"\"\n        # Computing this data can be slow, and we might call it multiple times. This is especially\n        # true for doing clustered polygon labeling\n        if cache_data:\n            mesh_hash = self.get_mesh_hash()\n            transform_hash = self.get_transform_hash()\n            faces_mask_hash = hash(\n                faces_mask.tobytes() if faces_mask is not None else 0\n            )\n            # Create a key that uniquely identifies the relavant inputs\n            cache_key = (mesh_hash, transform_hash, faces_mask_hash, crs)\n\n            # See if the face polygons were in the cache. If not, None will be returned\n            cached_values = self.face_polygons_cache.get(cache_key)\n        else:\n            cached_values = None\n\n        if cached_values is not None:\n            face_polygons, faces = cached_values\n            logging.info(\"Using cached face polygons\")\n        else:\n            self.logger.info(\"Computing faces in working CRS\")\n            # Get the mesh vertices in the desired export CRS\n            verts_in_crs = self.get_vertices_in_CRS(crs)\n            # Get a triangle in geospatial coords for each face\n            # (n_faces, 3 points, xyz)\n            faces = verts_in_crs[self.faces]\n\n            # Select only the requested faces\n            if faces_mask is not None:\n                faces = faces[faces_mask]\n\n            # Extract the first two columns and convert them to a list of tuples of tuples\n            faces_2d_tuples = [tuple(map(tuple, a)) for a in faces[..., :2]]\n            face_polygons = [\n                Polygon(face_tuple)\n                for face_tuple in tqdm(\n                    faces_2d_tuples, desc=f\"Converting faces to polygons\"\n                )\n            ]\n            self.logger.info(\"Creating dataframe of faces\")\n\n            if cache_data:\n                # Save computed data to the cache for the future\n                self.face_polygons_cache[cache_key] = (face_polygons, faces)\n\n        # Remove data corresponding to masked faces\n        if faces_mask is not None:\n            data_dict = {k: v[faces_mask] for k, v in data_dict.items()}\n\n        # Compute the ratio between the 3D area and the projected top-down 2D area\n        if include_3d_2d_ratio:\n            if cache_data:\n                # Check if ratios are cached\n                ratios = self.face_2d_3d_ratios_cache.get(cache_key)\n            else:\n                ratios = None\n\n            # Ratios need to be computed\n            if ratios is None:\n                ratios = []\n                for face in tqdm(faces, desc=\"Computing ratio of 3d to 2d area\"):\n                    area, area_2d = compute_3D_triangle_area(face)\n                    ratios.append(area / area_2d)\n\n                if cache_data:\n                    self.face_2d_3d_ratios_cache[cache_key] = ratios\n\n            # Add the ratios to the data dict\n            data_dict[RATIO_3D_2D_KEY] = ratios\n\n        # Create the dataframe\n        faces_gdf = gpd.GeoDataFrame(\n            data=data_dict,\n            geometry=face_polygons,\n            crs=crs,\n        )\n\n        return faces_gdf\n\n    # Transform labels face&lt;-&gt;vertex methods\n\n    def face_to_vert_texture(self, face_IDs):\n        \"\"\"_summary_\n\n        Args:\n            face_IDs (np.array): (n_faces,) The integer IDs of the faces\n        \"\"\"\n        raise NotImplementedError()\n        # TODO figure how to have a NaN class that\n        for i in tqdm(range(self.pyvista_mesh.points.shape[0])):\n            # Find which faces are using this vertex\n            matching = np.sum(self.faces == i, axis=1)\n            # matching_inds = np.where(matching)[0]\n            # matching_IDs = face_IDs[matching_inds]\n            # most_common_ind = Counter(matching_IDs).most_common(1)\n\n    def vert_to_face_texture(self, vert_IDs, discrete=True):\n        if vert_IDs is None:\n            raise ValueError(\"None\")\n\n        vert_IDs = np.squeeze(vert_IDs)\n        if vert_IDs.ndim != 1 and discrete:\n            raise ValueError(\n                f\"Can only perform discrete conversion with one dimensional array but instead had {vert_IDs.ndim}\"\n            )\n\n        # Each row contains the IDs of each vertex\n        values_per_face = vert_IDs[self.faces]\n        if discrete:\n            # Now we need to \"vote\" for the best one\n            max_ID = int(np.nanmax(vert_IDs))\n            # TODO consider using unique if these indices are sparse\n            counts_per_class_per_face = np.array(\n                [np.sum(values_per_face == i, axis=1) for i in range(max_ID + 1)]\n            ).T\n            # Check which entires had no classes reported and mask them out\n            # TODO consider removing these rows beforehand\n            zeros_mask = np.all(counts_per_class_per_face == 0, axis=1)\n            # We want to fairly tiebreak since np.argmax will always take th first index\n            # This is hard to do in a vectorized way, so we just add a small random value\n            # independently to each element\n            counts_per_class_per_face = (\n                counts_per_class_per_face\n                + np.random.random(counts_per_class_per_face.shape) * 0.5\n            )\n            most_common_class_per_face = np.argmax(\n                counts_per_class_per_face, axis=1\n            ).astype(float)\n            # Set any faces with zero counts to nan\n            most_common_class_per_face[zeros_mask] = np.nan\n\n            return most_common_class_per_face\n        else:\n            average_value_per_face = np.mean(values_per_face, axis=1)\n            return average_value_per_face\n\n    # Operations on vector data\n    def get_values_for_verts_from_vector(\n        self,\n        vector_source: typing.Union[gpd.GeoDataFrame, PATH_TYPE],\n        column_names: typing.Union[str, typing.List[str]],\n    ) -&gt; np.ndarray:\n        \"\"\"Get the value from a dataframe for each vertex\n\n        Args:\n            vector_source (typing.Union[gpd.GeoDataFrame, PATH_TYPE]): geo data frame or path to data that can be loaded by geopandas\n            column_names (typing.Union[str, typing.List[str]]): Which columns to obtain data from\n\n        Returns:\n            np.ndarray: Array of values for each vertex if there is one column name or\n            dict[np.ndarray]: A dict mapping from column names to numpy arrays\n        \"\"\"\n        # Lead the vector data if not already provided in memory\n        if isinstance(vector_source, gpd.GeoDataFrame):\n            gdf = vector_source\n        else:\n            # This will error if not readable\n            gdf = gpd.read_file(vector_source)\n\n        # Infer or standardize the column names\n        if column_names is None:\n            # Check if there is only one real column\n            if len(gdf.columns) == 2:\n                column_names = list(filter(lambda x: x != \"geometry\", gdf.columns))\n            else:\n                # Log as well since this may be caught by an exception handler,\n                # and it's a user error that can be corrected\n                self.logger.error(\n                    \"No column name provided and ambigious which column to use\"\n                )\n                raise ValueError(\n                    \"No column name provided and ambigious which column to use\"\n                )\n        # If only one column is provided, make it a one-length list\n        elif isinstance(column_names, str):\n            column_names = [column_names]\n\n        # Get a dataframe of vertices\n        verts_df = self.get_verts_geodataframe(gdf.crs)\n\n        # See which vertices are in the geopolygons\n        points_in_polygons_gdf = gpd.tools.overlay(verts_df, gdf, how=\"intersection\")\n        # Get the index array\n        index_array = points_in_polygons_gdf[VERT_ID].to_numpy()\n\n        # This is one entry per vertex\n        labeled_verts_dict = {}\n        all_values_dict = {}\n        # Extract the data from each\n        for column_name in column_names:\n            # Create an array corresponding to all the points and initialize to NaN\n            column_values = points_in_polygons_gdf[column_name]\n            # TODO clean this up\n            if column_values.dtype == str or column_values.dtype == np.dtype(\"O\"):\n                # TODO be set to the default value for the type of the column\n                null_value = \"null\"\n            elif column_values.dtype == int:\n                null_value = 255\n            else:\n                null_value = np.nan\n            # Create an array, one per vertex, with the null value\n            values = np.full(\n                shape=verts_df.shape[0],\n                dtype=column_values.dtype,\n                fill_value=null_value,\n            )\n            # Assign the labeled values\n            values[index_array] = column_values\n\n            # Record the results\n            labeled_verts_dict[column_name] = values\n            all_values_dict[column_name] = gdf[column_name]\n\n        # If only one name was requested, just return that\n        if len(column_names) == 1:\n            labeled_verts = np.array(list(labeled_verts_dict.values())[0])\n            all_values = np.array(list(all_values_dict.values())[0])\n\n            return labeled_verts, all_values\n        # Else return a dict of all requested values\n        return labeled_verts_dict, all_values_dict\n\n    def save_IDs_to_labels(self, savepath: PATH_TYPE):\n        \"\"\"saves the contents of the IDs_to_labels to the file savepath provided\n\n        Args:\n            savepath (PATH_TYPE): path to the file where the data must be saved\n        \"\"\"\n\n        # Save the classes filename\n        ensure_containing_folder(savepath)\n        if self.is_discrete_texture():\n            self.logger.info(\"discrete texture, saving classes\")\n            self.logger.info(f\"Saving IDs_to_labels to {str(savepath)}\")\n            with open(savepath, \"w\") as outfile_h:\n                json.dump(\n                    self.get_IDs_to_labels(), outfile_h, ensure_ascii=False, indent=4\n                )\n        else:\n            self.logger.warn(\"non-discrete texture, not saving classes\")\n\n    def save_mesh(self, savepath: PATH_TYPE, save_vert_texture: bool = True):\n        # TODO consider moving most of this functionality to a utils file\n        if save_vert_texture:\n            vert_texture = self.get_texture(request_vertex_texture=True)\n            n_channels = vert_texture.shape[1]\n\n            if n_channels == 1:\n                vert_texture = np.nan_to_num(vert_texture, nan=NULL_TEXTURE_INT_VALUE)\n                vert_texture = np.tile(vert_texture, reps=(1, 3))\n            if n_channels &gt; 3:\n                self.logger.warning(\n                    \"Too many channels to save, attempting to treat them as class probabilities and take the argmax\"\n                )\n                # Take the argmax\n                vert_texture = np.nanargmax(vert_texture, axis=1, keepdims=True)\n                # Replace nan with 255\n                vert_texture = np.nan_to_num(vert_texture, nan=NULL_TEXTURE_INT_VALUE)\n                # Expand to the right number of channels\n                vert_texture = np.repeat(vert_texture, repeats=(1, 3))\n\n            vert_texture = vert_texture.astype(np.uint8)\n        else:\n            vert_texture = None\n\n        # Create folder if it doesn't exist\n        ensure_containing_folder(savepath)\n        # Actually save the mesh\n        self.pyvista_mesh.save(savepath, texture=vert_texture)\n        self.save_IDs_to_labels(Path(savepath).stem + \"_IDs_to_labels.json\")\n\n    def label_polygons(\n        self,\n        face_labels: np.ndarray,\n        polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n        face_weighting: typing.Union[None, np.ndarray] = None,\n        sjoin_overlay: bool = True,\n        return_class_labels: bool = True,\n        unknown_class_label: str = \"unknown\",\n        buffer_dist_meters: float = 2.0,\n    ):\n        \"\"\"Assign a class label to polygons using labels per face\n\n        Args:\n            face_labels (np.ndarray): (n_faces,) array of integer labels\n            polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n            face_weighting (typing.Union[None, np.ndarray], optional):\n                (n_faces,) array of scalar weights for each face, to be multiplied with the\n                contribution of this face. Defaults to None.\n            sjoin_overlay (bool, optional):\n                Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n                substaintially faster, but only uses mesh faces that are entirely within the bounds\n                of the polygon, rather than computing the intersecting region for\n                partially-overlapping faces. Defaults to True.\n            return_class_labels: (bool, optional):\n                Return string representation of class labels rather than float. Defaults to True.\n            unknown_class_label (str, optional):\n                Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n            buffer_dist_meters: (Union[float, None], optional)\n                Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n                the region that is this distance in meters from the polygons. Defaults to 2.0.\n\n        Raises:\n            ValueError: if faces_labels or face_weighting is not 1D\n\n        Returns:\n            list(typing.Union[str, int]):\n                (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n                or string values representing the class label\n        \"\"\"\n        # Premptive error checking before expensive operations\n        face_labels = np.squeeze(face_labels)\n        if face_labels.ndim != 1:\n            raise ValueError(\n                f\"Faces labels must be one-dimensional, but is {face_labels.ndim}\"\n            )\n        if face_weighting is not None:\n            face_weighting = np.squeeze(face_weighting)\n            if face_weighting.ndim != 1:\n                raise ValueError(\n                    f\"Faces labels must be one-dimensional, but is {face_weighting.ndim}\"\n                )\n\n        # Ensure that the input is a geopandas dataframe\n        polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n        # Extract just the geometry\n        polygons_gdf = polygons_gdf[[\"geometry\"]]\n\n        # Only get faces for which there is a non-nan label. Otherwise it is just additional compute\n        faces_mask = np.isfinite(face_labels)\n\n        # Get the faces of the mesh as a geopandas dataframe\n        # Include the predicted face labels as a column in the dataframe\n        faces_2d_gdf = self.get_faces_2d_gdf(\n            polygons_gdf.crs,\n            include_3d_2d_ratio=True,\n            data_dict={CLASS_ID_KEY: face_labels},\n            faces_mask=faces_mask,\n            cache_data=True,\n        )\n\n        # If a per-face weighting is provided, multiply that with the 3d to 2d ratio\n        if face_weighting is not None:\n            face_weighting = face_weighting[faces_mask]\n            faces_2d_gdf[\"face_weighting\"] = (\n                faces_2d_gdf[RATIO_3D_2D_KEY] * face_weighting\n            )\n        # If not, just use the ratio\n        else:\n            faces_2d_gdf[\"face_weighting\"] = faces_2d_gdf[RATIO_3D_2D_KEY]\n\n        # Set the precision to avoid approximate coliniearity errors\n        faces_2d_gdf.geometry = shapely.set_precision(\n            faces_2d_gdf.geometry.values, 1e-6\n        )\n        polygons_gdf.geometry = shapely.set_precision(\n            polygons_gdf.geometry.values, 1e-6\n        )\n\n        # Set the ID field so it's available after the overlay operation\n        # Note that polygons_gdf.index is a bad choice, because this df could be a subset of another\n        # one and the index would not start from 0\n        polygons_gdf[\"polygon_ID\"] = np.arange(len(polygons_gdf))\n\n        # Since overlay is expensive, we first discard faces that are not near the polygons\n\n        # Dissolve the polygons to form one ROI\n        merged_polygons = polygons_gdf.dissolve()\n        # Try to decrease the number of elements in the polygon by expanding\n        # and then simplifying the number of elements in the polygon\n        merged_polygons.geometry = merged_polygons.buffer(buffer_dist_meters)\n        merged_polygons.geometry = merged_polygons.simplify(buffer_dist_meters)\n\n        # Determine which face IDs intersect the ROI. This is slow\n        start = time()\n        self.logger.info(\"Starting to subset to ROI\")\n\n        # Check which faces are fully within the buffered regions around the query polygons\n        # Note that using sjoin has been faster than any other approach I've tried, despite seeming\n        # to compute more information than something like gpd.within\n        contained_faces = gpd.sjoin(\n            faces_2d_gdf, merged_polygons, how=\"left\", predicate=\"within\"\n        )[\"index_right\"].notna()\n        faces_2d_gdf = faces_2d_gdf.loc[contained_faces]\n        self.logger.info(f\"Subset to ROI in {time() - start} seconds\")\n\n        start = time()\n        self.logger.info(\"Starting `overlay`\")\n        if sjoin_overlay:\n            overlay = gpd.sjoin(\n                faces_2d_gdf, polygons_gdf, how=\"left\", predicate=\"within\"\n            )\n            self.logger.info(f\"Overlay time with gpd.sjoin: {time() - start}\")\n        else:\n            # Drop faces not included\n            overlay = polygons_gdf.overlay(\n                faces_2d_gdf, how=\"identity\", keep_geom_type=False\n            )\n            self.logger.info(f\"Overlay time with gpd.overlay: {time() - start}\")\n\n        # Drop nan, for geometries that don't intersect the polygons\n        overlay.dropna(inplace=True)\n        # Compute the weighted area for each face, which may have been broken up by the overlay\n        overlay[\"weighted_area\"] = overlay.area * overlay[\"face_weighting\"]\n\n        # Extract only the neccessary columns\n        overlay = overlay.loc[:, [\"polygon_ID\", CLASS_ID_KEY, \"weighted_area\"]]\n        aggregated_data = overlay.groupby([\"polygon_ID\", CLASS_ID_KEY]).agg(np.sum)\n        # Compute the highest weighted class prediction\n        # Modified from https://stackoverflow.com/questions/27914360/python-pandas-idxmax-for-multiple-indexes-in-a-dataframe\n        max_rows = aggregated_data.loc[\n            aggregated_data.groupby([\"polygon_ID\"], sort=False)[\n                \"weighted_area\"\n            ].idxmax()\n        ].reset_index()\n\n        # Make the class predictions a list of IDs with nans where no information is available\n        pred_subset_IDs = max_rows[CLASS_ID_KEY].to_numpy(dtype=float)\n        pred_subset_IDs[max_rows[\"weighted_area\"].to_numpy() == 0] = np.nan\n\n        predicted_class_IDs = np.full(len(polygons_gdf), np.nan)\n        predicted_class_IDs[max_rows[\"polygon_ID\"].to_numpy(dtype=int)] = (\n            pred_subset_IDs\n        )\n        predicted_class_IDs = predicted_class_IDs.tolist()\n\n        # Post-process to string label names if requested and IDs_to_labels exists\n        if return_class_labels and (\n            (IDs_to_labels := self.get_IDs_to_labels()) is not None\n        ):\n            # convert the IDs into labels\n            # Any label marked as nan is set to the unknown class label, since we had no predictions for it\n            predicted_class_IDs = [\n                (IDs_to_labels[int(pi)] if np.isfinite(pi) else unknown_class_label)\n                for pi in predicted_class_IDs\n            ]\n        return predicted_class_IDs\n\n    def export_face_labels_vector(\n        self,\n        face_labels: typing.Union[np.ndarray, None] = None,\n        export_file: PATH_TYPE = None,\n        export_crs: pyproj.CRS = LAT_LON_EPSG_CODE,\n        label_names: typing.Tuple = None,\n        ensure_non_overlapping: bool = False,\n        simplify_tol: float = 0.0,\n        drop_nan: bool = True,\n        vis: bool = True,\n        batched_unary_union_kwargs: typing.Dict = {\n            \"batch_size\": 500000,\n            \"sort_by_loc\": True,\n            \"grid_size\": 0.05,\n            \"simplify_tol\": 0.05,\n        },\n        vis_kwargs: typing.Dict = {},\n    ) -&gt; gpd.GeoDataFrame:\n        \"\"\"Export the labels for each face as a on-per-class multipolygon\n\n        Args:\n            face_labels (np.ndarray):\n                This can either be a 1- or 2-D array. If 1-D, it is (n_faces,) where each element\n                is an integer class label for that face. If 2-D, it's (n_faces, n_classes) and a\n                nonzero element at (i, j) represents a class prediction for the ith faces and jth\n                class\n            export_file (PATH_TYPE, optional):\n                Where to export. The extension must be a filetype that geopandas can write.\n                Defaults to None, if unset, nothing will be written.\n            export_crs (pyproj.CRS, optional): What CRS to export in.. Defaults to pyproj.CRS.from_epsg(4326), lat lon.\n            label_names (typing.Tuple, optional): Optional names, that are indexed by the labels. Defaults to None.\n            ensure_non_overlapping (bool, optional): Should regions where two classes are predicted at different z heights be assigned to one class\n            simplify_tol: (float, optional): Tolerence in meters to use to simplify geometry\n            drop_nan (bool, optional): Don't export the nan class, often used for background\n            vis: should the result be visualzed\n            batched_unary_union_kwargs (dict, optional): Keyword arguments for batched_unary_union_call\n            vis_kwargs: keyword argmument dict for visualization\n\n        Raises:\n            ValueError: If the wrong number of faces labels are provided\n\n        Returns:\n            gpd.GeoDataFrame: Merged data\n        \"\"\"\n        # Compute the working projected CRS\n        # This is important because having things in meters makes things easier\n        self.logger.info(\"Computing working CRS\")\n        lon, lat, _ = self.get_vertices_in_CRS(output_CRS=LAT_LON_EPSG_CODE)[0]\n        working_CRS = get_projected_CRS(lon=lon, lat=lat)\n\n        # Try to extract face labels if not set\n        if face_labels is None:\n            face_labels = self.get_texture(request_vertex_texture=False)\n\n        # Check that the correct number of labels are provided\n        if face_labels.shape[0] != self.faces.shape[0]:\n            raise ValueError()\n\n        # Get the geospatial faces dataframe\n        faces_gdf = self.get_faces_2d_gdf(crs=working_CRS)\n\n        self.logger.info(\"Creating dataframe of multipolygons\")\n\n        # Check how the data is represented, as a 1-D list of integers or one/many-hot encoding\n        face_labels_is_2d = face_labels.ndim == 2 and face_labels.shape[1] != 1\n        if face_labels_is_2d:\n            # Non-null columns\n            unique_IDs = np.nonzero(np.sum(face_labels, axis=0))[1]\n        else:\n            face_labels = np.squeeze(face_labels)\n            unique_IDs = np.unique(face_labels)\n\n        if drop_nan:\n            # Drop nan from the list of IDs\n            unique_IDs = unique_IDs[np.isfinite(unique_IDs)]\n        multipolygon_list = []\n        # For each unique ID, aggregate all the faces together\n        # This is the same as geopandas.groupby, but that is slow and can out of memory easily\n        # due to the large number of polygons\n        # Instead, we replace the default shapely.unary_union with our batched implementation\n        for unique_ID in unique_IDs:\n            if face_labels_is_2d:\n                # Nonzero elements of the column\n                matching_face_mask = face_labels[:, unique_ID] &gt; 0\n            else:\n                # Elements that match the ID in question\n                matching_face_mask = face_labels == unique_ID\n            matching_face_inds = np.nonzero(matching_face_mask)[0]\n            matching_face_polygons = faces_gdf.iloc[matching_face_inds]\n            list_of_polygons = matching_face_polygons.geometry.values\n            multipolygon = batched_unary_union(\n                list_of_polygons, **batched_unary_union_kwargs\n            )\n            multipolygon_list.append(multipolygon)\n\n        working_gdf = gpd.GeoDataFrame(\n            {CLASS_ID_KEY: unique_IDs}, geometry=multipolygon_list, crs=working_CRS\n        )\n\n        if label_names is not None:\n            names = [\n                (label_names[int(ID)] if np.isfinite(ID) else \"nan\")\n                for ID in working_gdf[CLASS_ID_KEY]\n            ]\n            working_gdf[CLASS_NAMES_KEY] = names\n\n        # Simplify the output geometry\n        if simplify_tol &gt; 0.0:\n            self.logger.info(\"Running simplification\")\n            working_gdf.geometry = working_gdf.geometry.simplify(simplify_tol)\n\n        # Make sure that the polygons are non-overlapping\n        if ensure_non_overlapping:\n            # TODO create a version that tie-breaks based on the number of predicted faces for each\n            # class and optionally the ratios of 3D to top-down areas for the input triangles.\n            self.logger.info(\"Ensuring non-overlapping polygons\")\n            working_gdf = ensure_non_overlapping_polygons(working_gdf)\n\n        # Transform from the working crs to export crs\n        export_gdf = working_gdf.to_crs(export_crs)\n\n        # Export if a file is provided\n        if export_file is not None:\n            ensure_containing_folder(export_file)\n            export_gdf.to_file(export_file)\n\n        # Vis if requested\n        if vis:\n            self.logger.info(\"Plotting\")\n            export_gdf.plot(\n                column=CLASS_NAMES_KEY if label_names is not None else CLASS_ID_KEY,\n                aspect=1,\n                legend=True,\n                **vis_kwargs,\n            )\n            plt.show()\n\n        return export_gdf\n\n    # Operations on raster files\n\n    def get_vert_values_from_raster_file(\n        self,\n        raster_file: PATH_TYPE,\n        return_verts_in_CRS: bool = False,\n        nodata_fill_value: float = np.nan,\n    ):\n        \"\"\"Compute the height above groun for each point on the mesh\n\n        Args:\n            raster_file (PATH_TYPE, optional): The path to the geospatial raster file.\n            return_verts_in_CRS (bool, optional): Return the vertices transformed into the raster CRS\n            nodata_fill_value (float, optional): Set data defined by the opened file as NODATAVAL to this value\n\n        Returns:\n            np.ndarray: samples from raster. Either (n_verts,) or (n_verts, n_raster_channels)\n            np.ndarray (optional): (n_verts, 3) the vertices in the raster CRS\n        \"\"\"\n        # Open the DTM file\n        raster = rio.open(raster_file)\n        # Get the mesh points in the coordinate reference system of the DTM\n        verts_in_raster_CRS = self.get_vertices_in_CRS(\n            raster.crs, force_easting_northing=True\n        )\n\n        # Get the points as a list\n        easting_points = verts_in_raster_CRS[:, 0].tolist()\n        northing_points = verts_in_raster_CRS[:, 1].tolist()\n\n        # Zip them together\n        zipped_locations = zip(easting_points, northing_points)\n        sampling_iter = tqdm(\n            zipped_locations,\n            desc=f\"Sampling values from raster {raster_file}\",\n            total=verts_in_raster_CRS.shape[0],\n        )\n        # Sample the raster file and squeeze if single channel\n        sampled_raster_values = np.squeeze(np.array(list(raster.sample(sampling_iter))))\n\n        # Set nodata locations to nan\n        # TODO figure out if it will ever be a problem to take the first value\n        sampled_raster_values[sampled_raster_values == raster.nodatavals[0]] = (\n            nodata_fill_value\n        )\n\n        if return_verts_in_CRS:\n            return sampled_raster_values, verts_in_raster_CRS\n\n        return sampled_raster_values\n\n    def get_height_above_ground(\n        self, DTM_file: PATH_TYPE, threshold: float = None\n    ) -&gt; np.ndarray:\n        \"\"\"Return height above ground for a points in the mesh and a given DTM\n\n        Args:\n            DTM_file (PATH_TYPE): Path to the digital terrain model raster\n            threshold (float, optional):\n                If not None, return a boolean mask for points under this height. Defaults to None.\n\n        Returns:\n            np.ndarray: Either the height above ground or a boolean mask for ground points\n        \"\"\"\n        # Get the height from the DTM and the points in the same CRS\n        DTM_heights, verts_in_raster_CRS = self.get_vert_values_from_raster_file(\n            DTM_file, return_verts_in_CRS=True\n        )\n        # Extract the vertex height as the third channel\n        verts_height = verts_in_raster_CRS[:, 2]\n        # Subtract the two to get the height above ground\n        height_above_ground = verts_height - DTM_heights\n\n        # If the threshold is not None, return a boolean mask that is true for ground points\n        if threshold is not None:\n            # Return boolean mask\n            # TODO see if this will break for nan values\n            return height_above_ground &lt; threshold\n        # Return height above ground\n        return height_above_ground\n\n    def label_ground_class(\n        self,\n        DTM_file: PATH_TYPE,\n        height_above_ground_threshold: float,\n        labels: typing.Union[None, np.ndarray] = None,\n        only_label_existing_labels: bool = True,\n        ground_class_name: str = \"ground\",\n        ground_ID: typing.Union[None, int] = None,\n        set_mesh_texture: bool = False,\n    ) -&gt; np.ndarray:\n        \"\"\"\n        Set vertices to a potentially-new class with a thresholded height above the DTM.\n        TODO, consider handling face textures as well\n\n        Args:\n            DTM_file (PATH_TYPE): Path to the DTM file\n            height_above_ground_threshold (float): Height (meters) above that DTM that points below are considered ground\n            labels (typing.Union[None, np.ndarray], optional): Vertex texture, otherwise will be queried from mesh. Defaults to None.\n            only_label_existing_labels (bool, optional): Only label points that already have non-null labels. Defaults to True.\n            ground_class_name (str, optional): The potentially-new ground class name. Defaults to \"ground\".\n            ground_ID (typing.Union[None, int], optional): What value to use for the ground class. Will be set inteligently if not provided. Defaults to None.\n\n        Returns:\n            np.ndarray: The updated labels\n        \"\"\"\n\n        if labels is None:\n            # Default to using vertex labels since it's the native way to check height above the DTM\n            use_vertex_labels = True\n        elif labels is not None:\n            # Check the size of the input labels and set what type they are. Note this could override existing value\n            if labels.shape[0] == self.pyvista_mesh.points.shape[0]:\n                use_vertex_labels = True\n            elif labels.shape[0] == self.faces.shape[0]:\n                use_vertex_labels = False\n            else:\n                raise ValueError(\n                    \"Labels were provided but didn't match the shape of vertices or faces\"\n                )\n\n        # if a labels are not provided, get it from the mesh\n        if labels is None:\n            # Get the vertex textures from the mesh\n            labels = self.get_texture(\n                request_vertex_texture=use_vertex_labels,\n            )\n\n        # Compute which vertices are part of the ground by thresholding the height above the DTM\n        ground_mask = self.get_height_above_ground(\n            DTM_file=DTM_file, threshold=height_above_ground_threshold\n        )\n        # If we needed a mask for the faces, compute that instead\n        if not use_vertex_labels:\n            ground_mask = self.vert_to_face_texture(ground_mask.astype(int)).astype(\n                bool\n            )\n\n        # Replace only vertices that were previously labeled as something else, to avoid class imbalance\n        if only_label_existing_labels:\n            # Find which vertices are labeled\n            is_labeled = np.isfinite(labels[:, 0])\n            # Find which points are ground that were previously labeled as something else\n            ground_mask = np.logical_and(is_labeled, ground_mask)\n\n        # Get the existing label names\n        IDs_to_labels = self.get_IDs_to_labels()\n\n        if IDs_to_labels is None and ground_ID is None:\n            # This means that the label is continous, so the concept of ID is meaningless\n            ground_ID = np.nan\n        elif IDs_to_labels is not None and ground_class_name in IDs_to_labels:\n            # If the ground class name is already in the list, set newly-predicted vertices to that class\n            ground_ID = IDs_to_labels.find(ground_class_name)\n        elif IDs_to_labels is not None:\n            # If the label names are present, and the class is not already included, add it as the last element\n            if ground_ID is None:\n                # Set it to the first unused ID\n                # TODO improve this since it should be the max plus one\n                ground_ID = len(IDs_to_labels)\n\n        self.add_label(label_name=ground_class_name, label_ID=ground_ID)\n\n        # Replace mask for ground_vertices\n        labels[ground_mask, 0] = ground_ID\n\n        # Optionally apply the texture to the mesh\n        if set_mesh_texture:\n            self.set_texture(labels, use_derived_IDs_to_labels=False)\n\n        return labels\n\n    def get_transform_hash(self):\n        \"\"\"Generates a hash value for the transform to geospatial coordinates\n        Returns:\n            int: A hash value representing transformation.\n        \"\"\"\n        hasher = hashlib.sha256()\n        hasher.update(\n            self.local_to_epgs_4978_transform.tobytes()\n            if self.local_to_epgs_4978_transform is not None\n            else 0\n        )\n        return hasher.hexdigest()\n\n    def get_mesh_hash(self):\n        \"\"\"Generates a hash value for the mesh based on its points and faces\n        Returns:\n            int: A hash value representing the current mesh.\n        \"\"\"\n        hasher = hashlib.sha256()\n        hasher.update(self.pyvista_mesh.points.tobytes())\n        hasher.update(self.pyvista_mesh.faces.tobytes())\n        return hasher.hexdigest()\n\n    def pix2face(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        render_img_scale: float = 1,\n        save_to_cache: bool = False,\n        cache_folder: typing.Union[None, PATH_TYPE] = CACHE_FOLDER,\n    ) -&gt; np.ndarray:\n        \"\"\"Compute the face that a ray from each pixel would intersect for each camera\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                A single camera or set of cameras. For each camera, the correspondences between\n                pixels and the face IDs of the mesh will be computed. The images of all cameras\n                are assumed to be the same size.\n            render_img_scale (float, optional):\n                Create a pix2face map that is this fraction of the original image scale. Defaults\n                to 1.\n            save_to_cache (bool, optional):\n                Should newly-computed values be saved to the cache. This may speed up future operations\n                but can take up 100s of GBs of space. Defaults to False.\n            cache_folder ((PATH_TYPE, None), optional):\n                Where to check for and save to cached data. Only applicable if use_cache=True.\n                Defaults to CACHE_FOLDER\n\n        Returns:\n            np.ndarray: For each camera, there is an array that is the shape of an image and\n            contains the integer face index for the ray originating at that pixel. Any pixel for\n            which the given ray does not intersect a face is given a value of -1. If the input is\n            a single PhotogrammetryCamera, the shape is (h, w). If it's a camera set, then it is\n            (n_cameras, h, w). Note that a one-length camera set will have a leading singleton dim.\n        \"\"\"\n        # If a set of cameras is passed in, call this method on each camera and concatenate\n        # Other derived methods might be able to compute a batch of renders and once, but pyvista\n        # cannot as far as I can tell\n        if isinstance(cameras, PhotogrammetryCameraSet):\n            pix2face_list = [\n                self.pix2face(camera, render_img_scale=render_img_scale)\n                for camera in cameras\n            ]\n            pix2face = np.stack(pix2face_list, axis=0)\n            return pix2face\n\n        ## Single camera case\n\n        # Check if the cache contains a valid pix2face for the camera based on the dependencies\n        # Compute hashes for the mesh and camera to unique identify mesh+camera pair\n        # The cache will generate a unique key for each combination of the dependencies\n        # If the cache generated key matches a cache file on disk, pix2face will be filled with the correct correspondance\n        # If no match is found, recompute pix2face\n        # If there\u2019s an error loading the cached data, then clear the cache's contents, signified by on_error='clear'\n        mesh_hash = self.get_mesh_hash()\n        camera_hash = cameras.get_camera_hash()\n        cacher = ub.Cacher(\n            \"pix2face\",\n            depends=[mesh_hash, camera_hash, render_img_scale],\n            dpath=cache_folder,\n            verbose=0,\n        )\n        pix2face = cacher.tryload(on_error=\"clear\")\n        ## Cache is valid\n        if pix2face is not None:\n            return pix2face\n\n        # This needs to be an attribute of the class because creating a large number of plotters\n        # results in an un-fixable memory leak.\n        # See https://github.com/pyvista/pyvista/issues/2252\n        # The first step is to clear it\n        self.pix2face_plotter.clear()\n        # This is important so there aren't intermediate values\n        self.pix2face_plotter.disable_anti_aliasing()\n        # Set the camera to the corresponding viewpoint\n        self.pix2face_plotter.camera = cameras.get_pyvista_camera()\n\n        ## Compute the base 256 encoding of the face ID\n        n_faces = self.faces.shape[0]\n        ID_values = np.arange(n_faces)\n\n        # determine how many channels will be required to represent the number of faces\n        n_channels = int(np.ceil(np.emath.logn(256, n_faces))) if n_faces != 0 else 0\n        channel_multipliers = [256**i for i in range(n_channels)]\n\n        # Compute the encoding of each value, least significant value first\n        base_256_encoding = [\n            np.mod(np.floor(ID_values / m).astype(int), 256)\n            for m in channel_multipliers\n        ]\n\n        # ensure that there's a multiple of three channels\n        n_padding = n_channels % 3\n        base_256_encoding.extend([np.zeros(n_faces)] * n_padding)\n\n        # Assume that all images are the same size\n        image_size = cameras.get_image_size(image_scale=render_img_scale)\n\n        # Initialize pix2face\n        pix2face = np.zeros(image_size, dtype=int)\n        # Iterate over three-channel chunks. Each will be encoded as RGB and rendered\n        for chunk_ind in range(int(len(base_256_encoding) / 3)):\n            chunk_scalars = np.stack(\n                base_256_encoding[3 * chunk_ind : 3 * (chunk_ind + 1)], axis=1\n            ).astype(np.uint8)\n            # Add the mesh with the associated scalars\n            self.pix2face_plotter.add_mesh(\n                self.pyvista_mesh,\n                scalars=chunk_scalars.copy(),\n                rgb=True,\n                diffuse=0.0,\n                ambient=1.0,\n            )\n\n            # Perform rendering, this is the slow step\n            rendered_img = self.pix2face_plotter.screenshot(\n                window_size=(image_size[1], image_size[0]),\n            )\n            # Take the rendered values and interpret them as the encoded value\n            # Make sure to not try to interpret channels that are not used in the encoding\n            channels_to_decode = min(3, len(channel_multipliers) - 3 * chunk_ind)\n            for i in range(channels_to_decode):\n                channel_multiplier = channel_multipliers[chunk_ind * 3 + i]\n                channel_value = (rendered_img[..., i] * channel_multiplier).astype(int)\n                pix2face += channel_value\n\n        # Mask out pixels for which the mesh was not visible\n        # This is because the background will render as white\n        # If there happen to be an exact power of (256^3) number of faces, the last one may get\n        # erronously masked. This seems like a minimal concern but it could be addressed by adding\n        # another channel or something like that\n        pix2face[pix2face &gt; n_faces] = -1\n\n        if save_to_cache:\n            # Save the most recently computed pix2face correspondance in the cache\n            cacher.save(pix2face)\n\n        return pix2face\n\n    def render_flat(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        render_img_scale: float = 1,\n        **pix2face_kwargs,\n    ):\n        \"\"\"\n        Render the texture from the viewpoint of each camera in cameras. Note that this is a\n        generator so if you want to actually execute the computation, call list(*) on the output\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                Either a single camera or a camera set. The texture will be rendered from the\n                perspective of each one\n            batch_size (int, optional):\n                The batch size for pix2face. Defaults to 1.\n            render_img_scale (float, optional):\n                The rendered image will be this fraction of the original image corresponding to the\n                virtual camera. Defaults to 1.\n\n        Raises:\n            TypeError: If cameras is not the correct type\n\n        Yields:\n            np.ndarray:\n               The pix2face array for the next camera. The shape is\n               (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n        \"\"\"\n        if isinstance(cameras, PhotogrammetryCamera):\n            # Construct a camera set of length one\n            cameras = PhotogrammetryCameraSet([cameras])\n        elif not isinstance(cameras, PhotogrammetryCameraSet):\n            raise TypeError()\n\n        # Get the face texture from the mesh\n        # TODO consider whether the user should be able to pass a texture to this method. It could\n        # make the user's life easier but makes this method more complex\n        face_texture = self.get_texture(\n            request_vertex_texture=False, try_verts_faces_conversion=True\n        )\n        texture_dim = face_texture.shape[1]\n\n        # Iterate over batch of the cameras\n        batch_stop = max(len(cameras) - batch_size + 1, 1)\n        for batch_start in range(0, batch_stop, batch_size):\n            batch_end = batch_start + batch_size\n            batch_cameras = cameras[batch_start:batch_end]\n            # Compute a batch of pix2face correspondences. This is likely the slowest step\n            batch_pix2face = self.pix2face(\n                cameras=batch_cameras,\n                render_img_scale=render_img_scale,\n                **pix2face_kwargs,\n            )\n\n            # Iterate over the batch dimension\n            for pix2face in batch_pix2face:\n                # Record the original shape of the image\n                img_shape = pix2face.shape[:2]\n                # Flatten for indexing\n                pix2face = pix2face.flatten()\n                # Compute which pixels intersected the mesh\n                mesh_pixel_inds = np.where(pix2face != -1)[0]\n                # Initialize and all-nan array\n                rendered_flattened = np.full(\n                    (pix2face.shape[0], texture_dim), fill_value=np.nan\n                )\n                # Fill the values for which correspondences exist\n                rendered_flattened[mesh_pixel_inds] = face_texture[\n                    pix2face[mesh_pixel_inds]\n                ]\n                # reshape to an image, where the last dimension is the texture dimension\n                rendered_img = rendered_flattened.reshape(img_shape + (texture_dim,))\n                yield rendered_img\n\n    def project_images(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        aggregate_img_scale: float = 1,\n        check_null_image: bool = False,\n        **pix2face_kwargs,\n    ):\n        \"\"\"Find the per-face projection for each of a set of images and associated camera\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The cameras to project images from. cam.get_image() will be called on each one\n            batch_size (int, optional):\n                The number of cameras to compute correspondences for at once. Defaults to 1.\n            aggregate_img_scale (float, optional):\n                The scale of pixel-to-face correspondences image, as a fraction of the original\n                image. Lower values lead to better runtimes but decreased precision at content\n                boundaries in the images. Defaults to 1.\n            check_null_image (bool, optional):\n                Only do indexing if there are non-null image values. This adds additional overhead,\n                but can save the expensive operation of indexing in cases where it would be a no-op.\n\n        Yields:\n            np.ndarray: The per-face projection of an image in the camera set\n        \"\"\"\n        n_faces = self.faces.shape[0]\n\n        # Iterate over batch of the cameras\n        batch_stop = max(len(cameras) - batch_size + 1, 1)\n        for batch_start in range(0, batch_stop, batch_size):\n            batch_inds = list(range(batch_start, batch_start + batch_size))\n            batch_cameras = cameras.get_subset_cameras(batch_inds)\n            # Compute a batch of pix2face correspondences. This is likely the slowest step\n            batch_pix2face = self.pix2face(\n                cameras=batch_cameras,\n                render_img_scale=aggregate_img_scale,\n                **pix2face_kwargs,\n            )\n            for i, pix2face in enumerate(batch_pix2face):\n                img = cameras.get_image_by_index(batch_start + i, aggregate_img_scale)\n\n                n_channels = 1 if img.ndim == 2 else img.shape[-1]\n                textured_faces = np.full((n_faces, n_channels), fill_value=np.nan)\n\n                # Only do the expensive indexing step if there are finite values in the image. This is most\n                # significant for sparse detection tasks where some images may have no real data\n                if not check_null_image or np.any(np.isfinite(img)):\n                    flat_img = np.reshape(img, (img.shape[0] * img.shape[1], -1))\n                    flat_pix2face = pix2face.flatten()\n                    # TODO this creates ill-defined behavior if multiple pixels map to the same face\n                    # my guess is the later pixel in the flattened array will override the former\n                    # TODO make sure that null pix2face values are handled properly\n                    textured_faces[flat_pix2face] = flat_img\n                yield textured_faces\n\n    def aggregate_projected_images(\n        self,\n        cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n        batch_size: int = 1,\n        aggregate_img_scale: float = 1,\n        return_all: bool = False,\n        **kwargs,\n    ):\n        \"\"\"Aggregate the imagery from multiple cameras into per-face averges\n\n        Args:\n            cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n                The cameras to aggregate the images from. cam.get_image() will be called on each\n                element.\n            batch_size (int, optional):\n                The number of cameras to compute correspondences for at once. Defaults to 1.\n            aggregate_img_scale (float, optional):\n                The scale of pixel-to-face correspondences image, as a fraction of the original\n                image. Lower values lead to better runtimes but decreased precision at content\n                boundaries in the images. Defaults to 1.\n            return_all (bool, optional):\n                Return the projection of each individual image, rather than just the aggregates.\n                Defaults to False.\n\n        Returns:\n            np.ndarray: (n_faces, n_image_channels) The average projected image per face\n            dict: Additional information, including the summed projections, observations per face,\n                  and potentially each individual projection\n        \"\"\"\n        project_images_generator = self.project_images(\n            cameras=cameras,\n            batch_size=batch_size,\n            aggregate_img_scale=aggregate_img_scale,\n            **kwargs,\n        )\n\n        if return_all:\n            all_projections = []\n\n        # TODO this should be a convenience method\n        n_faces = self.faces.shape[0]\n\n        projection_counts = np.zeros(n_faces)\n        summed_projection = None\n\n        for projection_for_image in tqdm(\n            project_images_generator,\n            total=len(cameras),\n            desc=\"Aggregating projected viewpoints\",\n        ):\n            if return_all:\n                all_projections.append(projection_for_image)\n\n            if summed_projection is None:\n                summed_projection = projection_for_image.astype(float)\n            else:\n                summed_projection = np.nansum(\n                    [summed_projection, projection_for_image], axis=0\n                )\n\n            projected_faces = np.any(np.isfinite(projection_for_image), axis=1).astype(\n                int\n            )\n            projection_counts += projected_faces\n\n        no_projections = projection_counts == 0\n        summed_projection[no_projections] = np.nan\n\n        additional_information = {\n            \"projection_counts\": projection_counts,\n            \"summed_projections\": summed_projection,\n        }\n\n        if return_all:\n            additional_information[\"all_projections\"] = all_projections\n\n        average_projections = np.divide(\n            summed_projection, np.expand_dims(projection_counts, 1)\n        )\n\n        return average_projections, additional_information\n\n    # Visualization and saving methods\n    def vis(\n        self,\n        plotter: pv.Plotter = None,\n        interactive: bool = True,\n        camera_set: PhotogrammetryCameraSet = None,\n        screenshot_filename: PATH_TYPE = None,\n        vis_scalars: typing.Union[None, np.ndarray] = None,\n        mesh_kwargs: typing.Dict = None,\n        interactive_jupyter: bool = False,\n        plotter_kwargs: typing.Dict = {},\n        enable_ssao: bool = True,\n        force_xvfb: bool = False,\n        frustum_scale: float = 2,\n        IDs_to_labels: typing.Union[None, dict] = None,\n    ):\n        \"\"\"Show the mesh and cameras\n\n        Args:\n            plotter (pyvista.Plotter, optional):\n                Plotter to use, else one will be created\n            off_screen (bool, optional):\n                Show offscreen\n            camera_set (PhotogrammetryCameraSet, optional):\n                Cameras to visualize. Defaults to None.\n            screenshot_filename (PATH_TYPE, optional):\n                Filepath to save to, will show interactively if None. Defaults to None.\n            vis_scalars (None, np.ndarray):\n                Scalars to show\n            mesh_kwargs:\n                dict of keyword arguments for the mesh\n            interactive_jupyter (bool):\n                Should jupyter windows be interactive. This doesn't always work, especially on VSCode.\n            plotter_kwargs:\n                dict of keyword arguments for the plotter\n            frustum_scale (float, optional):\n                Size of cameras in world units. Defaults to None.\n            IDs_to_labels ([None, dict], optional):\n                Mapping from IDs to human readable labels for discrete classes. Defaults to the mesh\n                IDs_to_labels if unset.\n        \"\"\"\n        off_screen = (not interactive) or (screenshot_filename is not None)\n\n        # If the IDs to labels is not set, use the default ones for this mesh\n        if IDs_to_labels is None:\n            IDs_to_labels = self.get_IDs_to_labels()\n\n        # Set the mesh kwargs if not set\n        if mesh_kwargs is None:\n            # This needs to be a dict, even if it's empty\n            mesh_kwargs = {}\n\n            # If there are discrete labels, set the colormap and limits inteligently\n            if IDs_to_labels is not None:\n                # Compute the largest ID\n                max_ID = max(IDs_to_labels.keys())\n                if max_ID &lt; 20:\n                    colors = [\n                        matplotlib.colors.to_hex(c)\n                        for c in plt.get_cmap(\n                            (\"tab10\" if max_ID &lt; 10 else \"tab20\")\n                        ).colors\n                    ]\n                    mesh_kwargs[\"cmap\"] = colors[0 : max_ID + 1]\n                    mesh_kwargs[\"clim\"] = (-0.5, max_ID + 0.5)\n\n        # Create the plotter if it's None\n        plotter = create_pv_plotter(\n            off_screen=off_screen, force_xvfb=force_xvfb, plotter=plotter\n        )\n\n        # If the vis scalars are None, use the saved texture\n        if vis_scalars is None:\n            vis_scalars = self.get_texture(\n                # Request vertex texture if both are available\n                request_vertex_texture=(\n                    True\n                    if (\n                        self.vertex_texture is not None\n                        and self.face_texture is not None\n                    )\n                    else None\n                )\n            )\n\n        is_rgb = (\n            self.pyvista_mesh.active_scalars_name == \"RGB\"\n            if vis_scalars is None\n            else (vis_scalars.ndim == 2 and vis_scalars.shape[1] &gt; 1)\n        )\n\n        # Data in the range [0, 255] must be uint8 type\n        if is_rgb and np.max(vis_scalars) &gt; 1.0:\n            vis_scalars = np.clip(vis_scalars, 0, 255).astype(np.uint8)\n\n        scalar_bar_args = {\"vertical\": True}\n        if IDs_to_labels is not None and \"annotations\" not in mesh_kwargs:\n            mesh_kwargs[\"annotations\"] = IDs_to_labels\n            scalar_bar_args[\"n_labels\"] = 0\n\n        if \"jupyter_backend\" not in plotter_kwargs:\n            if interactive_jupyter:\n                plotter_kwargs[\"jupyter_backend\"] = \"trame\"\n            else:\n                plotter_kwargs[\"jupyter_backend\"] = \"static\"\n\n        # Add the mesh\n        plotter.add_mesh(\n            self.pyvista_mesh,\n            scalars=vis_scalars,\n            rgb=is_rgb,\n            scalar_bar_args=scalar_bar_args,\n            **mesh_kwargs,\n        )\n        # If the camera set is provided, show this too\n        if camera_set is not None:\n            # Adjust the frustum scale if the mesh came from metashape\n            # Find the cube root of the determinant of the upper-left 3x3 submatrix to find the scaling factor\n            if (\n                self.local_to_epgs_4978_transform is not None\n                and frustum_scale is not None\n            ):\n                transform_determinant = np.linalg.det(\n                    self.local_to_epgs_4978_transform[:3, :3]\n                )\n                scale_factor = np.cbrt(transform_determinant)\n                frustum_scale = frustum_scale / scale_factor\n            camera_set.vis(\n                plotter, add_orientation_cube=False, frustum_scale=frustum_scale\n            )\n\n        # Enable screen space shading\n        if enable_ssao:\n            plotter.enable_ssao()\n\n        # Create parent folder if none exists\n        if screenshot_filename is not None:\n            ensure_containing_folder(screenshot_filename)\n\n        # Show\n        return plotter.show(\n            screenshot=screenshot_filename,\n            title=\"Geograypher mesh viewer\",\n            **plotter_kwargs,\n        )\n\n    def save_renders(\n        self,\n        camera_set: PhotogrammetryCameraSet,\n        render_image_scale=1.0,\n        output_folder: PATH_TYPE = Path(VIS_FOLDER, \"renders\"),\n        make_composites: bool = False,\n        save_native_resolution: bool = False,\n        cast_to_uint8: bool = True,\n        uint8_value_for_null_texture: np.uint8 = NULL_TEXTURE_INT_VALUE,\n        **render_kwargs,\n    ):\n        \"\"\"Render an image from the viewpoint of each specified camera and save a composite\n\n        Args:\n            camera_set (PhotogrammetryCameraSet):\n                Camera set to use for rendering\n            render_image_scale (float, optional):\n                Multiplier on the real image scale to obtain size for rendering. Lower values\n                yield a lower-resolution render but the runtime is quiker. Defaults to 1.0.\n            render_folder (PATH_TYPE, optional):\n                Save images to this folder. Defaults to Path(VIS_FOLDER, \"renders\")\n            make_composites (bool, optional):\n                Should a triple pane composite with the original image be saved rather than the\n                raw label\n            cast_to_uint8: (bool, optional):\n                cast the float valued data to unit8 for saving efficiency. May dramatically increase\n                efficiency due to png compression\n            uint8_value_for_null_texture (np.uint8, optional):\n                What value to assign for values that can't be represented as unsigned 8-bit data.\n                Defaults to NULL_TEXTURE_INT_VALUE\n            render_kwargs:\n                keyword arguments passed to the render.\n        \"\"\"\n\n        ensure_folder(output_folder)\n        self.logger.info(f\"Saving renders to {output_folder}\")\n\n        # Save the classes filename\n        self.save_IDs_to_labels(Path(output_folder, \"IDs_to_labels.json\"))\n\n        # Create the generator object to render the images\n        # Since this is a generator, this will be fast\n        render_gen = self.render_flat(\n            camera_set, render_img_scale=render_image_scale, **render_kwargs\n        )\n\n        # The computation only happens when items are requested from the generator\n        for i, rendered in enumerate(\n            tqdm(\n                render_gen,\n                total=len(camera_set),\n                desc=\"Computing and saving renders\",\n            )\n        ):\n            ## All this is post-processing to visualize the rendered label.\n            # rendered could either be a one channel image of integer IDs,\n            # a one-channel image of scalars, or a three-channel image of\n            # RGB. It could also be multi-channel image corresponding to anything,\n            # but we don't expect that yet\n\n            if save_native_resolution and render_image_scale != 1:\n                native_size = camera_set[i].get_image_size()\n                # Upsample using nearest neighbor interpolation for discrete labels and\n                # bilinear for non-discrete\n                # TODO this will need to be fixed for multi-channel images since I don't think resize works\n                rendered = resize(\n                    rendered,\n                    native_size,\n                    order=(0 if self.is_discrete_texture() else 1),\n                )\n\n            if make_composites:\n                RGB_image = camera_set[i].get_image(\n                    image_scale=(1.0 if save_native_resolution else render_image_scale)\n                )\n                rendered = create_composite(\n                    RGB_image=RGB_image,\n                    label_image=rendered,\n                    IDs_to_labels=self.get_IDs_to_labels(),\n                )\n            else:\n                # Clip channels if needed\n                if rendered.ndim == 3:\n                    rendered = rendered[..., :3]\n\n            if cast_to_uint8:\n                # Deterimine values that cannot be represented as uint8\n                mask = np.logical_or.reduce(\n                    [\n                        rendered &lt; 0,\n                        rendered &gt; 255,\n                        np.logical_not(np.isfinite(rendered)),\n                    ]\n                )\n                rendered[mask] = uint8_value_for_null_texture\n                # Cast and squeeze since you can't save a one-channel image\n                rendered = np.squeeze(rendered.astype(np.uint8))\n\n            # Saving\n            output_filename = Path(\n                output_folder, camera_set.get_image_filename(i, absolute=False)\n            )\n            # This may create nested folders in the output dir\n            ensure_containing_folder(output_filename)\n            if rendered.dtype == np.uint8:\n                output_filename = str(output_filename.with_suffix(\".png\"))\n\n                # Save the image\n                skimage.io.imsave(output_filename, rendered, check_contrast=False)\n            else:\n                output_filename = str(output_filename.with_suffix(\".npy\"))\n                # Save the image\n                np.save(output_filename, rendered)\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh-functions","title":"Functions","text":""},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.__init__","title":"<code>__init__(mesh, downsample_target=1.0, transform_filename=None, texture=None, texture_column_name=None, IDs_to_labels=None, ROI=None, ROI_buffer_meters=0, require_transform=False, log_level='INFO')</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Union[PATH_TYPE, PolyData]</code> <p>Path to the mesh, in a format pyvista can read, or pyvista mesh</p> required <code>downsample_target</code> <code>float</code> <p>Downsample to this fraction of vertices. Defaults to 1.0.</p> <code>1.0</code> <code>texture</code> <code>Union[PATH_TYPE, ndarray, None]</code> <p>Texture or path to one. See more details in <code>load_texture</code> documentation</p> <code>None</code> <code>texture_column_name</code> <code>Union[PATH_TYPE, None]</code> <p>The name of the column to use for a vectorfile input</p> <code>None</code> <code>IDs_to_labels</code> <code>Union[PATH_TYPE, dict, None]</code> <p>dictionary or JSON file containing the mapping from integer IDs to string class names</p> <code>None</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def __init__(\n    self,\n    mesh: typing.Union[PATH_TYPE, pv.PolyData],\n    downsample_target: float = 1.0,\n    transform_filename: PATH_TYPE = None,\n    texture: typing.Union[PATH_TYPE, np.ndarray, None] = None,\n    texture_column_name: typing.Union[PATH_TYPE, None] = None,\n    IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n    ROI=None,\n    ROI_buffer_meters: float = 0,\n    require_transform: bool = False,\n    log_level: str = \"INFO\",\n):\n    \"\"\"_summary_\n\n    Args:\n        mesh (typing.Union[PATH_TYPE, pv.PolyData]): Path to the mesh, in a format pyvista can read, or pyvista mesh\n        downsample_target (float, optional): Downsample to this fraction of vertices. Defaults to 1.0.\n        texture (typing.Union[PATH_TYPE, np.ndarray, None]): Texture or path to one. See more details in `load_texture` documentation\n        texture_column_name: The name of the column to use for a vectorfile input\n        IDs_to_labels (typing.Union[PATH_TYPE, dict, None]): dictionary or JSON file containing the mapping from integer IDs to string class names\n    \"\"\"\n    self.downsample_target = downsample_target\n\n    self.pyvista_mesh = None\n    self.texture = None\n    self.vertex_texture = None\n    self.face_texture = None\n    self.local_to_epgs_4978_transform = None\n    self.IDs_to_labels = None\n    # Create the plotter that will later be used to compute correspondences between pixels\n    # and the mesh. Note that this is only done to prevent a memory leak from creating multiple\n    # plotters. See https://github.com/pyvista/pyvista/issues/2252\n    self.pix2face_plotter = create_pv_plotter(off_screen=True)\n    self.face_polygons_cache = {}\n    self.face_2d_3d_ratios_cache = {}\n\n    self.logger = logging.getLogger(f\"mesh_{id(self)}\")\n    self.logger.setLevel(log_level)\n    # Potentially necessary for Jupyter\n    # https://stackoverflow.com/questions/35936086/jupyter-notebook-does-not-print-logs-to-the-output-cell\n    # If you don't check that there's already a handler, you can have situations with duplicated\n    # print outs if you have multiple mesh objects\n    if not self.logger.hasHandlers():\n        self.logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n\n    # Load the transform\n    self.logger.info(\"Loading transform to EPSG:4326\")\n    self.load_transform_to_epsg_4326(\n        transform_filename, require_transform=require_transform\n    )\n    # Load the mesh with the pyvista loader\n    self.logger.info(\"Loading mesh\")\n    self.load_mesh(\n        mesh=mesh,\n        downsample_target=downsample_target,\n        ROI=ROI,\n        ROI_buffer_meters=ROI_buffer_meters,\n    )\n    # Load the texture\n    self.logger.info(\"Loading texture\")\n    # load IDs_to_labels\n    # if IDs_to_labels not provided, check the directory of the mesh and get the file if found\n    if IDs_to_labels is None and isinstance(mesh, PATH_TYPE.__args__):\n        possible_json = Path(Path(mesh).stem + \"_IDs_to_labels.json\")\n        if possible_json.exists():\n            IDs_to_labels = possible_json\n    # convert IDs_to_labels from file to dict\n    if isinstance(IDs_to_labels, PATH_TYPE.__args__):\n        with open(IDs_to_labels, \"r\") as file:\n            IDs_to_labels = json.load(file)\n            IDs_to_labels = {int(id): label for id, label in IDs_to_labels.items()}\n    self.load_texture(texture, texture_column_name, IDs_to_labels=IDs_to_labels)\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.aggregate_projected_images","title":"<code>aggregate_projected_images(cameras, batch_size=1, aggregate_img_scale=1, return_all=False, **kwargs)</code>","text":"<p>Aggregate the imagery from multiple cameras into per-face averges</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The cameras to aggregate the images from. cam.get_image() will be called on each element.</p> required <code>batch_size</code> <code>int</code> <p>The number of cameras to compute correspondences for at once. Defaults to 1.</p> <code>1</code> <code>aggregate_img_scale</code> <code>float</code> <p>The scale of pixel-to-face correspondences image, as a fraction of the original image. Lower values lead to better runtimes but decreased precision at content boundaries in the images. Defaults to 1.</p> <code>1</code> <code>return_all</code> <code>bool</code> <p>Return the projection of each individual image, rather than just the aggregates. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <p>np.ndarray: (n_faces, n_image_channels) The average projected image per face</p> <code>dict</code> <p>Additional information, including the summed projections, observations per face,   and potentially each individual projection</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def aggregate_projected_images(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    aggregate_img_scale: float = 1,\n    return_all: bool = False,\n    **kwargs,\n):\n    \"\"\"Aggregate the imagery from multiple cameras into per-face averges\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The cameras to aggregate the images from. cam.get_image() will be called on each\n            element.\n        batch_size (int, optional):\n            The number of cameras to compute correspondences for at once. Defaults to 1.\n        aggregate_img_scale (float, optional):\n            The scale of pixel-to-face correspondences image, as a fraction of the original\n            image. Lower values lead to better runtimes but decreased precision at content\n            boundaries in the images. Defaults to 1.\n        return_all (bool, optional):\n            Return the projection of each individual image, rather than just the aggregates.\n            Defaults to False.\n\n    Returns:\n        np.ndarray: (n_faces, n_image_channels) The average projected image per face\n        dict: Additional information, including the summed projections, observations per face,\n              and potentially each individual projection\n    \"\"\"\n    project_images_generator = self.project_images(\n        cameras=cameras,\n        batch_size=batch_size,\n        aggregate_img_scale=aggregate_img_scale,\n        **kwargs,\n    )\n\n    if return_all:\n        all_projections = []\n\n    # TODO this should be a convenience method\n    n_faces = self.faces.shape[0]\n\n    projection_counts = np.zeros(n_faces)\n    summed_projection = None\n\n    for projection_for_image in tqdm(\n        project_images_generator,\n        total=len(cameras),\n        desc=\"Aggregating projected viewpoints\",\n    ):\n        if return_all:\n            all_projections.append(projection_for_image)\n\n        if summed_projection is None:\n            summed_projection = projection_for_image.astype(float)\n        else:\n            summed_projection = np.nansum(\n                [summed_projection, projection_for_image], axis=0\n            )\n\n        projected_faces = np.any(np.isfinite(projection_for_image), axis=1).astype(\n            int\n        )\n        projection_counts += projected_faces\n\n    no_projections = projection_counts == 0\n    summed_projection[no_projections] = np.nan\n\n    additional_information = {\n        \"projection_counts\": projection_counts,\n        \"summed_projections\": summed_projection,\n    }\n\n    if return_all:\n        additional_information[\"all_projections\"] = all_projections\n\n    average_projections = np.divide(\n        summed_projection, np.expand_dims(projection_counts, 1)\n    )\n\n    return average_projections, additional_information\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.export_face_labels_vector","title":"<code>export_face_labels_vector(face_labels=None, export_file=None, export_crs=LAT_LON_EPSG_CODE, label_names=None, ensure_non_overlapping=False, simplify_tol=0.0, drop_nan=True, vis=True, batched_unary_union_kwargs={'batch_size': 500000, 'sort_by_loc': True, 'grid_size': 0.05, 'simplify_tol': 0.05}, vis_kwargs={})</code>","text":"<p>Export the labels for each face as a on-per-class multipolygon</p> <p>Parameters:</p> Name Type Description Default <code>face_labels</code> <code>ndarray</code> <p>This can either be a 1- or 2-D array. If 1-D, it is (n_faces,) where each element is an integer class label for that face. If 2-D, it's (n_faces, n_classes) and a nonzero element at (i, j) represents a class prediction for the ith faces and jth class</p> <code>None</code> <code>export_file</code> <code>PATH_TYPE</code> <p>Where to export. The extension must be a filetype that geopandas can write. Defaults to None, if unset, nothing will be written.</p> <code>None</code> <code>export_crs</code> <code>CRS</code> <p>What CRS to export in.. Defaults to pyproj.CRS.from_epsg(4326), lat lon.</p> <code>LAT_LON_EPSG_CODE</code> <code>label_names</code> <code>Tuple</code> <p>Optional names, that are indexed by the labels. Defaults to None.</p> <code>None</code> <code>ensure_non_overlapping</code> <code>bool</code> <p>Should regions where two classes are predicted at different z heights be assigned to one class</p> <code>False</code> <code>simplify_tol</code> <code>float</code> <p>(float, optional): Tolerence in meters to use to simplify geometry</p> <code>0.0</code> <code>drop_nan</code> <code>bool</code> <p>Don't export the nan class, often used for background</p> <code>True</code> <code>vis</code> <code>bool</code> <p>should the result be visualzed</p> <code>True</code> <code>batched_unary_union_kwargs</code> <code>dict</code> <p>Keyword arguments for batched_unary_union_call</p> <code>{'batch_size': 500000, 'sort_by_loc': True, 'grid_size': 0.05, 'simplify_tol': 0.05}</code> <code>vis_kwargs</code> <code>Dict</code> <p>keyword argmument dict for visualization</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the wrong number of faces labels are provided</p> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: Merged data</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def export_face_labels_vector(\n    self,\n    face_labels: typing.Union[np.ndarray, None] = None,\n    export_file: PATH_TYPE = None,\n    export_crs: pyproj.CRS = LAT_LON_EPSG_CODE,\n    label_names: typing.Tuple = None,\n    ensure_non_overlapping: bool = False,\n    simplify_tol: float = 0.0,\n    drop_nan: bool = True,\n    vis: bool = True,\n    batched_unary_union_kwargs: typing.Dict = {\n        \"batch_size\": 500000,\n        \"sort_by_loc\": True,\n        \"grid_size\": 0.05,\n        \"simplify_tol\": 0.05,\n    },\n    vis_kwargs: typing.Dict = {},\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Export the labels for each face as a on-per-class multipolygon\n\n    Args:\n        face_labels (np.ndarray):\n            This can either be a 1- or 2-D array. If 1-D, it is (n_faces,) where each element\n            is an integer class label for that face. If 2-D, it's (n_faces, n_classes) and a\n            nonzero element at (i, j) represents a class prediction for the ith faces and jth\n            class\n        export_file (PATH_TYPE, optional):\n            Where to export. The extension must be a filetype that geopandas can write.\n            Defaults to None, if unset, nothing will be written.\n        export_crs (pyproj.CRS, optional): What CRS to export in.. Defaults to pyproj.CRS.from_epsg(4326), lat lon.\n        label_names (typing.Tuple, optional): Optional names, that are indexed by the labels. Defaults to None.\n        ensure_non_overlapping (bool, optional): Should regions where two classes are predicted at different z heights be assigned to one class\n        simplify_tol: (float, optional): Tolerence in meters to use to simplify geometry\n        drop_nan (bool, optional): Don't export the nan class, often used for background\n        vis: should the result be visualzed\n        batched_unary_union_kwargs (dict, optional): Keyword arguments for batched_unary_union_call\n        vis_kwargs: keyword argmument dict for visualization\n\n    Raises:\n        ValueError: If the wrong number of faces labels are provided\n\n    Returns:\n        gpd.GeoDataFrame: Merged data\n    \"\"\"\n    # Compute the working projected CRS\n    # This is important because having things in meters makes things easier\n    self.logger.info(\"Computing working CRS\")\n    lon, lat, _ = self.get_vertices_in_CRS(output_CRS=LAT_LON_EPSG_CODE)[0]\n    working_CRS = get_projected_CRS(lon=lon, lat=lat)\n\n    # Try to extract face labels if not set\n    if face_labels is None:\n        face_labels = self.get_texture(request_vertex_texture=False)\n\n    # Check that the correct number of labels are provided\n    if face_labels.shape[0] != self.faces.shape[0]:\n        raise ValueError()\n\n    # Get the geospatial faces dataframe\n    faces_gdf = self.get_faces_2d_gdf(crs=working_CRS)\n\n    self.logger.info(\"Creating dataframe of multipolygons\")\n\n    # Check how the data is represented, as a 1-D list of integers or one/many-hot encoding\n    face_labels_is_2d = face_labels.ndim == 2 and face_labels.shape[1] != 1\n    if face_labels_is_2d:\n        # Non-null columns\n        unique_IDs = np.nonzero(np.sum(face_labels, axis=0))[1]\n    else:\n        face_labels = np.squeeze(face_labels)\n        unique_IDs = np.unique(face_labels)\n\n    if drop_nan:\n        # Drop nan from the list of IDs\n        unique_IDs = unique_IDs[np.isfinite(unique_IDs)]\n    multipolygon_list = []\n    # For each unique ID, aggregate all the faces together\n    # This is the same as geopandas.groupby, but that is slow and can out of memory easily\n    # due to the large number of polygons\n    # Instead, we replace the default shapely.unary_union with our batched implementation\n    for unique_ID in unique_IDs:\n        if face_labels_is_2d:\n            # Nonzero elements of the column\n            matching_face_mask = face_labels[:, unique_ID] &gt; 0\n        else:\n            # Elements that match the ID in question\n            matching_face_mask = face_labels == unique_ID\n        matching_face_inds = np.nonzero(matching_face_mask)[0]\n        matching_face_polygons = faces_gdf.iloc[matching_face_inds]\n        list_of_polygons = matching_face_polygons.geometry.values\n        multipolygon = batched_unary_union(\n            list_of_polygons, **batched_unary_union_kwargs\n        )\n        multipolygon_list.append(multipolygon)\n\n    working_gdf = gpd.GeoDataFrame(\n        {CLASS_ID_KEY: unique_IDs}, geometry=multipolygon_list, crs=working_CRS\n    )\n\n    if label_names is not None:\n        names = [\n            (label_names[int(ID)] if np.isfinite(ID) else \"nan\")\n            for ID in working_gdf[CLASS_ID_KEY]\n        ]\n        working_gdf[CLASS_NAMES_KEY] = names\n\n    # Simplify the output geometry\n    if simplify_tol &gt; 0.0:\n        self.logger.info(\"Running simplification\")\n        working_gdf.geometry = working_gdf.geometry.simplify(simplify_tol)\n\n    # Make sure that the polygons are non-overlapping\n    if ensure_non_overlapping:\n        # TODO create a version that tie-breaks based on the number of predicted faces for each\n        # class and optionally the ratios of 3D to top-down areas for the input triangles.\n        self.logger.info(\"Ensuring non-overlapping polygons\")\n        working_gdf = ensure_non_overlapping_polygons(working_gdf)\n\n    # Transform from the working crs to export crs\n    export_gdf = working_gdf.to_crs(export_crs)\n\n    # Export if a file is provided\n    if export_file is not None:\n        ensure_containing_folder(export_file)\n        export_gdf.to_file(export_file)\n\n    # Vis if requested\n    if vis:\n        self.logger.info(\"Plotting\")\n        export_gdf.plot(\n            column=CLASS_NAMES_KEY if label_names is not None else CLASS_ID_KEY,\n            aspect=1,\n            legend=True,\n            **vis_kwargs,\n        )\n        plt.show()\n\n    return export_gdf\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.face_to_vert_texture","title":"<code>face_to_vert_texture(face_IDs)</code>","text":"<p>summary</p> <p>Parameters:</p> Name Type Description Default <code>face_IDs</code> <code>array</code> <p>(n_faces,) The integer IDs of the faces</p> required Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def face_to_vert_texture(self, face_IDs):\n    \"\"\"_summary_\n\n    Args:\n        face_IDs (np.array): (n_faces,) The integer IDs of the faces\n    \"\"\"\n    raise NotImplementedError()\n    # TODO figure how to have a NaN class that\n    for i in tqdm(range(self.pyvista_mesh.points.shape[0])):\n        # Find which faces are using this vertex\n        matching = np.sum(self.faces == i, axis=1)\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_faces_2d_gdf","title":"<code>get_faces_2d_gdf(crs, include_3d_2d_ratio=False, data_dict={}, faces_mask=None, cache_data=False)</code>","text":"<p>Get a geodataframe of triangles for the 2D projection of each face of the mesh</p> <p>Parameters:</p> Name Type Description Default <code>crs</code> <code>CRS</code> <p>Coordinate reference system of the dataframe</p> required <code>include_3d_2d_ratio</code> <code>bool</code> <p>Compute the ratio of the 3D area of the face to the 2D area. This relates to the slope of the face relative to horizontal. The computed data will be stored in the column corresponding to the value of RATIO_3D_2D_KEY. Defaults to False.</p> <code>False</code> <code>data_dict</code> <code>dict</code> <p>Additional information to add to the dataframe. It must be a dict where the keys are the names of the columns and the data is a np.ndarray of n_faces elemenets. Defaults to {}.</p> <code>{}</code> <code>faces_mask</code> <code>Union[ndarray, None]</code> <p>A binary mask corresponding to which faces to return. Used to improve runtime of creating the dataframe or downstream steps. Defaults to None.</p> <code>None</code> <code>cache_data</code> <code>bool</code> <p>Whether to cache expensive results in memory as object attributes. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>geopandas.GeoDataFrame: A dataframe for each triangular face</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_faces_2d_gdf(\n    self,\n    crs: pyproj.CRS,\n    include_3d_2d_ratio: bool = False,\n    data_dict: dict = {},\n    faces_mask: typing.Union[np.ndarray, None] = None,\n    cache_data: bool = False,\n) -&gt; gpd.GeoDataFrame:\n    \"\"\"Get a geodataframe of triangles for the 2D projection of each face of the mesh\n\n    Args:\n        crs (pyproj.CRS):\n            Coordinate reference system of the dataframe\n        include_3d_2d_ratio (bool, optional):\n            Compute the ratio of the 3D area of the face to the 2D area. This relates to the\n            slope of the face relative to horizontal. The computed data will be stored in the\n            column corresponding to the value of RATIO_3D_2D_KEY. Defaults to False.\n        data_dict (dict, optional):\n            Additional information to add to the dataframe. It must be a dict where the keys\n            are the names of the columns and the data is a np.ndarray of n_faces elemenets.\n            Defaults to {}.\n        faces_mask (typing.Union[np.ndarray, None], optional):\n            A binary mask corresponding to which faces to return. Used to improve runtime of\n            creating the dataframe or downstream steps. Defaults to None.\n        cache_data (bool):\n            Whether to cache expensive results in memory as object attributes. Defaults to False.\n\n    Returns:\n        geopandas.GeoDataFrame: A dataframe for each triangular face\n    \"\"\"\n    # Computing this data can be slow, and we might call it multiple times. This is especially\n    # true for doing clustered polygon labeling\n    if cache_data:\n        mesh_hash = self.get_mesh_hash()\n        transform_hash = self.get_transform_hash()\n        faces_mask_hash = hash(\n            faces_mask.tobytes() if faces_mask is not None else 0\n        )\n        # Create a key that uniquely identifies the relavant inputs\n        cache_key = (mesh_hash, transform_hash, faces_mask_hash, crs)\n\n        # See if the face polygons were in the cache. If not, None will be returned\n        cached_values = self.face_polygons_cache.get(cache_key)\n    else:\n        cached_values = None\n\n    if cached_values is not None:\n        face_polygons, faces = cached_values\n        logging.info(\"Using cached face polygons\")\n    else:\n        self.logger.info(\"Computing faces in working CRS\")\n        # Get the mesh vertices in the desired export CRS\n        verts_in_crs = self.get_vertices_in_CRS(crs)\n        # Get a triangle in geospatial coords for each face\n        # (n_faces, 3 points, xyz)\n        faces = verts_in_crs[self.faces]\n\n        # Select only the requested faces\n        if faces_mask is not None:\n            faces = faces[faces_mask]\n\n        # Extract the first two columns and convert them to a list of tuples of tuples\n        faces_2d_tuples = [tuple(map(tuple, a)) for a in faces[..., :2]]\n        face_polygons = [\n            Polygon(face_tuple)\n            for face_tuple in tqdm(\n                faces_2d_tuples, desc=f\"Converting faces to polygons\"\n            )\n        ]\n        self.logger.info(\"Creating dataframe of faces\")\n\n        if cache_data:\n            # Save computed data to the cache for the future\n            self.face_polygons_cache[cache_key] = (face_polygons, faces)\n\n    # Remove data corresponding to masked faces\n    if faces_mask is not None:\n        data_dict = {k: v[faces_mask] for k, v in data_dict.items()}\n\n    # Compute the ratio between the 3D area and the projected top-down 2D area\n    if include_3d_2d_ratio:\n        if cache_data:\n            # Check if ratios are cached\n            ratios = self.face_2d_3d_ratios_cache.get(cache_key)\n        else:\n            ratios = None\n\n        # Ratios need to be computed\n        if ratios is None:\n            ratios = []\n            for face in tqdm(faces, desc=\"Computing ratio of 3d to 2d area\"):\n                area, area_2d = compute_3D_triangle_area(face)\n                ratios.append(area / area_2d)\n\n            if cache_data:\n                self.face_2d_3d_ratios_cache[cache_key] = ratios\n\n        # Add the ratios to the data dict\n        data_dict[RATIO_3D_2D_KEY] = ratios\n\n    # Create the dataframe\n    faces_gdf = gpd.GeoDataFrame(\n        data=data_dict,\n        geometry=face_polygons,\n        crs=crs,\n    )\n\n    return faces_gdf\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_height_above_ground","title":"<code>get_height_above_ground(DTM_file, threshold=None)</code>","text":"<p>Return height above ground for a points in the mesh and a given DTM</p> <p>Parameters:</p> Name Type Description Default <code>DTM_file</code> <code>PATH_TYPE</code> <p>Path to the digital terrain model raster</p> required <code>threshold</code> <code>float</code> <p>If not None, return a boolean mask for points under this height. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Either the height above ground or a boolean mask for ground points</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_height_above_ground(\n    self, DTM_file: PATH_TYPE, threshold: float = None\n) -&gt; np.ndarray:\n    \"\"\"Return height above ground for a points in the mesh and a given DTM\n\n    Args:\n        DTM_file (PATH_TYPE): Path to the digital terrain model raster\n        threshold (float, optional):\n            If not None, return a boolean mask for points under this height. Defaults to None.\n\n    Returns:\n        np.ndarray: Either the height above ground or a boolean mask for ground points\n    \"\"\"\n    # Get the height from the DTM and the points in the same CRS\n    DTM_heights, verts_in_raster_CRS = self.get_vert_values_from_raster_file(\n        DTM_file, return_verts_in_CRS=True\n    )\n    # Extract the vertex height as the third channel\n    verts_height = verts_in_raster_CRS[:, 2]\n    # Subtract the two to get the height above ground\n    height_above_ground = verts_height - DTM_heights\n\n    # If the threshold is not None, return a boolean mask that is true for ground points\n    if threshold is not None:\n        # Return boolean mask\n        # TODO see if this will break for nan values\n        return height_above_ground &lt; threshold\n    # Return height above ground\n    return height_above_ground\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_mesh_hash","title":"<code>get_mesh_hash()</code>","text":"<p>Generates a hash value for the mesh based on its points and faces Returns:     int: A hash value representing the current mesh.</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_mesh_hash(self):\n    \"\"\"Generates a hash value for the mesh based on its points and faces\n    Returns:\n        int: A hash value representing the current mesh.\n    \"\"\"\n    hasher = hashlib.sha256()\n    hasher.update(self.pyvista_mesh.points.tobytes())\n    hasher.update(self.pyvista_mesh.faces.tobytes())\n    return hasher.hexdigest()\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_transform_hash","title":"<code>get_transform_hash()</code>","text":"<p>Generates a hash value for the transform to geospatial coordinates Returns:     int: A hash value representing transformation.</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_transform_hash(self):\n    \"\"\"Generates a hash value for the transform to geospatial coordinates\n    Returns:\n        int: A hash value representing transformation.\n    \"\"\"\n    hasher = hashlib.sha256()\n    hasher.update(\n        self.local_to_epgs_4978_transform.tobytes()\n        if self.local_to_epgs_4978_transform is not None\n        else 0\n    )\n    return hasher.hexdigest()\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_values_for_verts_from_vector","title":"<code>get_values_for_verts_from_vector(vector_source, column_names)</code>","text":"<p>Get the value from a dataframe for each vertex</p> <p>Parameters:</p> Name Type Description Default <code>vector_source</code> <code>Union[GeoDataFrame, PATH_TYPE]</code> <p>geo data frame or path to data that can be loaded by geopandas</p> required <code>column_names</code> <code>Union[str, List[str]]</code> <p>Which columns to obtain data from</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of values for each vertex if there is one column name or</p> <code>ndarray</code> <p>dict[np.ndarray]: A dict mapping from column names to numpy arrays</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_values_for_verts_from_vector(\n    self,\n    vector_source: typing.Union[gpd.GeoDataFrame, PATH_TYPE],\n    column_names: typing.Union[str, typing.List[str]],\n) -&gt; np.ndarray:\n    \"\"\"Get the value from a dataframe for each vertex\n\n    Args:\n        vector_source (typing.Union[gpd.GeoDataFrame, PATH_TYPE]): geo data frame or path to data that can be loaded by geopandas\n        column_names (typing.Union[str, typing.List[str]]): Which columns to obtain data from\n\n    Returns:\n        np.ndarray: Array of values for each vertex if there is one column name or\n        dict[np.ndarray]: A dict mapping from column names to numpy arrays\n    \"\"\"\n    # Lead the vector data if not already provided in memory\n    if isinstance(vector_source, gpd.GeoDataFrame):\n        gdf = vector_source\n    else:\n        # This will error if not readable\n        gdf = gpd.read_file(vector_source)\n\n    # Infer or standardize the column names\n    if column_names is None:\n        # Check if there is only one real column\n        if len(gdf.columns) == 2:\n            column_names = list(filter(lambda x: x != \"geometry\", gdf.columns))\n        else:\n            # Log as well since this may be caught by an exception handler,\n            # and it's a user error that can be corrected\n            self.logger.error(\n                \"No column name provided and ambigious which column to use\"\n            )\n            raise ValueError(\n                \"No column name provided and ambigious which column to use\"\n            )\n    # If only one column is provided, make it a one-length list\n    elif isinstance(column_names, str):\n        column_names = [column_names]\n\n    # Get a dataframe of vertices\n    verts_df = self.get_verts_geodataframe(gdf.crs)\n\n    # See which vertices are in the geopolygons\n    points_in_polygons_gdf = gpd.tools.overlay(verts_df, gdf, how=\"intersection\")\n    # Get the index array\n    index_array = points_in_polygons_gdf[VERT_ID].to_numpy()\n\n    # This is one entry per vertex\n    labeled_verts_dict = {}\n    all_values_dict = {}\n    # Extract the data from each\n    for column_name in column_names:\n        # Create an array corresponding to all the points and initialize to NaN\n        column_values = points_in_polygons_gdf[column_name]\n        # TODO clean this up\n        if column_values.dtype == str or column_values.dtype == np.dtype(\"O\"):\n            # TODO be set to the default value for the type of the column\n            null_value = \"null\"\n        elif column_values.dtype == int:\n            null_value = 255\n        else:\n            null_value = np.nan\n        # Create an array, one per vertex, with the null value\n        values = np.full(\n            shape=verts_df.shape[0],\n            dtype=column_values.dtype,\n            fill_value=null_value,\n        )\n        # Assign the labeled values\n        values[index_array] = column_values\n\n        # Record the results\n        labeled_verts_dict[column_name] = values\n        all_values_dict[column_name] = gdf[column_name]\n\n    # If only one name was requested, just return that\n    if len(column_names) == 1:\n        labeled_verts = np.array(list(labeled_verts_dict.values())[0])\n        all_values = np.array(list(all_values_dict.values())[0])\n\n        return labeled_verts, all_values\n    # Else return a dict of all requested values\n    return labeled_verts_dict, all_values_dict\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_vert_values_from_raster_file","title":"<code>get_vert_values_from_raster_file(raster_file, return_verts_in_CRS=False, nodata_fill_value=np.nan)</code>","text":"<p>Compute the height above groun for each point on the mesh</p> <p>Parameters:</p> Name Type Description Default <code>raster_file</code> <code>PATH_TYPE</code> <p>The path to the geospatial raster file.</p> required <code>return_verts_in_CRS</code> <code>bool</code> <p>Return the vertices transformed into the raster CRS</p> <code>False</code> <code>nodata_fill_value</code> <code>float</code> <p>Set data defined by the opened file as NODATAVAL to this value</p> <code>nan</code> <p>Returns:</p> Type Description <p>np.ndarray: samples from raster. Either (n_verts,) or (n_verts, n_raster_channels)</p> <p>np.ndarray (optional): (n_verts, 3) the vertices in the raster CRS</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_vert_values_from_raster_file(\n    self,\n    raster_file: PATH_TYPE,\n    return_verts_in_CRS: bool = False,\n    nodata_fill_value: float = np.nan,\n):\n    \"\"\"Compute the height above groun for each point on the mesh\n\n    Args:\n        raster_file (PATH_TYPE, optional): The path to the geospatial raster file.\n        return_verts_in_CRS (bool, optional): Return the vertices transformed into the raster CRS\n        nodata_fill_value (float, optional): Set data defined by the opened file as NODATAVAL to this value\n\n    Returns:\n        np.ndarray: samples from raster. Either (n_verts,) or (n_verts, n_raster_channels)\n        np.ndarray (optional): (n_verts, 3) the vertices in the raster CRS\n    \"\"\"\n    # Open the DTM file\n    raster = rio.open(raster_file)\n    # Get the mesh points in the coordinate reference system of the DTM\n    verts_in_raster_CRS = self.get_vertices_in_CRS(\n        raster.crs, force_easting_northing=True\n    )\n\n    # Get the points as a list\n    easting_points = verts_in_raster_CRS[:, 0].tolist()\n    northing_points = verts_in_raster_CRS[:, 1].tolist()\n\n    # Zip them together\n    zipped_locations = zip(easting_points, northing_points)\n    sampling_iter = tqdm(\n        zipped_locations,\n        desc=f\"Sampling values from raster {raster_file}\",\n        total=verts_in_raster_CRS.shape[0],\n    )\n    # Sample the raster file and squeeze if single channel\n    sampled_raster_values = np.squeeze(np.array(list(raster.sample(sampling_iter))))\n\n    # Set nodata locations to nan\n    # TODO figure out if it will ever be a problem to take the first value\n    sampled_raster_values[sampled_raster_values == raster.nodatavals[0]] = (\n        nodata_fill_value\n    )\n\n    if return_verts_in_CRS:\n        return sampled_raster_values, verts_in_raster_CRS\n\n    return sampled_raster_values\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_vertices_in_CRS","title":"<code>get_vertices_in_CRS(output_CRS, force_easting_northing=True)</code>","text":"<p>Return the coordinates of the mesh vertices in a given CRS</p> <p>Parameters:</p> Name Type Description Default <code>output_CRS</code> <code>CRS</code> <p>The coordinate reference system to transform to</p> required <code>force_easting_northing</code> <code>bool</code> <p>Ensure that the returned points are east first, then north</p> <code>True</code> <p>Returns:</p> Type Description <p>np.ndarray: (n_points, 3)</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_vertices_in_CRS(\n    self, output_CRS: pyproj.CRS, force_easting_northing: bool = True\n):\n    \"\"\"Return the coordinates of the mesh vertices in a given CRS\n\n    Args:\n        output_CRS (pyproj.CRS): The coordinate reference system to transform to\n        force_easting_northing (bool, optional): Ensure that the returned points are east first, then north\n\n    Returns:\n        np.ndarray: (n_points, 3)\n    \"\"\"\n    # If no CRS is requested, just return the points\n    if output_CRS is None:\n        return self.pyvista_mesh.points\n\n    # The mesh points are defined in an arbitrary local coordinate system but we can transform them to EPGS:4978,\n    # the earth-centered, earth-fixed coordinate system, using an included transform\n    epgs4978_verts = self.transform_vertices(self.local_to_epgs_4978_transform)\n\n    # TODO figure out why this conversion was required. I think it was some typing issue\n    output_CRS = pyproj.CRS.from_epsg(output_CRS.to_epsg())\n    # Build a pyproj transfrormer from EPGS:4978 to the desired CRS\n    transformer = pyproj.Transformer.from_crs(\n        EARTH_CENTERED_EARTH_FIXED_EPSG_CODE, output_CRS\n    )\n\n    # Transform the coordinates\n    verts_in_output_CRS = transformer.transform(\n        xx=epgs4978_verts[:, 0],\n        yy=epgs4978_verts[:, 1],\n        zz=epgs4978_verts[:, 2],\n    )\n    # Stack and transpose\n    verts_in_output_CRS = np.vstack(verts_in_output_CRS).T\n\n    # Pyproj respects the CRS axis ordering, which is northing/easting for most projected coordinate systems\n    # This causes headaches because it's assumed by rasterio and geopandas to be easting/northing\n    # https://rasterio.readthedocs.io/en/stable/api/rasterio.crs.html#rasterio.crs.epsg_treats_as_latlong\n    if force_easting_northing and rio.crs.epsg_treats_as_latlong(output_CRS):\n        # Swap first two columns\n        verts_in_output_CRS = verts_in_output_CRS[:, [1, 0, 2]]\n\n    return verts_in_output_CRS\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.get_verts_geodataframe","title":"<code>get_verts_geodataframe(crs)</code>","text":"<p>Obtain the vertices as a dataframe</p> <p>Parameters:</p> Name Type Description Default <code>crs</code> <code>CRS</code> <p>The CRS to use</p> required <p>Returns:</p> Type Description <code>GeoDataFrame</code> <p>gpd.GeoDataFrame: A dataframe with all the vertices</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def get_verts_geodataframe(self, crs: pyproj.CRS) -&gt; gpd.GeoDataFrame:\n    \"\"\"Obtain the vertices as a dataframe\n\n    Args:\n        crs (pyproj.CRS): The CRS to use\n\n    Returns:\n        gpd.GeoDataFrame: A dataframe with all the vertices\n    \"\"\"\n    # Get the vertices in the same CRS as the geofile\n    verts_in_geopolygon_crs = self.get_vertices_in_CRS(crs)\n\n    df = pd.DataFrame(\n        {\n            \"east\": verts_in_geopolygon_crs[:, 0],\n            \"north\": verts_in_geopolygon_crs[:, 1],\n        }\n    )\n    # Create a column of Point objects to use as the geometry\n    df[\"geometry\"] = gpd.points_from_xy(df[\"east\"], df[\"north\"])\n    points = gpd.GeoDataFrame(df, crs=crs)\n\n    # Add an index column because the normal index will not be preserved in future operations\n    points[VERT_ID] = df.index\n\n    return points\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.label_ground_class","title":"<code>label_ground_class(DTM_file, height_above_ground_threshold, labels=None, only_label_existing_labels=True, ground_class_name='ground', ground_ID=None, set_mesh_texture=False)</code>","text":"<p>Set vertices to a potentially-new class with a thresholded height above the DTM. TODO, consider handling face textures as well</p> <p>Parameters:</p> Name Type Description Default <code>DTM_file</code> <code>PATH_TYPE</code> <p>Path to the DTM file</p> required <code>height_above_ground_threshold</code> <code>float</code> <p>Height (meters) above that DTM that points below are considered ground</p> required <code>labels</code> <code>Union[None, ndarray]</code> <p>Vertex texture, otherwise will be queried from mesh. Defaults to None.</p> <code>None</code> <code>only_label_existing_labels</code> <code>bool</code> <p>Only label points that already have non-null labels. Defaults to True.</p> <code>True</code> <code>ground_class_name</code> <code>str</code> <p>The potentially-new ground class name. Defaults to \"ground\".</p> <code>'ground'</code> <code>ground_ID</code> <code>Union[None, int]</code> <p>What value to use for the ground class. Will be set inteligently if not provided. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: The updated labels</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def label_ground_class(\n    self,\n    DTM_file: PATH_TYPE,\n    height_above_ground_threshold: float,\n    labels: typing.Union[None, np.ndarray] = None,\n    only_label_existing_labels: bool = True,\n    ground_class_name: str = \"ground\",\n    ground_ID: typing.Union[None, int] = None,\n    set_mesh_texture: bool = False,\n) -&gt; np.ndarray:\n    \"\"\"\n    Set vertices to a potentially-new class with a thresholded height above the DTM.\n    TODO, consider handling face textures as well\n\n    Args:\n        DTM_file (PATH_TYPE): Path to the DTM file\n        height_above_ground_threshold (float): Height (meters) above that DTM that points below are considered ground\n        labels (typing.Union[None, np.ndarray], optional): Vertex texture, otherwise will be queried from mesh. Defaults to None.\n        only_label_existing_labels (bool, optional): Only label points that already have non-null labels. Defaults to True.\n        ground_class_name (str, optional): The potentially-new ground class name. Defaults to \"ground\".\n        ground_ID (typing.Union[None, int], optional): What value to use for the ground class. Will be set inteligently if not provided. Defaults to None.\n\n    Returns:\n        np.ndarray: The updated labels\n    \"\"\"\n\n    if labels is None:\n        # Default to using vertex labels since it's the native way to check height above the DTM\n        use_vertex_labels = True\n    elif labels is not None:\n        # Check the size of the input labels and set what type they are. Note this could override existing value\n        if labels.shape[0] == self.pyvista_mesh.points.shape[0]:\n            use_vertex_labels = True\n        elif labels.shape[0] == self.faces.shape[0]:\n            use_vertex_labels = False\n        else:\n            raise ValueError(\n                \"Labels were provided but didn't match the shape of vertices or faces\"\n            )\n\n    # if a labels are not provided, get it from the mesh\n    if labels is None:\n        # Get the vertex textures from the mesh\n        labels = self.get_texture(\n            request_vertex_texture=use_vertex_labels,\n        )\n\n    # Compute which vertices are part of the ground by thresholding the height above the DTM\n    ground_mask = self.get_height_above_ground(\n        DTM_file=DTM_file, threshold=height_above_ground_threshold\n    )\n    # If we needed a mask for the faces, compute that instead\n    if not use_vertex_labels:\n        ground_mask = self.vert_to_face_texture(ground_mask.astype(int)).astype(\n            bool\n        )\n\n    # Replace only vertices that were previously labeled as something else, to avoid class imbalance\n    if only_label_existing_labels:\n        # Find which vertices are labeled\n        is_labeled = np.isfinite(labels[:, 0])\n        # Find which points are ground that were previously labeled as something else\n        ground_mask = np.logical_and(is_labeled, ground_mask)\n\n    # Get the existing label names\n    IDs_to_labels = self.get_IDs_to_labels()\n\n    if IDs_to_labels is None and ground_ID is None:\n        # This means that the label is continous, so the concept of ID is meaningless\n        ground_ID = np.nan\n    elif IDs_to_labels is not None and ground_class_name in IDs_to_labels:\n        # If the ground class name is already in the list, set newly-predicted vertices to that class\n        ground_ID = IDs_to_labels.find(ground_class_name)\n    elif IDs_to_labels is not None:\n        # If the label names are present, and the class is not already included, add it as the last element\n        if ground_ID is None:\n            # Set it to the first unused ID\n            # TODO improve this since it should be the max plus one\n            ground_ID = len(IDs_to_labels)\n\n    self.add_label(label_name=ground_class_name, label_ID=ground_ID)\n\n    # Replace mask for ground_vertices\n    labels[ground_mask, 0] = ground_ID\n\n    # Optionally apply the texture to the mesh\n    if set_mesh_texture:\n        self.set_texture(labels, use_derived_IDs_to_labels=False)\n\n    return labels\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.label_polygons","title":"<code>label_polygons(face_labels, polygons, face_weighting=None, sjoin_overlay=True, return_class_labels=True, unknown_class_label='unknown', buffer_dist_meters=2.0)</code>","text":"<p>Assign a class label to polygons using labels per face</p> <p>Parameters:</p> Name Type Description Default <code>face_labels</code> <code>ndarray</code> <p>(n_faces,) array of integer labels</p> required <code>polygons</code> <code>Union[PATH_TYPE, GeoDataFrame]</code> <p>Geospatial polygons to be labeled</p> required <code>face_weighting</code> <code>Union[None, ndarray]</code> <p>(n_faces,) array of scalar weights for each face, to be multiplied with the contribution of this face. Defaults to None.</p> <code>None</code> <code>sjoin_overlay</code> <code>bool</code> <p>Whether to use <code>gpd.sjoin</code> or <code>gpd.overlay</code> to compute the overlay. Sjoin is substaintially faster, but only uses mesh faces that are entirely within the bounds of the polygon, rather than computing the intersecting region for partially-overlapping faces. Defaults to True.</p> <code>True</code> <code>return_class_labels</code> <code>bool</code> <p>(bool, optional): Return string representation of class labels rather than float. Defaults to True.</p> <code>True</code> <code>unknown_class_label</code> <code>str</code> <p>Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".</p> <code>'unknown'</code> <code>buffer_dist_meters</code> <code>float</code> <p>(Union[float, None], optional) Only applicable if sjoin_overlay=False. In that case, include faces entirely within the region that is this distance in meters from the polygons. Defaults to 2.0.</p> <code>2.0</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>if faces_labels or face_weighting is not 1D</p> <p>Returns:</p> Name Type Description <code>list</code> <code>Union[str, int]</code> <p>(n_polygons,) list of labels. Either float values, represnting integer IDs or nan, or string values representing the class label</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def label_polygons(\n    self,\n    face_labels: np.ndarray,\n    polygons: typing.Union[PATH_TYPE, gpd.GeoDataFrame],\n    face_weighting: typing.Union[None, np.ndarray] = None,\n    sjoin_overlay: bool = True,\n    return_class_labels: bool = True,\n    unknown_class_label: str = \"unknown\",\n    buffer_dist_meters: float = 2.0,\n):\n    \"\"\"Assign a class label to polygons using labels per face\n\n    Args:\n        face_labels (np.ndarray): (n_faces,) array of integer labels\n        polygons (typing.Union[PATH_TYPE, gpd.GeoDataFrame]): Geospatial polygons to be labeled\n        face_weighting (typing.Union[None, np.ndarray], optional):\n            (n_faces,) array of scalar weights for each face, to be multiplied with the\n            contribution of this face. Defaults to None.\n        sjoin_overlay (bool, optional):\n            Whether to use `gpd.sjoin` or `gpd.overlay` to compute the overlay. Sjoin is\n            substaintially faster, but only uses mesh faces that are entirely within the bounds\n            of the polygon, rather than computing the intersecting region for\n            partially-overlapping faces. Defaults to True.\n        return_class_labels: (bool, optional):\n            Return string representation of class labels rather than float. Defaults to True.\n        unknown_class_label (str, optional):\n            Label for predicted class for polygons with no overlapping faces. Defaults to \"unknown\".\n        buffer_dist_meters: (Union[float, None], optional)\n            Only applicable if sjoin_overlay=False. In that case, include faces entirely within\n            the region that is this distance in meters from the polygons. Defaults to 2.0.\n\n    Raises:\n        ValueError: if faces_labels or face_weighting is not 1D\n\n    Returns:\n        list(typing.Union[str, int]):\n            (n_polygons,) list of labels. Either float values, represnting integer IDs or nan,\n            or string values representing the class label\n    \"\"\"\n    # Premptive error checking before expensive operations\n    face_labels = np.squeeze(face_labels)\n    if face_labels.ndim != 1:\n        raise ValueError(\n            f\"Faces labels must be one-dimensional, but is {face_labels.ndim}\"\n        )\n    if face_weighting is not None:\n        face_weighting = np.squeeze(face_weighting)\n        if face_weighting.ndim != 1:\n            raise ValueError(\n                f\"Faces labels must be one-dimensional, but is {face_weighting.ndim}\"\n            )\n\n    # Ensure that the input is a geopandas dataframe\n    polygons_gdf = ensure_projected_CRS(coerce_to_geoframe(polygons))\n    # Extract just the geometry\n    polygons_gdf = polygons_gdf[[\"geometry\"]]\n\n    # Only get faces for which there is a non-nan label. Otherwise it is just additional compute\n    faces_mask = np.isfinite(face_labels)\n\n    # Get the faces of the mesh as a geopandas dataframe\n    # Include the predicted face labels as a column in the dataframe\n    faces_2d_gdf = self.get_faces_2d_gdf(\n        polygons_gdf.crs,\n        include_3d_2d_ratio=True,\n        data_dict={CLASS_ID_KEY: face_labels},\n        faces_mask=faces_mask,\n        cache_data=True,\n    )\n\n    # If a per-face weighting is provided, multiply that with the 3d to 2d ratio\n    if face_weighting is not None:\n        face_weighting = face_weighting[faces_mask]\n        faces_2d_gdf[\"face_weighting\"] = (\n            faces_2d_gdf[RATIO_3D_2D_KEY] * face_weighting\n        )\n    # If not, just use the ratio\n    else:\n        faces_2d_gdf[\"face_weighting\"] = faces_2d_gdf[RATIO_3D_2D_KEY]\n\n    # Set the precision to avoid approximate coliniearity errors\n    faces_2d_gdf.geometry = shapely.set_precision(\n        faces_2d_gdf.geometry.values, 1e-6\n    )\n    polygons_gdf.geometry = shapely.set_precision(\n        polygons_gdf.geometry.values, 1e-6\n    )\n\n    # Set the ID field so it's available after the overlay operation\n    # Note that polygons_gdf.index is a bad choice, because this df could be a subset of another\n    # one and the index would not start from 0\n    polygons_gdf[\"polygon_ID\"] = np.arange(len(polygons_gdf))\n\n    # Since overlay is expensive, we first discard faces that are not near the polygons\n\n    # Dissolve the polygons to form one ROI\n    merged_polygons = polygons_gdf.dissolve()\n    # Try to decrease the number of elements in the polygon by expanding\n    # and then simplifying the number of elements in the polygon\n    merged_polygons.geometry = merged_polygons.buffer(buffer_dist_meters)\n    merged_polygons.geometry = merged_polygons.simplify(buffer_dist_meters)\n\n    # Determine which face IDs intersect the ROI. This is slow\n    start = time()\n    self.logger.info(\"Starting to subset to ROI\")\n\n    # Check which faces are fully within the buffered regions around the query polygons\n    # Note that using sjoin has been faster than any other approach I've tried, despite seeming\n    # to compute more information than something like gpd.within\n    contained_faces = gpd.sjoin(\n        faces_2d_gdf, merged_polygons, how=\"left\", predicate=\"within\"\n    )[\"index_right\"].notna()\n    faces_2d_gdf = faces_2d_gdf.loc[contained_faces]\n    self.logger.info(f\"Subset to ROI in {time() - start} seconds\")\n\n    start = time()\n    self.logger.info(\"Starting `overlay`\")\n    if sjoin_overlay:\n        overlay = gpd.sjoin(\n            faces_2d_gdf, polygons_gdf, how=\"left\", predicate=\"within\"\n        )\n        self.logger.info(f\"Overlay time with gpd.sjoin: {time() - start}\")\n    else:\n        # Drop faces not included\n        overlay = polygons_gdf.overlay(\n            faces_2d_gdf, how=\"identity\", keep_geom_type=False\n        )\n        self.logger.info(f\"Overlay time with gpd.overlay: {time() - start}\")\n\n    # Drop nan, for geometries that don't intersect the polygons\n    overlay.dropna(inplace=True)\n    # Compute the weighted area for each face, which may have been broken up by the overlay\n    overlay[\"weighted_area\"] = overlay.area * overlay[\"face_weighting\"]\n\n    # Extract only the neccessary columns\n    overlay = overlay.loc[:, [\"polygon_ID\", CLASS_ID_KEY, \"weighted_area\"]]\n    aggregated_data = overlay.groupby([\"polygon_ID\", CLASS_ID_KEY]).agg(np.sum)\n    # Compute the highest weighted class prediction\n    # Modified from https://stackoverflow.com/questions/27914360/python-pandas-idxmax-for-multiple-indexes-in-a-dataframe\n    max_rows = aggregated_data.loc[\n        aggregated_data.groupby([\"polygon_ID\"], sort=False)[\n            \"weighted_area\"\n        ].idxmax()\n    ].reset_index()\n\n    # Make the class predictions a list of IDs with nans where no information is available\n    pred_subset_IDs = max_rows[CLASS_ID_KEY].to_numpy(dtype=float)\n    pred_subset_IDs[max_rows[\"weighted_area\"].to_numpy() == 0] = np.nan\n\n    predicted_class_IDs = np.full(len(polygons_gdf), np.nan)\n    predicted_class_IDs[max_rows[\"polygon_ID\"].to_numpy(dtype=int)] = (\n        pred_subset_IDs\n    )\n    predicted_class_IDs = predicted_class_IDs.tolist()\n\n    # Post-process to string label names if requested and IDs_to_labels exists\n    if return_class_labels and (\n        (IDs_to_labels := self.get_IDs_to_labels()) is not None\n    ):\n        # convert the IDs into labels\n        # Any label marked as nan is set to the unknown class label, since we had no predictions for it\n        predicted_class_IDs = [\n            (IDs_to_labels[int(pi)] if np.isfinite(pi) else unknown_class_label)\n            for pi in predicted_class_IDs\n        ]\n    return predicted_class_IDs\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.load_mesh","title":"<code>load_mesh(mesh, downsample_target=1.0, ROI=None, ROI_buffer_meters=0, ROI_simplify_tol_meters=2)</code>","text":"<p>Load the pyvista mesh and create the texture</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Union[PATH_TYPE, PolyData]</code> <p>Path to the mesh or actual mesh</p> required <code>downsample_target</code> <code>float</code> <p>What fraction of mesh vertices to downsample to. Defaults to 1.0, (does nothing).</p> <code>1.0</code> <code>ROI</code> <p>See select_mesh_ROI. Defaults to None</p> <code>None</code> <code>ROI_buffer_meters</code> <p>See select_mesh_ROI. Defaults to 0.</p> <code>0</code> <code>ROI_simplify_tol_meters</code> <p>See select_mesh_ROI. Defaults to 2.</p> <code>2</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def load_mesh(\n    self,\n    mesh: typing.Union[PATH_TYPE, pv.PolyData],\n    downsample_target: float = 1.0,\n    ROI=None,\n    ROI_buffer_meters=0,\n    ROI_simplify_tol_meters=2,\n):\n    \"\"\"Load the pyvista mesh and create the texture\n\n    Args:\n        mesh (typing.Union[PATH_TYPE, pv.PolyData]):\n            Path to the mesh or actual mesh\n        downsample_target (float, optional):\n            What fraction of mesh vertices to downsample to. Defaults to 1.0, (does nothing).\n        ROI:\n            See select_mesh_ROI. Defaults to None\n        ROI_buffer_meters:\n            See select_mesh_ROI. Defaults to 0.\n        ROI_simplify_tol_meters:\n            See select_mesh_ROI. Defaults to 2.\n    \"\"\"\n    if isinstance(mesh, pv.PolyData):\n        self.pyvista_mesh = mesh\n    else:\n        # Load the mesh using pyvista\n        # TODO see if pytorch3d has faster/more flexible readers. I'd assume no, but it's good to check\n        self.logger.info(\"Reading the mesh\")\n        self.pyvista_mesh = pv.read(mesh)\n\n    self.logger.info(\"Selecting an ROI from mesh\")\n    # Select a region of interest if needed\n    self.pyvista_mesh = self.select_mesh_ROI(\n        region_of_interest=ROI,\n        buffer_meters=ROI_buffer_meters,\n        simplify_tol_meters=ROI_simplify_tol_meters,\n    )\n\n    # Downsample mesh and transfer active scalars from original mesh to downsampled mesh\n    if downsample_target != 1.0:\n        # TODO try decimate_pro and compare quality and runtime\n        # TODO see if there's a way to preserve the mesh colors\n        # TODO also see this decimation algorithm: https://pyvista.github.io/fast-simplification/\n        self.logger.info(\"Downsampling the mesh\")\n        # Have a temporary mesh so we can use the original mesh to transfer the active scalars to the downsampled one\n        downsampled_mesh_without_textures = self.pyvista_mesh.decimate(\n            target_reduction=(1 - downsample_target)\n        )\n        self.pyvista_mesh = self.transfer_texture(downsampled_mesh_without_textures)\n    self.logger.info(\"Extracting faces from mesh\")\n    # See here for format: https://github.com/pyvista/pyvista-support/issues/96\n    self.faces = self.pyvista_mesh.faces.reshape((-1, 4))[:, 1:4].copy()\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.load_texture","title":"<code>load_texture(texture, texture_column_name=None, IDs_to_labels=None)</code>","text":"<p>Sets either self.face_texture or self.vertex_texture to an (n_{faces, verts}, m channels) array. Note that the other    one will be left as None</p> <p>Parameters:</p> Name Type Description Default <code>texture</code> <code>Union[PATH_TYPE, ndarray, None]</code> <p>This is either a numpy array or a file to one of the following * A numpy array file in \".npy\" format * A vector file readable by geopandas and a label(s) specifying which column to use.   This should be dataset of polygons/multipolygons. Ideally, there should be no overlap between   regions with different labels. These regions may be assigned based on the order of the rows. * A raster file readable by rasterio. We may want to support using a subset of bands</p> required <code>texture_column_name</code> <code>Union[None, PATH_TYPE]</code> <p>The column to use as the label for a vector data input</p> <code>None</code> <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Dictionary mapping from integer IDs to string class names</p> <code>None</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def load_texture(\n    self,\n    texture: typing.Union[str, PATH_TYPE, np.ndarray, None],\n    texture_column_name: typing.Union[None, PATH_TYPE] = None,\n    IDs_to_labels: typing.Union[PATH_TYPE, dict, None] = None,\n):\n    \"\"\"Sets either self.face_texture or self.vertex_texture to an (n_{faces, verts}, m channels) array. Note that the other\n       one will be left as None\n\n    Args:\n        texture (typing.Union[PATH_TYPE, np.ndarray, None]): This is either a numpy array or a file to one of the following\n            * A numpy array file in \".npy\" format\n            * A vector file readable by geopandas and a label(s) specifying which column to use.\n              This should be dataset of polygons/multipolygons. Ideally, there should be no overlap between\n              regions with different labels. These regions may be assigned based on the order of the rows.\n            * A raster file readable by rasterio. We may want to support using a subset of bands\n        texture_column_name: The column to use as the label for a vector data input\n        IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n    \"\"\"\n    # The easy case, a texture is passed in directly\n    if isinstance(texture, np.ndarray):\n        self.set_texture(\n            texture_array=texture,\n            IDs_to_labels=IDs_to_labels,\n            use_derived_IDs_to_labels=True,\n        )\n    # If the texture is None, try to load it from the mesh\n    # Note that this requires us to have not decimated yet\n    elif texture is None:\n        # See if the mesh has a texture, else this will be None\n        texture_array = self.pyvista_mesh.active_scalars\n\n        if texture_array is not None:\n            # Check if this was a really one channel that had to be tiled to\n            # three for saving\n            if len(texture_array.shape) == 2:\n                min_val_per_row = np.min(texture_array, axis=1)\n                max_val_per_row = np.max(texture_array, axis=1)\n                if np.array_equal(min_val_per_row, max_val_per_row):\n                    # This is supposted to be one channel\n                    texture_array = texture_array[:, 0].astype(float)\n                    # Set any values that are the ignore int value to nan\n            texture_array = texture_array.astype(float)\n            texture_array[texture_array == NULL_TEXTURE_INT_VALUE] = np.nan\n\n            self.set_texture(\n                texture_array,\n                IDs_to_labels=IDs_to_labels,\n                use_derived_IDs_to_labels=True,\n            )\n        else:\n            if IDs_to_labels is not None:\n                self.IDs_to_labels = IDs_to_labels\n            # Assume that no texture will be needed, consider printing a warning\n            self.logger.warn(\"No texture provided\")\n    else:\n        # Try handling all the other supported filetypes\n        texture_array = None\n        all_values = None\n\n        # Name of scalar in the mesh\n        try:\n            self.logger.warn(\n                \"Trying to read texture as a scalar from the pyvista mesh:\"\n            )\n            texture_array = self.pyvista_mesh[texture]\n            self.logger.warn(\"- success\")\n        except (KeyError, ValueError):\n            self.logger.warn(\"- failed\")\n\n        # Numpy file\n        if texture_array is None:\n            try:\n                self.logger.warn(\"Trying to read texture as a numpy file:\")\n                texture_array = np.load(texture, allow_pickle=True)\n                self.logger.warn(\"- success\")\n            except:\n                self.logger.warn(\"- failed\")\n\n        # Vector file\n        if texture_array is None:\n            try:\n                self.logger.warn(\"Trying to read texture as vector file:\")\n                # TODO IDs to labels should be used here if set so the computed IDs are aligned with that mapping\n                texture_array, all_values = self.get_values_for_verts_from_vector(\n                    column_names=texture_column_name,\n                    vector_source=texture,\n                )\n                self.logger.warn(\"- success\")\n            except IndexError:\n                self.logger.warn(\"- failed\")\n\n        # Raster file\n        if texture_array is None:\n            try:\n                # TODO\n                self.logger.warn(\"Trying to read as texture as raster file: \")\n                texture_array = self.get_vert_values_from_raster_file(texture)\n                self.logger.warn(\"- success\")\n            except:\n                self.logger.warn(\"- failed\")\n\n        # Error out if not set, since we assume the intent was to have a texture at this point\n        if texture_array is None:\n            raise ValueError(f\"Could not load texture for {texture}\")\n\n        # This will error if something is wrong with the texture that was loaded\n        self.set_texture(\n            texture_array,\n            all_discrete_texture_values=all_values,\n            use_derived_IDs_to_labels=True,\n            IDs_to_labels=IDs_to_labels,\n        )\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.load_transform_to_epsg_4326","title":"<code>load_transform_to_epsg_4326(transform_filename, require_transform=False)</code>","text":"<p>Load the 4x4 transform projects points from their local coordnate system into EPSG:4326, the earth-centered, earth-fixed coordinate frame. This can either be from a CSV file specifying it directly or extracted from a Metashape camera output</p> <p>Args     transform_filename (PATH_TYPE):     require_transform (bool): Does a local-to-global transform file need to be available\" Raises:     FileNotFoundError: Cannot find texture file     ValueError: Transform file doesn't have 4x4 matrix</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def load_transform_to_epsg_4326(\n    self, transform_filename: PATH_TYPE, require_transform: bool = False\n):\n    \"\"\"\n    Load the 4x4 transform projects points from their local coordnate system into EPSG:4326,\n    the earth-centered, earth-fixed coordinate frame. This can either be from a CSV file specifying\n    it directly or extracted from a Metashape camera output\n\n    Args\n        transform_filename (PATH_TYPE):\n        require_transform (bool): Does a local-to-global transform file need to be available\"\n    Raises:\n        FileNotFoundError: Cannot find texture file\n        ValueError: Transform file doesn't have 4x4 matrix\n    \"\"\"\n    if transform_filename is None:\n        if require_transform:\n            raise ValueError(\"Transform is required but not provided\")\n        # If not required, do nothing. TODO consider adding a warning\n        return\n\n    elif Path(transform_filename).suffix == \".xml\":\n        self.local_to_epgs_4978_transform = parse_transform_metashape(\n            transform_filename\n        )\n    elif Path(transform_filename).suffix == \".csv\":\n        self.local_to_epgs_4978_transform = np.loadtxt(\n            transform_filename, delimiter=\",\"\n        )\n        if self.local_to_epgs_4978_transform.shape != (4, 4):\n            raise ValueError(\n                f\"Transform should be (4,4) but is {self.local_to_epgs_4978_transform.shape}\"\n            )\n    else:\n        if require_transform:\n            raise ValueError(\n                f\"Transform could not be loaded from {transform_filename}\"\n            )\n        # Not set\n        return\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.pix2face","title":"<code>pix2face(cameras, render_img_scale=1, save_to_cache=False, cache_folder=CACHE_FOLDER)</code>","text":"<p>Compute the face that a ray from each pixel would intersect for each camera</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>A single camera or set of cameras. For each camera, the correspondences between pixels and the face IDs of the mesh will be computed. The images of all cameras are assumed to be the same size.</p> required <code>render_img_scale</code> <code>float</code> <p>Create a pix2face map that is this fraction of the original image scale. Defaults to 1.</p> <code>1</code> <code>save_to_cache</code> <code>bool</code> <p>Should newly-computed values be saved to the cache. This may speed up future operations but can take up 100s of GBs of space. Defaults to False.</p> <code>False</code> <code>cache_folder</code> <code>PATH_TYPE, None)</code> <p>Where to check for and save to cached data. Only applicable if use_cache=True. Defaults to CACHE_FOLDER</p> <code>CACHE_FOLDER</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: For each camera, there is an array that is the shape of an image and</p> <code>ndarray</code> <p>contains the integer face index for the ray originating at that pixel. Any pixel for</p> <code>ndarray</code> <p>which the given ray does not intersect a face is given a value of -1. If the input is</p> <code>ndarray</code> <p>a single PhotogrammetryCamera, the shape is (h, w). If it's a camera set, then it is</p> <code>ndarray</code> <p>(n_cameras, h, w). Note that a one-length camera set will have a leading singleton dim.</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def pix2face(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    render_img_scale: float = 1,\n    save_to_cache: bool = False,\n    cache_folder: typing.Union[None, PATH_TYPE] = CACHE_FOLDER,\n) -&gt; np.ndarray:\n    \"\"\"Compute the face that a ray from each pixel would intersect for each camera\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            A single camera or set of cameras. For each camera, the correspondences between\n            pixels and the face IDs of the mesh will be computed. The images of all cameras\n            are assumed to be the same size.\n        render_img_scale (float, optional):\n            Create a pix2face map that is this fraction of the original image scale. Defaults\n            to 1.\n        save_to_cache (bool, optional):\n            Should newly-computed values be saved to the cache. This may speed up future operations\n            but can take up 100s of GBs of space. Defaults to False.\n        cache_folder ((PATH_TYPE, None), optional):\n            Where to check for and save to cached data. Only applicable if use_cache=True.\n            Defaults to CACHE_FOLDER\n\n    Returns:\n        np.ndarray: For each camera, there is an array that is the shape of an image and\n        contains the integer face index for the ray originating at that pixel. Any pixel for\n        which the given ray does not intersect a face is given a value of -1. If the input is\n        a single PhotogrammetryCamera, the shape is (h, w). If it's a camera set, then it is\n        (n_cameras, h, w). Note that a one-length camera set will have a leading singleton dim.\n    \"\"\"\n    # If a set of cameras is passed in, call this method on each camera and concatenate\n    # Other derived methods might be able to compute a batch of renders and once, but pyvista\n    # cannot as far as I can tell\n    if isinstance(cameras, PhotogrammetryCameraSet):\n        pix2face_list = [\n            self.pix2face(camera, render_img_scale=render_img_scale)\n            for camera in cameras\n        ]\n        pix2face = np.stack(pix2face_list, axis=0)\n        return pix2face\n\n    ## Single camera case\n\n    # Check if the cache contains a valid pix2face for the camera based on the dependencies\n    # Compute hashes for the mesh and camera to unique identify mesh+camera pair\n    # The cache will generate a unique key for each combination of the dependencies\n    # If the cache generated key matches a cache file on disk, pix2face will be filled with the correct correspondance\n    # If no match is found, recompute pix2face\n    # If there\u2019s an error loading the cached data, then clear the cache's contents, signified by on_error='clear'\n    mesh_hash = self.get_mesh_hash()\n    camera_hash = cameras.get_camera_hash()\n    cacher = ub.Cacher(\n        \"pix2face\",\n        depends=[mesh_hash, camera_hash, render_img_scale],\n        dpath=cache_folder,\n        verbose=0,\n    )\n    pix2face = cacher.tryload(on_error=\"clear\")\n    ## Cache is valid\n    if pix2face is not None:\n        return pix2face\n\n    # This needs to be an attribute of the class because creating a large number of plotters\n    # results in an un-fixable memory leak.\n    # See https://github.com/pyvista/pyvista/issues/2252\n    # The first step is to clear it\n    self.pix2face_plotter.clear()\n    # This is important so there aren't intermediate values\n    self.pix2face_plotter.disable_anti_aliasing()\n    # Set the camera to the corresponding viewpoint\n    self.pix2face_plotter.camera = cameras.get_pyvista_camera()\n\n    ## Compute the base 256 encoding of the face ID\n    n_faces = self.faces.shape[0]\n    ID_values = np.arange(n_faces)\n\n    # determine how many channels will be required to represent the number of faces\n    n_channels = int(np.ceil(np.emath.logn(256, n_faces))) if n_faces != 0 else 0\n    channel_multipliers = [256**i for i in range(n_channels)]\n\n    # Compute the encoding of each value, least significant value first\n    base_256_encoding = [\n        np.mod(np.floor(ID_values / m).astype(int), 256)\n        for m in channel_multipliers\n    ]\n\n    # ensure that there's a multiple of three channels\n    n_padding = n_channels % 3\n    base_256_encoding.extend([np.zeros(n_faces)] * n_padding)\n\n    # Assume that all images are the same size\n    image_size = cameras.get_image_size(image_scale=render_img_scale)\n\n    # Initialize pix2face\n    pix2face = np.zeros(image_size, dtype=int)\n    # Iterate over three-channel chunks. Each will be encoded as RGB and rendered\n    for chunk_ind in range(int(len(base_256_encoding) / 3)):\n        chunk_scalars = np.stack(\n            base_256_encoding[3 * chunk_ind : 3 * (chunk_ind + 1)], axis=1\n        ).astype(np.uint8)\n        # Add the mesh with the associated scalars\n        self.pix2face_plotter.add_mesh(\n            self.pyvista_mesh,\n            scalars=chunk_scalars.copy(),\n            rgb=True,\n            diffuse=0.0,\n            ambient=1.0,\n        )\n\n        # Perform rendering, this is the slow step\n        rendered_img = self.pix2face_plotter.screenshot(\n            window_size=(image_size[1], image_size[0]),\n        )\n        # Take the rendered values and interpret them as the encoded value\n        # Make sure to not try to interpret channels that are not used in the encoding\n        channels_to_decode = min(3, len(channel_multipliers) - 3 * chunk_ind)\n        for i in range(channels_to_decode):\n            channel_multiplier = channel_multipliers[chunk_ind * 3 + i]\n            channel_value = (rendered_img[..., i] * channel_multiplier).astype(int)\n            pix2face += channel_value\n\n    # Mask out pixels for which the mesh was not visible\n    # This is because the background will render as white\n    # If there happen to be an exact power of (256^3) number of faces, the last one may get\n    # erronously masked. This seems like a minimal concern but it could be addressed by adding\n    # another channel or something like that\n    pix2face[pix2face &gt; n_faces] = -1\n\n    if save_to_cache:\n        # Save the most recently computed pix2face correspondance in the cache\n        cacher.save(pix2face)\n\n    return pix2face\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.project_images","title":"<code>project_images(cameras, batch_size=1, aggregate_img_scale=1, check_null_image=False, **pix2face_kwargs)</code>","text":"<p>Find the per-face projection for each of a set of images and associated camera</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>The cameras to project images from. cam.get_image() will be called on each one</p> required <code>batch_size</code> <code>int</code> <p>The number of cameras to compute correspondences for at once. Defaults to 1.</p> <code>1</code> <code>aggregate_img_scale</code> <code>float</code> <p>The scale of pixel-to-face correspondences image, as a fraction of the original image. Lower values lead to better runtimes but decreased precision at content boundaries in the images. Defaults to 1.</p> <code>1</code> <code>check_null_image</code> <code>bool</code> <p>Only do indexing if there are non-null image values. This adds additional overhead, but can save the expensive operation of indexing in cases where it would be a no-op.</p> <code>False</code> <p>Yields:</p> Type Description <p>np.ndarray: The per-face projection of an image in the camera set</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def project_images(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    aggregate_img_scale: float = 1,\n    check_null_image: bool = False,\n    **pix2face_kwargs,\n):\n    \"\"\"Find the per-face projection for each of a set of images and associated camera\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            The cameras to project images from. cam.get_image() will be called on each one\n        batch_size (int, optional):\n            The number of cameras to compute correspondences for at once. Defaults to 1.\n        aggregate_img_scale (float, optional):\n            The scale of pixel-to-face correspondences image, as a fraction of the original\n            image. Lower values lead to better runtimes but decreased precision at content\n            boundaries in the images. Defaults to 1.\n        check_null_image (bool, optional):\n            Only do indexing if there are non-null image values. This adds additional overhead,\n            but can save the expensive operation of indexing in cases where it would be a no-op.\n\n    Yields:\n        np.ndarray: The per-face projection of an image in the camera set\n    \"\"\"\n    n_faces = self.faces.shape[0]\n\n    # Iterate over batch of the cameras\n    batch_stop = max(len(cameras) - batch_size + 1, 1)\n    for batch_start in range(0, batch_stop, batch_size):\n        batch_inds = list(range(batch_start, batch_start + batch_size))\n        batch_cameras = cameras.get_subset_cameras(batch_inds)\n        # Compute a batch of pix2face correspondences. This is likely the slowest step\n        batch_pix2face = self.pix2face(\n            cameras=batch_cameras,\n            render_img_scale=aggregate_img_scale,\n            **pix2face_kwargs,\n        )\n        for i, pix2face in enumerate(batch_pix2face):\n            img = cameras.get_image_by_index(batch_start + i, aggregate_img_scale)\n\n            n_channels = 1 if img.ndim == 2 else img.shape[-1]\n            textured_faces = np.full((n_faces, n_channels), fill_value=np.nan)\n\n            # Only do the expensive indexing step if there are finite values in the image. This is most\n            # significant for sparse detection tasks where some images may have no real data\n            if not check_null_image or np.any(np.isfinite(img)):\n                flat_img = np.reshape(img, (img.shape[0] * img.shape[1], -1))\n                flat_pix2face = pix2face.flatten()\n                # TODO this creates ill-defined behavior if multiple pixels map to the same face\n                # my guess is the later pixel in the flattened array will override the former\n                # TODO make sure that null pix2face values are handled properly\n                textured_faces[flat_pix2face] = flat_img\n            yield textured_faces\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.render_flat","title":"<code>render_flat(cameras, batch_size=1, render_img_scale=1, **pix2face_kwargs)</code>","text":"<p>Render the texture from the viewpoint of each camera in cameras. Note that this is a generator so if you want to actually execute the computation, call list(*) on the output</p> <p>Parameters:</p> Name Type Description Default <code>cameras</code> <code>Union[PhotogrammetryCamera, PhotogrammetryCameraSet]</code> <p>Either a single camera or a camera set. The texture will be rendered from the perspective of each one</p> required <code>batch_size</code> <code>int</code> <p>The batch size for pix2face. Defaults to 1.</p> <code>1</code> <code>render_img_scale</code> <code>float</code> <p>The rendered image will be this fraction of the original image corresponding to the virtual camera. Defaults to 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If cameras is not the correct type</p> <p>Yields:</p> Type Description <p>np.ndarray: The pix2face array for the next camera. The shape is (int(img_hrender_img_scale), int(img_wrender_img_scale)).</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def render_flat(\n    self,\n    cameras: typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet],\n    batch_size: int = 1,\n    render_img_scale: float = 1,\n    **pix2face_kwargs,\n):\n    \"\"\"\n    Render the texture from the viewpoint of each camera in cameras. Note that this is a\n    generator so if you want to actually execute the computation, call list(*) on the output\n\n    Args:\n        cameras (typing.Union[PhotogrammetryCamera, PhotogrammetryCameraSet]):\n            Either a single camera or a camera set. The texture will be rendered from the\n            perspective of each one\n        batch_size (int, optional):\n            The batch size for pix2face. Defaults to 1.\n        render_img_scale (float, optional):\n            The rendered image will be this fraction of the original image corresponding to the\n            virtual camera. Defaults to 1.\n\n    Raises:\n        TypeError: If cameras is not the correct type\n\n    Yields:\n        np.ndarray:\n           The pix2face array for the next camera. The shape is\n           (int(img_h*render_img_scale), int(img_w*render_img_scale)).\n    \"\"\"\n    if isinstance(cameras, PhotogrammetryCamera):\n        # Construct a camera set of length one\n        cameras = PhotogrammetryCameraSet([cameras])\n    elif not isinstance(cameras, PhotogrammetryCameraSet):\n        raise TypeError()\n\n    # Get the face texture from the mesh\n    # TODO consider whether the user should be able to pass a texture to this method. It could\n    # make the user's life easier but makes this method more complex\n    face_texture = self.get_texture(\n        request_vertex_texture=False, try_verts_faces_conversion=True\n    )\n    texture_dim = face_texture.shape[1]\n\n    # Iterate over batch of the cameras\n    batch_stop = max(len(cameras) - batch_size + 1, 1)\n    for batch_start in range(0, batch_stop, batch_size):\n        batch_end = batch_start + batch_size\n        batch_cameras = cameras[batch_start:batch_end]\n        # Compute a batch of pix2face correspondences. This is likely the slowest step\n        batch_pix2face = self.pix2face(\n            cameras=batch_cameras,\n            render_img_scale=render_img_scale,\n            **pix2face_kwargs,\n        )\n\n        # Iterate over the batch dimension\n        for pix2face in batch_pix2face:\n            # Record the original shape of the image\n            img_shape = pix2face.shape[:2]\n            # Flatten for indexing\n            pix2face = pix2face.flatten()\n            # Compute which pixels intersected the mesh\n            mesh_pixel_inds = np.where(pix2face != -1)[0]\n            # Initialize and all-nan array\n            rendered_flattened = np.full(\n                (pix2face.shape[0], texture_dim), fill_value=np.nan\n            )\n            # Fill the values for which correspondences exist\n            rendered_flattened[mesh_pixel_inds] = face_texture[\n                pix2face[mesh_pixel_inds]\n            ]\n            # reshape to an image, where the last dimension is the texture dimension\n            rendered_img = rendered_flattened.reshape(img_shape + (texture_dim,))\n            yield rendered_img\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.save_IDs_to_labels","title":"<code>save_IDs_to_labels(savepath)</code>","text":"<p>saves the contents of the IDs_to_labels to the file savepath provided</p> <p>Parameters:</p> Name Type Description Default <code>savepath</code> <code>PATH_TYPE</code> <p>path to the file where the data must be saved</p> required Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def save_IDs_to_labels(self, savepath: PATH_TYPE):\n    \"\"\"saves the contents of the IDs_to_labels to the file savepath provided\n\n    Args:\n        savepath (PATH_TYPE): path to the file where the data must be saved\n    \"\"\"\n\n    # Save the classes filename\n    ensure_containing_folder(savepath)\n    if self.is_discrete_texture():\n        self.logger.info(\"discrete texture, saving classes\")\n        self.logger.info(f\"Saving IDs_to_labels to {str(savepath)}\")\n        with open(savepath, \"w\") as outfile_h:\n            json.dump(\n                self.get_IDs_to_labels(), outfile_h, ensure_ascii=False, indent=4\n            )\n    else:\n        self.logger.warn(\"non-discrete texture, not saving classes\")\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.save_renders","title":"<code>save_renders(camera_set, render_image_scale=1.0, output_folder=Path(VIS_FOLDER, 'renders'), make_composites=False, save_native_resolution=False, cast_to_uint8=True, uint8_value_for_null_texture=NULL_TEXTURE_INT_VALUE, **render_kwargs)</code>","text":"<p>Render an image from the viewpoint of each specified camera and save a composite</p> <p>Parameters:</p> Name Type Description Default <code>camera_set</code> <code>PhotogrammetryCameraSet</code> <p>Camera set to use for rendering</p> required <code>render_image_scale</code> <code>float</code> <p>Multiplier on the real image scale to obtain size for rendering. Lower values yield a lower-resolution render but the runtime is quiker. Defaults to 1.0.</p> <code>1.0</code> <code>render_folder</code> <code>PATH_TYPE</code> <p>Save images to this folder. Defaults to Path(VIS_FOLDER, \"renders\")</p> required <code>make_composites</code> <code>bool</code> <p>Should a triple pane composite with the original image be saved rather than the raw label</p> <code>False</code> <code>cast_to_uint8</code> <code>bool</code> <p>(bool, optional): cast the float valued data to unit8 for saving efficiency. May dramatically increase efficiency due to png compression</p> <code>True</code> <code>uint8_value_for_null_texture</code> <code>uint8</code> <p>What value to assign for values that can't be represented as unsigned 8-bit data. Defaults to NULL_TEXTURE_INT_VALUE</p> <code>NULL_TEXTURE_INT_VALUE</code> <code>render_kwargs</code> <p>keyword arguments passed to the render.</p> <code>{}</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def save_renders(\n    self,\n    camera_set: PhotogrammetryCameraSet,\n    render_image_scale=1.0,\n    output_folder: PATH_TYPE = Path(VIS_FOLDER, \"renders\"),\n    make_composites: bool = False,\n    save_native_resolution: bool = False,\n    cast_to_uint8: bool = True,\n    uint8_value_for_null_texture: np.uint8 = NULL_TEXTURE_INT_VALUE,\n    **render_kwargs,\n):\n    \"\"\"Render an image from the viewpoint of each specified camera and save a composite\n\n    Args:\n        camera_set (PhotogrammetryCameraSet):\n            Camera set to use for rendering\n        render_image_scale (float, optional):\n            Multiplier on the real image scale to obtain size for rendering. Lower values\n            yield a lower-resolution render but the runtime is quiker. Defaults to 1.0.\n        render_folder (PATH_TYPE, optional):\n            Save images to this folder. Defaults to Path(VIS_FOLDER, \"renders\")\n        make_composites (bool, optional):\n            Should a triple pane composite with the original image be saved rather than the\n            raw label\n        cast_to_uint8: (bool, optional):\n            cast the float valued data to unit8 for saving efficiency. May dramatically increase\n            efficiency due to png compression\n        uint8_value_for_null_texture (np.uint8, optional):\n            What value to assign for values that can't be represented as unsigned 8-bit data.\n            Defaults to NULL_TEXTURE_INT_VALUE\n        render_kwargs:\n            keyword arguments passed to the render.\n    \"\"\"\n\n    ensure_folder(output_folder)\n    self.logger.info(f\"Saving renders to {output_folder}\")\n\n    # Save the classes filename\n    self.save_IDs_to_labels(Path(output_folder, \"IDs_to_labels.json\"))\n\n    # Create the generator object to render the images\n    # Since this is a generator, this will be fast\n    render_gen = self.render_flat(\n        camera_set, render_img_scale=render_image_scale, **render_kwargs\n    )\n\n    # The computation only happens when items are requested from the generator\n    for i, rendered in enumerate(\n        tqdm(\n            render_gen,\n            total=len(camera_set),\n            desc=\"Computing and saving renders\",\n        )\n    ):\n        ## All this is post-processing to visualize the rendered label.\n        # rendered could either be a one channel image of integer IDs,\n        # a one-channel image of scalars, or a three-channel image of\n        # RGB. It could also be multi-channel image corresponding to anything,\n        # but we don't expect that yet\n\n        if save_native_resolution and render_image_scale != 1:\n            native_size = camera_set[i].get_image_size()\n            # Upsample using nearest neighbor interpolation for discrete labels and\n            # bilinear for non-discrete\n            # TODO this will need to be fixed for multi-channel images since I don't think resize works\n            rendered = resize(\n                rendered,\n                native_size,\n                order=(0 if self.is_discrete_texture() else 1),\n            )\n\n        if make_composites:\n            RGB_image = camera_set[i].get_image(\n                image_scale=(1.0 if save_native_resolution else render_image_scale)\n            )\n            rendered = create_composite(\n                RGB_image=RGB_image,\n                label_image=rendered,\n                IDs_to_labels=self.get_IDs_to_labels(),\n            )\n        else:\n            # Clip channels if needed\n            if rendered.ndim == 3:\n                rendered = rendered[..., :3]\n\n        if cast_to_uint8:\n            # Deterimine values that cannot be represented as uint8\n            mask = np.logical_or.reduce(\n                [\n                    rendered &lt; 0,\n                    rendered &gt; 255,\n                    np.logical_not(np.isfinite(rendered)),\n                ]\n            )\n            rendered[mask] = uint8_value_for_null_texture\n            # Cast and squeeze since you can't save a one-channel image\n            rendered = np.squeeze(rendered.astype(np.uint8))\n\n        # Saving\n        output_filename = Path(\n            output_folder, camera_set.get_image_filename(i, absolute=False)\n        )\n        # This may create nested folders in the output dir\n        ensure_containing_folder(output_filename)\n        if rendered.dtype == np.uint8:\n            output_filename = str(output_filename.with_suffix(\".png\"))\n\n            # Save the image\n            skimage.io.imsave(output_filename, rendered, check_contrast=False)\n        else:\n            output_filename = str(output_filename.with_suffix(\".npy\"))\n            # Save the image\n            np.save(output_filename, rendered)\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.select_mesh_ROI","title":"<code>select_mesh_ROI(region_of_interest, buffer_meters=0, simplify_tol_meters=0, default_CRS=pyproj.CRS.from_epsg(4326), return_original_IDs=False)</code>","text":"<p>Get a subset of the mesh based on geospatial data</p> <p>Parameters:</p> Name Type Description Default <code>region_of_interest</code> <code>Union[GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE]</code> <p>Region of interest. Can be a * dataframe, where all columns will be colapsed * A shapely polygon/multipolygon * A file that can be loaded by geopandas</p> required <code>buffer_meters</code> <code>float</code> <p>Expand the geometry by this amount of meters. Defaults to 0.</p> <code>0</code> <code>simplify_tol_meters</code> <code>float</code> <p>Simplify the geometry using this as the tolerance. Defaults to 0.</p> <code>0</code> <code>default_CRS</code> <code>CRS</code> <p>The CRS to use if one isn't provided. Defaults to pyproj.CRS.from_epsg(4326).</p> <code>from_epsg(4326)</code> <code>return_original_IDs</code> <code>bool</code> <p>Return the indices into the original mesh. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>pyvista.PolyData: The subset of the mesh</p> <p>np.ndarray: The indices of the points in the original mesh (only if return_original_IDs set)</p> <p>np.ndarray: The indices of the faces in the original mesh (only if return_original_IDs set)</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def select_mesh_ROI(\n    self,\n    region_of_interest: typing.Union[\n        gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE, None\n    ],\n    buffer_meters: float = 0,\n    simplify_tol_meters: int = 0,\n    default_CRS: pyproj.CRS = pyproj.CRS.from_epsg(4326),\n    return_original_IDs: bool = False,\n):\n    \"\"\"Get a subset of the mesh based on geospatial data\n\n    Args:\n        region_of_interest (typing.Union[gpd.GeoDataFrame, Polygon, MultiPolygon, PATH_TYPE]):\n            Region of interest. Can be a\n            * dataframe, where all columns will be colapsed\n            * A shapely polygon/multipolygon\n            * A file that can be loaded by geopandas\n        buffer_meters (float, optional): Expand the geometry by this amount of meters. Defaults to 0.\n        simplify_tol_meters (float, optional): Simplify the geometry using this as the tolerance. Defaults to 0.\n        default_CRS (pyproj.CRS, optional): The CRS to use if one isn't provided. Defaults to pyproj.CRS.from_epsg(4326).\n        return_original_IDs (bool, optional): Return the indices into the original mesh. Defaults to False.\n\n    Returns:\n        pyvista.PolyData: The subset of the mesh\n        np.ndarray: The indices of the points in the original mesh (only if return_original_IDs set)\n        np.ndarray: The indices of the faces in the original mesh (only if return_original_IDs set)\n    \"\"\"\n    if region_of_interest is None:\n        return self.pyvista_mesh\n\n    # Get the ROI into a geopandas GeoDataFrame\n    self.logger.info(\"Standardizing ROI\")\n    if isinstance(region_of_interest, gpd.GeoDataFrame):\n        ROI_gpd = region_of_interest\n    elif isinstance(region_of_interest, (Polygon, MultiPolygon)):\n        ROI_gpd = gpd.DataFrame(crs=default_CRS, geometry=[region_of_interest])\n    else:\n        ROI_gpd = gpd.read_file(region_of_interest)\n\n    self.logger.info(\"Dissolving ROI\")\n    # Disolve to ensure there is only one row\n    ROI_gpd = ROI_gpd.dissolve()\n    self.logger.info(\"Setting CRS and buffering ROI\")\n    # Make sure we're using a projected CRS so a buffer can be applied\n    ROI_gpd = ensure_projected_CRS(ROI_gpd)\n    # Apply the buffer, plus the tolerance, to ensure we keep at least the requested region\n    ROI_gpd[\"geometry\"] = ROI_gpd.buffer(buffer_meters + simplify_tol_meters)\n    # Simplify the geometry to reduce the computational load\n    ROI_gpd.geometry = ROI_gpd.geometry.simplify(simplify_tol_meters)\n    self.logger.info(\"Dissolving buffered ROI\")\n    # Disolve again in case\n    ROI_gpd = ROI_gpd.dissolve()\n\n    self.logger.info(\"Extracting verts for dataframe\")\n    # Get the vertices as a dataframe in the same CRS\n    verts_df = self.get_verts_geodataframe(ROI_gpd.crs)\n    self.logger.info(\"Checking intersection of verts with ROI\")\n    # Determine which vertices are within the ROI polygon\n    verts_in_ROI = gpd.tools.overlay(verts_df, ROI_gpd, how=\"intersection\")\n    # Extract the IDs of the set within the polygon\n    vert_inds = verts_in_ROI[\"vert_ID\"].to_numpy()\n\n    self.logger.info(\"Extracting points from pyvista mesh\")\n    # Extract a submesh using these IDs, which is returned as an UnstructuredGrid\n    subset_unstructured_grid = self.pyvista_mesh.extract_points(vert_inds)\n    self.logger.info(\"Extraction surface from subset mesh\")\n    # Convert the unstructured grid to a PolyData (mesh) again\n    subset_mesh = subset_unstructured_grid.extract_surface()\n\n    # If we need the indices into the original mesh, return those\n    if return_original_IDs:\n        try:\n            point_IDs = subset_unstructured_grid[\"vtkOriginalPointIds\"]\n            face_IDs = subset_unstructured_grid[\"vtkOriginalCellIds\"]\n        except KeyError:\n            point_IDs = np.array([])\n            face_IDs = np.array([])\n\n        return (\n            subset_mesh,\n            point_IDs,\n            face_IDs,\n        )\n    # Else return just the mesh\n    return subset_mesh\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.set_texture","title":"<code>set_texture(texture_array, IDs_to_labels=None, all_discrete_texture_values=None, is_vertex_texture=None, use_derived_IDs_to_labels=False, delete_existing=True)</code>","text":"<p>Set the internal texture representation</p> <p>Parameters:</p> Name Type Description Default <code>texture_array</code> <code>ndarray</code> <p>The array of texture values. The first dimension must be the length of faces or verts. A second dimension is optional.</p> required <code>IDs_to_labels</code> <code>Union[None, dict]</code> <p>Mapping from integer IDs to string names. Defaults to None.</p> <code>None</code> <code>all_discrete_texture_values</code> <code>Union[List, None]</code> <p>Are all the texture values known to be discrete, representing IDs. Computed from the data if not set. Defaults to None.</p> <code>None</code> <code>is_vertex_texture</code> <code>Union[bool, None]</code> <p>Are the texture values supposed to correspond to the vertices. Computed from the data if not set. Defaults to None.</p> <code>None</code> <code>use_derived_IDs_to_labels</code> <code>bool</code> <p>Use IDs to labels derived from data if not explicitly provided. Defaults to False.</p> <code>False</code> <code>delete_existing</code> <code>bool</code> <p>Delete the existing texture when the other one (face, vertex) is set. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the size of the texture doesn't match the number of either faces or vertices</p> <code>ValueError</code> <p>If the number of faces and vertices are the same and is_vertex_texture isn't set</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def set_texture(\n    self,\n    texture_array: np.ndarray,\n    IDs_to_labels: typing.Union[None, dict] = None,\n    all_discrete_texture_values: typing.Union[typing.List, None] = None,\n    is_vertex_texture: typing.Union[bool, None] = None,\n    use_derived_IDs_to_labels: bool = False,\n    delete_existing: bool = True,\n):\n    \"\"\"Set the internal texture representation\n\n    Args:\n        texture_array (np.ndarray):\n            The array of texture values. The first dimension must be the length of faces or verts. A second dimension is optional.\n        IDs_to_labels (typing.Union[None, dict], optional): Mapping from integer IDs to string names. Defaults to None.\n        all_discrete_texture_values (typing.Union[typing.List, None], optional):\n            Are all the texture values known to be discrete, representing IDs. Computed from the data if not set. Defaults to None.\n        is_vertex_texture (typing.Union[bool, None], optional):\n            Are the texture values supposed to correspond to the vertices. Computed from the data if not set. Defaults to None.\n        use_derived_IDs_to_labels (bool, optional): Use IDs to labels derived from data if not explicitly provided. Defaults to False.\n        delete_existing (bool, optional): Delete the existing texture when the other one (face, vertex) is set. Defaults to True.\n\n    Raises:\n        ValueError: If the size of the texture doesn't match the number of either faces or vertices\n        ValueError: If the number of faces and vertices are the same and is_vertex_texture isn't set\n    \"\"\"\n    texture_array = self.standardize_texture(texture_array)\n    # IDs_to_labels (typing.Union[None, dict]): Dictionary mapping from integer IDs to string class names\n\n    # If it is not specified whether this is a vertex texture, attempt to infer it from the shape\n    # TODO consider refactoring to check whether it matches the number of one of them,\n    # no matter whether is_vertex_texture is specified\n    if is_vertex_texture is None:\n        # Check that the number of matches face or verts\n        n_values = texture_array.shape[0]\n        n_faces = self.faces.shape[0]\n        n_verts = self.pyvista_mesh.points.shape[0]\n\n        if n_verts == n_faces:\n            raise ValueError(\n                \"Cannot infer whether texture should be applied to vertices of faces because the number is the same\"\n            )\n        elif n_values == n_verts:\n            is_vertex_texture = True\n        elif n_values == n_faces:\n            is_vertex_texture = False\n        else:\n            raise ValueError(\n                f\"The number of elements in the texture ({n_values}) did not match the number of faces ({n_faces}) or vertices ({n_verts})\"\n            )\n\n    # Ensure that the actual data type is float, and record label names\n    if texture_array.ndim == 2 and texture_array.shape[1] != 1:\n        # If it is more than one column, it's assumed to be a real-valued\n        # quantity and we try to cast it to a float\n        texture_array = texture_array.astype(float)\n        derived_IDs_to_labels = None\n    else:\n        texture_array, derived_IDs_to_labels = ensure_float_labels(\n            texture_array, full_array=all_discrete_texture_values\n        )\n\n    # If IDs to labels is explicitly provided, trust that\n    # TODO should do some type checking here\n    if isinstance(IDs_to_labels, dict):\n        self.IDs_to_labels = IDs_to_labels\n    # If not, but we can compute it, use that. Otherwise, we might want to force them to be set to None\n    elif use_derived_IDs_to_labels:\n        self.IDs_to_labels = derived_IDs_to_labels\n\n    # Set the appropriate texture and optionally delete the other one\n    if is_vertex_texture:\n        self.vertex_texture = texture_array\n        if delete_existing:\n            self.face_texture = None\n    else:\n        self.face_texture = texture_array\n        if delete_existing:\n            self.vertex_texture = None\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.transfer_texture","title":"<code>transfer_texture(downsampled_mesh)</code>","text":"<p>Transfer texture from original mesh to a downsampled version using KDTree for nearest neighbor point searches</p> <p>Parameters:</p> Name Type Description Default <code>downsampled_mesh</code> <code>PolyData</code> <p>The downsampled version of the original mesh</p> required <p>Returns:</p> Type Description <p>pv.PolyData: The downsampled mesh with the transferred textures</p> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def transfer_texture(self, downsampled_mesh):\n    \"\"\"Transfer texture from original mesh to a downsampled version using KDTree for nearest neighbor point searches\n\n    Args:\n        downsampled_mesh (pv.PolyData): The downsampled version of the original mesh\n\n    Returns:\n        pv.PolyData: The downsampled mesh with the transferred textures\n    \"\"\"\n    # Only transfer textures if there are point based scalars in the original mesh\n    if self.pyvista_mesh.point_data:\n        # Store original mesh points in KDTree for nearest neighbor search\n        kdtree = KDTree(self.pyvista_mesh.points)\n\n        # For ecah point in the downsampled mesh find the nearest neighbor point in the original mesh\n        _, nearest_neighbor_indices = kdtree.query(downsampled_mesh.points)\n\n        # Iterate over all the point based scalars\n        for scalar_name in self.pyvista_mesh.point_data.keys():\n            # Retrieve scalar data of appropriate index using the nearest neighbor indices\n            transferred_scalars = self.pyvista_mesh.point_data[scalar_name][\n                nearest_neighbor_indices\n            ]\n            # Set the corresponding scalar data in the downsampled mesh\n            downsampled_mesh.point_data[scalar_name] = transferred_scalars\n\n        # Set active mesh of downsampled mesh\n        if self.pyvista_mesh.active_scalars_name:\n            downsampled_mesh.active_scalars_name = (\n                self.pyvista_mesh.active_scalars_name\n            )\n    else:\n        self.logger.warning(\n            \"Textures not transferred, active scalars data is assoicated with cell data not point data\"\n        )\n    return downsampled_mesh\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.transform_vertices","title":"<code>transform_vertices(transform_4x4, in_place=False)</code>","text":"<p>Apply a transform to the vertex coordinates</p> <p>Parameters:</p> Name Type Description Default <code>transform_4x4</code> <code>ndarray</code> <p>Transform to be applied</p> required <code>in_place</code> <code>bool</code> <p>Should the vertices be updated for all member objects</p> <code>False</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def transform_vertices(self, transform_4x4: np.ndarray, in_place: bool = False):\n    \"\"\"Apply a transform to the vertex coordinates\n\n    Args:\n        transform_4x4 (np.ndarray): Transform to be applied\n        in_place (bool): Should the vertices be updated for all member objects\n    \"\"\"\n    homogenous_local_points = np.vstack(\n        (self.pyvista_mesh.points.T, np.ones(self.pyvista_mesh.points.shape[0]))\n    )\n    transformed_local_points = transform_4x4 @ homogenous_local_points\n    transformed_local_points = transformed_local_points[:3].T\n\n    # Overwrite existing vertices in both pytorch3d and pyvista mesh\n    if in_place:\n        self.pyvista_mesh.points = transformed_local_points.copy()\n    return transformed_local_points\n</code></pre>"},{"location":"meshes/meshes/#geograypher.meshes.TexturedPhotogrammetryMesh.vis","title":"<code>vis(plotter=None, interactive=True, camera_set=None, screenshot_filename=None, vis_scalars=None, mesh_kwargs=None, interactive_jupyter=False, plotter_kwargs={}, enable_ssao=True, force_xvfb=False, frustum_scale=2, IDs_to_labels=None)</code>","text":"<p>Show the mesh and cameras</p> <p>Parameters:</p> Name Type Description Default <code>plotter</code> <code>Plotter</code> <p>Plotter to use, else one will be created</p> <code>None</code> <code>off_screen</code> <code>bool</code> <p>Show offscreen</p> required <code>camera_set</code> <code>PhotogrammetryCameraSet</code> <p>Cameras to visualize. Defaults to None.</p> <code>None</code> <code>screenshot_filename</code> <code>PATH_TYPE</code> <p>Filepath to save to, will show interactively if None. Defaults to None.</p> <code>None</code> <code>vis_scalars</code> <code>(None, ndarray)</code> <p>Scalars to show</p> <code>None</code> <code>mesh_kwargs</code> <code>Dict</code> <p>dict of keyword arguments for the mesh</p> <code>None</code> <code>interactive_jupyter</code> <code>bool</code> <p>Should jupyter windows be interactive. This doesn't always work, especially on VSCode.</p> <code>False</code> <code>plotter_kwargs</code> <code>Dict</code> <p>dict of keyword arguments for the plotter</p> <code>{}</code> <code>frustum_scale</code> <code>float</code> <p>Size of cameras in world units. Defaults to None.</p> <code>2</code> <code>IDs_to_labels</code> <code>[None, dict]</code> <p>Mapping from IDs to human readable labels for discrete classes. Defaults to the mesh IDs_to_labels if unset.</p> <code>None</code> Source code in <code>geograypher/meshes/meshes.py</code> <pre><code>def vis(\n    self,\n    plotter: pv.Plotter = None,\n    interactive: bool = True,\n    camera_set: PhotogrammetryCameraSet = None,\n    screenshot_filename: PATH_TYPE = None,\n    vis_scalars: typing.Union[None, np.ndarray] = None,\n    mesh_kwargs: typing.Dict = None,\n    interactive_jupyter: bool = False,\n    plotter_kwargs: typing.Dict = {},\n    enable_ssao: bool = True,\n    force_xvfb: bool = False,\n    frustum_scale: float = 2,\n    IDs_to_labels: typing.Union[None, dict] = None,\n):\n    \"\"\"Show the mesh and cameras\n\n    Args:\n        plotter (pyvista.Plotter, optional):\n            Plotter to use, else one will be created\n        off_screen (bool, optional):\n            Show offscreen\n        camera_set (PhotogrammetryCameraSet, optional):\n            Cameras to visualize. Defaults to None.\n        screenshot_filename (PATH_TYPE, optional):\n            Filepath to save to, will show interactively if None. Defaults to None.\n        vis_scalars (None, np.ndarray):\n            Scalars to show\n        mesh_kwargs:\n            dict of keyword arguments for the mesh\n        interactive_jupyter (bool):\n            Should jupyter windows be interactive. This doesn't always work, especially on VSCode.\n        plotter_kwargs:\n            dict of keyword arguments for the plotter\n        frustum_scale (float, optional):\n            Size of cameras in world units. Defaults to None.\n        IDs_to_labels ([None, dict], optional):\n            Mapping from IDs to human readable labels for discrete classes. Defaults to the mesh\n            IDs_to_labels if unset.\n    \"\"\"\n    off_screen = (not interactive) or (screenshot_filename is not None)\n\n    # If the IDs to labels is not set, use the default ones for this mesh\n    if IDs_to_labels is None:\n        IDs_to_labels = self.get_IDs_to_labels()\n\n    # Set the mesh kwargs if not set\n    if mesh_kwargs is None:\n        # This needs to be a dict, even if it's empty\n        mesh_kwargs = {}\n\n        # If there are discrete labels, set the colormap and limits inteligently\n        if IDs_to_labels is not None:\n            # Compute the largest ID\n            max_ID = max(IDs_to_labels.keys())\n            if max_ID &lt; 20:\n                colors = [\n                    matplotlib.colors.to_hex(c)\n                    for c in plt.get_cmap(\n                        (\"tab10\" if max_ID &lt; 10 else \"tab20\")\n                    ).colors\n                ]\n                mesh_kwargs[\"cmap\"] = colors[0 : max_ID + 1]\n                mesh_kwargs[\"clim\"] = (-0.5, max_ID + 0.5)\n\n    # Create the plotter if it's None\n    plotter = create_pv_plotter(\n        off_screen=off_screen, force_xvfb=force_xvfb, plotter=plotter\n    )\n\n    # If the vis scalars are None, use the saved texture\n    if vis_scalars is None:\n        vis_scalars = self.get_texture(\n            # Request vertex texture if both are available\n            request_vertex_texture=(\n                True\n                if (\n                    self.vertex_texture is not None\n                    and self.face_texture is not None\n                )\n                else None\n            )\n        )\n\n    is_rgb = (\n        self.pyvista_mesh.active_scalars_name == \"RGB\"\n        if vis_scalars is None\n        else (vis_scalars.ndim == 2 and vis_scalars.shape[1] &gt; 1)\n    )\n\n    # Data in the range [0, 255] must be uint8 type\n    if is_rgb and np.max(vis_scalars) &gt; 1.0:\n        vis_scalars = np.clip(vis_scalars, 0, 255).astype(np.uint8)\n\n    scalar_bar_args = {\"vertical\": True}\n    if IDs_to_labels is not None and \"annotations\" not in mesh_kwargs:\n        mesh_kwargs[\"annotations\"] = IDs_to_labels\n        scalar_bar_args[\"n_labels\"] = 0\n\n    if \"jupyter_backend\" not in plotter_kwargs:\n        if interactive_jupyter:\n            plotter_kwargs[\"jupyter_backend\"] = \"trame\"\n        else:\n            plotter_kwargs[\"jupyter_backend\"] = \"static\"\n\n    # Add the mesh\n    plotter.add_mesh(\n        self.pyvista_mesh,\n        scalars=vis_scalars,\n        rgb=is_rgb,\n        scalar_bar_args=scalar_bar_args,\n        **mesh_kwargs,\n    )\n    # If the camera set is provided, show this too\n    if camera_set is not None:\n        # Adjust the frustum scale if the mesh came from metashape\n        # Find the cube root of the determinant of the upper-left 3x3 submatrix to find the scaling factor\n        if (\n            self.local_to_epgs_4978_transform is not None\n            and frustum_scale is not None\n        ):\n            transform_determinant = np.linalg.det(\n                self.local_to_epgs_4978_transform[:3, :3]\n            )\n            scale_factor = np.cbrt(transform_determinant)\n            frustum_scale = frustum_scale / scale_factor\n        camera_set.vis(\n            plotter, add_orientation_cube=False, frustum_scale=frustum_scale\n        )\n\n    # Enable screen space shading\n    if enable_ssao:\n        plotter.enable_ssao()\n\n    # Create parent folder if none exists\n    if screenshot_filename is not None:\n        ensure_containing_folder(screenshot_filename)\n\n    # Show\n    return plotter.show(\n        screenshot=screenshot_filename,\n        title=\"Geograypher mesh viewer\",\n        **plotter_kwargs,\n    )\n</code></pre>"}]}